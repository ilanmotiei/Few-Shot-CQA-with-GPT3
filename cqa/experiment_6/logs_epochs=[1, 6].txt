 --------------- Epoch 1/10 Training Start --------------- 
 ------------ Epoch 1/10 Batch 50/11616 Training Results ------------ 
Total Loss: 13.211904788017273
Span Start Loss: 5.919897937774659
Span End Loss: 5.940604991912842
Type Loss: 1.351401720046997
 ------------ Epoch 1/10 Batch 100/11616 Training Results ------------ 
Total Loss: 13.202936482429504
Span Start Loss: 5.914592609405518
Span End Loss: 5.938591933250427
Type Loss: 1.3497517478466035
 ------------ Epoch 1/10 Batch 150/11616 Training Results ------------ 
Total Loss: 13.187981470425923
Span Start Loss: 5.909859272638957
Span End Loss: 5.936630112330119
Type Loss: 1.3414918597539265
 ------------ Epoch 1/10 Batch 200/11616 Training Results ------------ 
Total Loss: 13.191792887449264
Span Start Loss: 5.913890700340271
Span End Loss: 5.947175693511963
Type Loss: 1.3307263022661209
 ------------ Epoch 1/10 Batch 250/11616 Training Results ------------ 
Total Loss: 13.175169262886048
Span Start Loss: 5.910417865753174
Span End Loss: 5.9425041980743405
Type Loss: 1.3222469801902772
 ------------ Epoch 1/10 Batch 300/11616 Training Results ------------ 
Total Loss: 13.164864770571391
Span Start Loss: 5.912726718584697
Span End Loss: 5.942905000050863
Type Loss: 1.3092328329881033
 ------------ Epoch 1/10 Batch 350/11616 Training Results ------------ 
Total Loss: 13.128991750308446
Span Start Loss: 5.9036643559592115
Span End Loss: 5.930214621680124
Type Loss: 1.2951125618389674
 ------------ Epoch 1/10 Batch 400/11616 Training Results ------------ 
Total Loss: 13.108094131946563
Span Start Loss: 5.901917643547058
Span End Loss: 5.927883585691452
Type Loss: 1.2782926937937737
 ------------ Epoch 1/10 Batch 450/11616 Training Results ------------ 
Total Loss: 13.084811369578043
Span Start Loss: 5.898327273262872
Span End Loss: 5.9238216124640575
Type Loss: 1.2626622596051957
 ------------ Epoch 1/10 Batch 500/11616 Training Results ------------ 
Total Loss: 13.055970346927642
Span Start Loss: 5.8931422662734985
Span End Loss: 5.91985498714447
Type Loss: 1.2429728544950485
 ------------ Epoch 1/10 Batch 550/11616 Training Results ------------ 
Total Loss: 13.028323093327609
Span Start Loss: 5.8887817807631055
Span End Loss: 5.915523871508512
Type Loss: 1.2240171972188083
 ------------ Epoch 1/10 Batch 600/11616 Training Results ------------ 
Total Loss: 12.99986500342687
Span Start Loss: 5.886392532984416
Span End Loss: 5.913252918720246
Type Loss: 1.2002193200588227
 ------------ Epoch 1/10 Batch 650/11616 Training Results ------------ 
Total Loss: 12.969961052674513
Span Start Loss: 5.882332204672006
Span End Loss: 5.908641314139733
Type Loss: 1.178987310207807
 ------------ Epoch 1/10 Batch 700/11616 Training Results ------------ 
Total Loss: 12.935776323931558
Span Start Loss: 5.876128322737557
Span End Loss: 5.901916064534869
Type Loss: 1.1577317159516471
 ------------ Epoch 1/10 Batch 750/11616 Training Results ------------ 
Total Loss: 12.904275361696879
Span Start Loss: 5.870833424886068
Span End Loss: 5.895516337712606
Type Loss: 1.1379253695011138
 ------------ Epoch 1/10 Batch 800/11616 Training Results ------------ 
Total Loss: 12.868046753108501
Span Start Loss: 5.863522717952728
Span End Loss: 5.887330701947212
Type Loss: 1.1171931046247483
 ------------ Epoch 1/10 Batch 850/11616 Training Results ------------ 
Total Loss: 12.834268166037166
Span Start Loss: 5.856868508843815
Span End Loss: 5.8808160136727725
Type Loss: 1.096583416181452
 ------------ Epoch 1/10 Batch 900/11616 Training Results ------------ 
Total Loss: 12.802027017540402
Span Start Loss: 5.849535718493992
Span End Loss: 5.8725544362598
Type Loss: 1.0799366310901113
 ------------ Epoch 1/10 Batch 950/11616 Training Results ------------ 
Total Loss: 12.770266566778485
Span Start Loss: 5.842257364172685
Span End Loss: 5.864176279871088
Type Loss: 1.063832694417552
 ------------ Epoch 1/10 Batch 1000/11616 Training Results ------------ 
Total Loss: 12.73302318096161
Span Start Loss: 5.8334610509872435
Span End Loss: 5.854013712406158
Type Loss: 1.0455481842458247
 ------------ Epoch 1/10 Batch 1050/11616 Training Results ------------ 
Total Loss: 12.698341869172596
Span Start Loss: 5.825735718863351
Span End Loss: 5.843599341256278
Type Loss: 1.0290065763110205
 ------------ Epoch 1/10 Batch 1100/11616 Training Results ------------ 
Total Loss: 12.661176081137224
Span Start Loss: 5.8156605937264185
Span End Loss: 5.83172857241197
Type Loss: 1.0137866857918827
 ------------ Epoch 1/10 Batch 1150/11616 Training Results ------------ 
Total Loss: 12.623396738715794
Span Start Loss: 5.804123719671498
Span End Loss: 5.819916852868122
Type Loss: 0.9993559441618297
 ------------ Epoch 1/10 Batch 1200/11616 Training Results ------------ 
Total Loss: 12.584770043691
Span Start Loss: 5.793985102971395
Span End Loss: 5.80595641652743
Type Loss: 0.9848283000042041
 ------------ Epoch 1/10 Batch 1250/11616 Training Results ------------ 
Total Loss: 12.547275241851807
Span Start Loss: 5.782242841339111
Span End Loss: 5.790754480361938
Type Loss: 0.9742776974320412
 ------------ Epoch 1/10 Batch 1300/11616 Training Results ------------ 
Total Loss: 12.511089926499587
Span Start Loss: 5.769754812900836
Span End Loss: 5.774657472096957
Type Loss: 0.9666774162764733
 ------------ Epoch 1/10 Batch 1350/11616 Training Results ------------ 
Total Loss: 12.47334046098921
Span Start Loss: 5.759423694610596
Span End Loss: 5.7598005693930165
Type Loss: 0.9541159702892656
 ------------ Epoch 1/10 Batch 1400/11616 Training Results ------------ 
Total Loss: 12.436309701630048
Span Start Loss: 5.7480193502562384
Span End Loss: 5.743106426170894
Type Loss: 0.9451836978111948
 ------------ Epoch 1/10 Batch 1450/11616 Training Results ------------ 
Total Loss: 12.394931919410311
Span Start Loss: 5.733250066822973
Span End Loss: 5.723433396569614
Type Loss: 0.9382482252449825
 ------------ Epoch 1/10 Batch 1500/11616 Training Results ------------ 
Total Loss: 12.364481879472732
Span Start Loss: 5.723180912971497
Span End Loss: 5.70898064772288
Type Loss: 0.9323200853466987
 ------------ Epoch 1/10 Batch 1550/11616 Training Results ------------ 
Total Loss: 12.324215238709604
Span Start Loss: 5.708732584984072
Span End Loss: 5.689808203789496
Type Loss: 0.9256742195929251
 ------------ Epoch 1/10 Batch 1600/11616 Training Results ------------ 
Total Loss: 12.278204615414143
Span Start Loss: 5.692438948452473
Span End Loss: 5.669488121271133
Type Loss: 0.9162773181125522
 ------------ Epoch 1/10 Batch 1650/11616 Training Results ------------ 
Total Loss: 12.237980298923723
Span Start Loss: 5.678064760728316
Span End Loss: 5.649530439954815
Type Loss: 0.9103848696658106
 ------------ Epoch 1/10 Batch 1700/11616 Training Results ------------ 
Total Loss: 12.189080884176143
Span Start Loss: 5.659182594523711
Span End Loss: 5.6269329951791205
Type Loss: 0.9029650708507089
 ------------ Epoch 1/10 Batch 1750/11616 Training Results ------------ 
Total Loss: 12.143347589629037
Span Start Loss: 5.641114731652396
Span End Loss: 5.606269731249128
Type Loss: 0.895962902035032
 ------------ Epoch 1/10 Batch 1800/11616 Training Results ------------ 
Total Loss: 12.09305758608712
Span Start Loss: 5.621708014541202
Span End Loss: 5.5841910701327855
Type Loss: 0.8871582742283741
 ------------ Epoch 1/10 Batch 1850/11616 Training Results ------------ 
Total Loss: 12.040171596810625
Span Start Loss: 5.599875064282804
Span End Loss: 5.560670575064582
Type Loss: 0.8796257282592155
 ------------ Epoch 1/10 Batch 1900/11616 Training Results ------------ 
Total Loss: 11.975625777244568
Span Start Loss: 5.5719509893969486
Span End Loss: 5.528866946446268
Type Loss: 0.8748076172094597
 ------------ Epoch 1/10 Batch 1950/11616 Training Results ------------ 
Total Loss: 11.911662642161051
Span Start Loss: 5.5443843331703775
Span End Loss: 5.499119855318314
Type Loss: 0.8681582331504577
 ------------ Epoch 1/10 Batch 2000/11616 Training Results ------------ 
Total Loss: 11.846039179861545
Span Start Loss: 5.5153596285581585
Span End Loss: 5.466764462828636
Type Loss: 0.8639148684591055
 --------------- Epoch 1/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 6.2, 'f1': 16.8, 'turns': 1425}), ('literature', {'em': 7.4, 'f1': 15.4, 'turns': 1630}), ('mid-high_school', {'em': 7.2, 'f1': 16.5, 'turns': 1653}), ('news', {'em': 7.9, 'f1': 17.6, 'turns': 1649}), ('wikipedia', {'em': 9.2, 'f1': 19.8, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 7.6, 'f1': 17.2, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 7.6, 'f1': 17.2, 'turns': 7983})])
 ------------ Epoch 1/10 Batch 2050/11616 Training Results ------------ 
Total Loss: 11.772074128651038
Span Start Loss: 5.4802226816735615
Span End Loss: 5.432514369545913
Type Loss: 0.8593368582609223
 ------------ Epoch 1/10 Batch 2100/11616 Training Results ------------ 
Total Loss: 11.700120934134437
Span Start Loss: 5.44683544862838
Span End Loss: 5.399102916149866
Type Loss: 0.8541823554393791
 ------------ Epoch 1/10 Batch 2150/11616 Training Results ------------ 
Total Loss: 11.621482704961023
Span Start Loss: 5.4112354768708695
Span End Loss: 5.360739302302516
Type Loss: 0.8495077130822248
 ------------ Epoch 1/10 Batch 2200/11616 Training Results ------------ 
Total Loss: 11.546910718896171
Span Start Loss: 5.377944278825413
Span End Loss: 5.3256500173698775
Type Loss: 0.8433162107725035
 ------------ Epoch 1/10 Batch 2250/11616 Training Results ------------ 
Total Loss: 11.473659219212003
Span Start Loss: 5.345012129889594
Span End Loss: 5.289501344892714
Type Loss: 0.8391455346677038
 ------------ Epoch 1/10 Batch 2300/11616 Training Results ------------ 
Total Loss: 11.396795110080554
Span Start Loss: 5.309371557028397
Span End Loss: 5.251559249836466
Type Loss: 0.8358640928372093
 ------------ Epoch 1/10 Batch 2350/11616 Training Results ------------ 
Total Loss: 11.324298290496177
Span Start Loss: 5.277392449480422
Span End Loss: 5.216934212877395
Type Loss: 0.8299714204351952
 ------------ Epoch 1/10 Batch 2400/11616 Training Results ------------ 
Total Loss: 11.256101311246555
Span Start Loss: 5.245530472497145
Span End Loss: 5.185065709600846
Type Loss: 0.8255049229909976
 ------------ Epoch 1/10 Batch 2450/11616 Training Results ------------ 
Total Loss: 11.177493877313575
Span Start Loss: 5.208704459326608
Span End Loss: 5.146173243765928
Type Loss: 0.8226159702819221
 ------------ Epoch 1/10 Batch 2500/11616 Training Results ------------ 
Total Loss: 11.109731513023377
Span Start Loss: 5.176057762908935
Span End Loss: 5.112928497457505
Type Loss: 0.8207450491845608
 ------------ Epoch 1/10 Batch 2550/11616 Training Results ------------ 
Total Loss: 11.043225855687085
Span Start Loss: 5.1455053275239235
Span End Loss: 5.0808347189192675
Type Loss: 0.8168856071081816
 ------------ Epoch 1/10 Batch 2600/11616 Training Results ------------ 
Total Loss: 10.976972864912106
Span Start Loss: 5.112777397540899
Span End Loss: 5.049855024860455
Type Loss: 0.8143402425887493
 ------------ Epoch 1/10 Batch 2650/11616 Training Results ------------ 
Total Loss: 10.910778731557558
Span Start Loss: 5.082143340380687
Span End Loss: 5.017857025479371
Type Loss: 0.8107781672027876
 ------------ Epoch 1/10 Batch 2700/11616 Training Results ------------ 
Total Loss: 10.843982487144293
Span Start Loss: 5.049899847286719
Span End Loss: 4.9857668588338075
Type Loss: 0.8083155855426082
 ------------ Epoch 1/10 Batch 2750/11616 Training Results ------------ 
Total Loss: 10.778191936449572
Span Start Loss: 5.0189004953124305
Span End Loss: 4.9546699448065326
Type Loss: 0.8046213022199544
 ------------ Epoch 1/10 Batch 2800/11616 Training Results ------------ 
Total Loss: 10.711526781214134
Span Start Loss: 4.988459498371396
Span End Loss: 4.923178852839129
Type Loss: 0.7998882357297199
 ------------ Epoch 1/10 Batch 2850/11616 Training Results ------------ 
Total Loss: 10.651618796168712
Span Start Loss: 4.959835024130972
Span End Loss: 4.894566481280745
Type Loss: 0.79721709838039
 ------------ Epoch 1/10 Batch 2900/11616 Training Results ------------ 
Total Loss: 10.591276602190117
Span Start Loss: 4.932053974250267
Span End Loss: 4.865199785438077
Type Loss: 0.794022650811179
 ------------ Epoch 1/10 Batch 2950/11616 Training Results ------------ 
Total Loss: 10.530557480201884
Span Start Loss: 4.903568519576122
Span End Loss: 4.836219318478794
Type Loss: 0.7907694517707421
 ------------ Epoch 1/10 Batch 3000/11616 Training Results ------------ 
Total Loss: 10.472266144851844
Span Start Loss: 4.8770549639463425
Span End Loss: 4.808210180838903
Type Loss: 0.7870008115967114
 ------------ Epoch 1/10 Batch 3050/11616 Training Results ------------ 
Total Loss: 10.413820751377793
Span Start Loss: 4.84788003003011
Span End Loss: 4.78128303613819
Type Loss: 0.7846574980606799
 ------------ Epoch 1/10 Batch 3100/11616 Training Results ------------ 
Total Loss: 10.3604398677426
Span Start Loss: 4.8229862257357565
Span End Loss: 4.755799651299753
Type Loss: 0.7816538031711694
 ------------ Epoch 1/10 Batch 3150/11616 Training Results ------------ 
Total Loss: 10.300627282592986
Span Start Loss: 4.7956177615740945
Span End Loss: 4.7267266369244405
Type Loss: 0.778282698508766
 ------------ Epoch 1/10 Batch 3200/11616 Training Results ------------ 
Total Loss: 10.244300005771219
Span Start Loss: 4.7698303432390095
Span End Loss: 4.699772661961615
Type Loss: 0.7746968159382231
 ------------ Epoch 1/10 Batch 3250/11616 Training Results ------------ 
Total Loss: 10.185497228273979
Span Start Loss: 4.742239345550537
Span End Loss: 4.671461669775156
Type Loss: 0.7717960291848733
 ------------ Epoch 1/10 Batch 3300/11616 Training Results ------------ 
Total Loss: 10.131340335174041
Span Start Loss: 4.717995915918639
Span End Loss: 4.644792733445312
Type Loss: 0.7685515045148856
 ------------ Epoch 1/10 Batch 3350/11616 Training Results ------------ 
Total Loss: 10.076670225698557
Span Start Loss: 4.692721444635249
Span End Loss: 4.618927824497223
Type Loss: 0.7650207773729492
 ------------ Epoch 1/10 Batch 3400/11616 Training Results ------------ 
Total Loss: 10.028687578790327
Span Start Loss: 4.671633697923492
Span End Loss: 4.596016003384309
Type Loss: 0.7610376989742851
 ------------ Epoch 1/10 Batch 3450/11616 Training Results ------------ 
Total Loss: 9.982407055989556
Span Start Loss: 4.651707787617394
Span End Loss: 4.573743176736693
Type Loss: 0.7569559134488952
 ------------ Epoch 1/10 Batch 3500/11616 Training Results ------------ 
Total Loss: 9.932614162223679
Span Start Loss: 4.63009820815495
Span End Loss: 4.549975847244263
Type Loss: 0.752539930136076
 ------------ Epoch 1/10 Batch 3550/11616 Training Results ------------ 
Total Loss: 9.873285436252473
Span Start Loss: 4.603242828711657
Span End Loss: 4.520963271705198
Type Loss: 0.7490791605906167
 ------------ Epoch 1/10 Batch 3600/11616 Training Results ------------ 
Total Loss: 9.826458028331398
Span Start Loss: 4.581085425052378
Span End Loss: 4.499416593677468
Type Loss: 0.7459558356170439
 ------------ Epoch 1/10 Batch 3650/11616 Training Results ------------ 
Total Loss: 9.781036123143483
Span Start Loss: 4.559120331235128
Span End Loss: 4.4785466043263265
Type Loss: 0.7433690147928588
 ------------ Epoch 1/10 Batch 3700/11616 Training Results ------------ 
Total Loss: 9.732341265638132
Span Start Loss: 4.536600736572936
Span End Loss: 4.455053023228774
Type Loss: 0.7406873339988492
 ------------ Epoch 1/10 Batch 3750/11616 Training Results ------------ 
Total Loss: 9.688711137413978
Span Start Loss: 4.516048821703593
Span End Loss: 4.434668583520254
Type Loss: 0.7379935611853997
 ------------ Epoch 1/10 Batch 3800/11616 Training Results ------------ 
Total Loss: 9.646999376502476
Span Start Loss: 4.497353769979979
Span End Loss: 4.414564696487628
Type Loss: 0.7350807414174472
 ------------ Epoch 1/10 Batch 3850/11616 Training Results ------------ 
Total Loss: 9.597133139240277
Span Start Loss: 4.473644737082642
Span End Loss: 4.391915147273571
Type Loss: 0.7315730876649742
 ------------ Epoch 1/10 Batch 3900/11616 Training Results ------------ 
Total Loss: 9.55193555458234
Span Start Loss: 4.453001575164306
Span End Loss: 4.369970157360419
Type Loss: 0.7289636559392779
 ------------ Epoch 1/10 Batch 3950/11616 Training Results ------------ 
Total Loss: 9.506736749372905
Span Start Loss: 4.432518276655221
Span End Loss: 4.348421753901469
Type Loss: 0.7257965537651053
 ------------ Epoch 1/10 Batch 4000/11616 Training Results ------------ 
Total Loss: 9.46330313961953
Span Start Loss: 4.4125800858736035
Span End Loss: 4.32776762226224
Type Loss: 0.7229552675271407
 --------------- Epoch 1/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 28.3, 'f1': 42.5, 'turns': 1425}), ('literature', {'em': 28.8, 'f1': 40.1, 'turns': 1630}), ('mid-high_school', {'em': 27.6, 'f1': 41.3, 'turns': 1653}), ('news', {'em': 32.7, 'f1': 45.2, 'turns': 1649}), ('wikipedia', {'em': 29.9, 'f1': 44.5, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 29.5, 'f1': 42.7, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 29.5, 'f1': 42.7, 'turns': 7983})])
 ------------ Epoch 1/10 Batch 4050/11616 Training Results ------------ 
Total Loss: 9.423153264765386
Span Start Loss: 4.393848895526227
Span End Loss: 4.309191397296058
Type Loss: 0.7201128093079653
 ------------ Epoch 1/10 Batch 4100/11616 Training Results ------------ 
Total Loss: 9.377923741028077
Span Start Loss: 4.373099583997959
Span End Loss: 4.288032025433168
Type Loss: 0.7167919690370923
 ------------ Epoch 1/10 Batch 4150/11616 Training Results ------------ 
Total Loss: 9.336602239328695
Span Start Loss: 4.354851514460092
Span End Loss: 4.268104446939676
Type Loss: 0.7136461160439683
 ------------ Epoch 1/10 Batch 4200/11616 Training Results ------------ 
Total Loss: 9.295974914055495
Span Start Loss: 4.336667861768178
Span End Loss: 4.249287304111889
Type Loss: 0.7100195871160498
 ------------ Epoch 1/10 Batch 4250/11616 Training Results ------------ 
Total Loss: 9.256476018744356
Span Start Loss: 4.318846365592059
Span End Loss: 4.229882481406717
Type Loss: 0.7077470109094591
 ------------ Epoch 1/10 Batch 4300/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.7057838046056933
 ------------ Epoch 1/10 Batch 4350/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.7026113954588942
 ------------ Epoch 1/10 Batch 4400/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.699812074128369
 ------------ Epoch 1/10 Batch 4450/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6969143003376013
 ------------ Epoch 1/10 Batch 4500/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6941947936034865
 ------------ Epoch 1/10 Batch 4550/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6919521728788431
 ------------ Epoch 1/10 Batch 4600/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.688775975641835
 ------------ Epoch 1/10 Batch 4650/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6864704125534783
 ------------ Epoch 1/10 Batch 4700/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6838517194883303
 ------------ Epoch 1/10 Batch 4750/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6815582889655702
 ------------ Epoch 1/10 Batch 4800/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6793875035115828
 ------------ Epoch 1/10 Batch 4850/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6772298432080094
 ------------ Epoch 1/10 Batch 4900/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6749847087876073
 ------------ Epoch 1/10 Batch 4950/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6726192799485241
 ------------ Epoch 1/10 Batch 5000/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6708167432490736
 ------------ Epoch 1/10 Batch 5050/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6686117628633533
 ------------ Epoch 1/10 Batch 5100/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.667437427618413
 ------------ Epoch 1/10 Batch 5150/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6657239556793594
 ------------ Epoch 1/10 Batch 5200/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6643573844013736
 ------------ Epoch 1/10 Batch 5250/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6619305537254327
 ------------ Epoch 1/10 Batch 5300/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6597168713565564
 ------------ Epoch 1/10 Batch 5350/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6574928679167528
 ------------ Epoch 1/10 Batch 5400/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6558051006078582
 ------------ Epoch 1/10 Batch 5450/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6535649207299319
 ------------ Epoch 1/10 Batch 5500/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.652215914008631
 ------------ Epoch 1/10 Batch 5550/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6500064590731048
 ------------ Epoch 1/10 Batch 5600/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.648233486713642
 ------------ Epoch 1/10 Batch 5650/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6464848272004619
 ------------ Epoch 1/10 Batch 5700/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6448721845510105
 ------------ Epoch 1/10 Batch 5750/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6426430194630571
 ------------ Epoch 1/10 Batch 5800/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6406071922793214
 ------------ Epoch 1/10 Batch 5850/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6390080696465368
 ------------ Epoch 1/10 Batch 5900/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6375177720574252
 ------------ Epoch 1/10 Batch 5950/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.635923285648978
 ------------ Epoch 1/10 Batch 6000/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6341628492542853
 --------------- Epoch 1/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 39.3, 'f1': 52.1, 'turns': 1425}), ('literature', {'em': 36.7, 'f1': 48.5, 'turns': 1630}), ('mid-high_school', {'em': 37.6, 'f1': 50.6, 'turns': 1653}), ('news', {'em': 42.3, 'f1': 54.3, 'turns': 1649}), ('wikipedia', {'em': 41.9, 'f1': 55.3, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 39.6, 'f1': 52.2, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 39.6, 'f1': 52.2, 'turns': 7983})])
 ------------ Epoch 1/10 Batch 6050/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6328051346638971
 ------------ Epoch 1/10 Batch 6100/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.631775528196795
 ------------ Epoch 1/10 Batch 6150/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6299926550135138
 ------------ Epoch 1/10 Batch 6200/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.628858321004457
 ------------ Epoch 1/10 Batch 6250/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6272524908310175
 ------------ Epoch 1/10 Batch 6300/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6257780119049408
 ------------ Epoch 1/10 Batch 6350/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6243501485033532
 ------------ Epoch 1/10 Batch 6400/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6233851612068247
 ------------ Epoch 1/10 Batch 6450/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6219736736493056
 ------------ Epoch 1/10 Batch 6500/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6198822337285831
 ------------ Epoch 1/10 Batch 6550/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6185803529404047
 ------------ Epoch 1/10 Batch 6600/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6167328956932056
 ------------ Epoch 1/10 Batch 6650/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6152538429262271
 ------------ Epoch 1/10 Batch 6700/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6140284984189072
 ------------ Epoch 1/10 Batch 6750/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6126785529457308
 ------------ Epoch 1/10 Batch 6800/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6114812305506647
 ------------ Epoch 1/10 Batch 6850/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.610062277109029
 ------------ Epoch 1/10 Batch 6900/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6087907799665371
 ------------ Epoch 1/10 Batch 6950/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6070897104625972
 ------------ Epoch 1/10 Batch 7000/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6056390579592968
 ------------ Epoch 1/10 Batch 7050/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6047015832517798
 ------------ Epoch 1/10 Batch 7100/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.603382860340283
 ------------ Epoch 1/10 Batch 7150/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6021593373690869
 ------------ Epoch 1/10 Batch 7200/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.6005205734539777
 ------------ Epoch 1/10 Batch 7250/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5993117027796548
 ------------ Epoch 1/10 Batch 7300/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5981865475683996
 ------------ Epoch 1/10 Batch 7350/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5972179146744564
 ------------ Epoch 1/10 Batch 7400/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5958526383385666
 ------------ Epoch 1/10 Batch 7450/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5945090212997974
 ------------ Epoch 1/10 Batch 7500/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5932507031132778
 ------------ Epoch 1/10 Batch 7550/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5921596779661956
 ------------ Epoch 1/10 Batch 7600/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5907707967427804
 ------------ Epoch 1/10 Batch 7650/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5893662026208016
 ------------ Epoch 1/10 Batch 7700/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5886836823071172
 ------------ Epoch 1/10 Batch 7750/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5874165392138304
 ------------ Epoch 1/10 Batch 7800/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5861605546422876
 ------------ Epoch 1/10 Batch 7850/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5848409078694927
 ------------ Epoch 1/10 Batch 7900/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5837407366811191
 ------------ Epoch 1/10 Batch 7950/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5826690620003537
 ------------ Epoch 1/10 Batch 8000/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5815687671555206
 --------------- Epoch 1/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 46.0, 'f1': 58.1, 'turns': 1425}), ('literature', {'em': 41.9, 'f1': 53.1, 'turns': 1630}), ('mid-high_school', {'em': 43.2, 'f1': 55.2, 'turns': 1653}), ('news', {'em': 47.6, 'f1': 59.0, 'turns': 1649}), ('wikipedia', {'em': 48.8, 'f1': 61.0, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 45.5, 'f1': 57.3, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 45.5, 'f1': 57.3, 'turns': 7983})])
 ------------ Epoch 1/10 Batch 8050/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5802791941008583
 ------------ Epoch 1/10 Batch 8100/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5793432411855017
 ------------ Epoch 1/10 Batch 8150/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5783175772154258
 ------------ Epoch 1/10 Batch 8200/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5772091516025546
 ------------ Epoch 1/10 Batch 8250/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5761526591014682
 ------------ Epoch 1/10 Batch 8300/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5751792128693806
 ------------ Epoch 1/10 Batch 8350/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5738917742761042
 ------------ Epoch 1/10 Batch 8400/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5728593052078836
 ------------ Epoch 1/10 Batch 8450/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5714481207563062
 ------------ Epoch 1/10 Batch 8500/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5703758785452913
 ------------ Epoch 1/10 Batch 8550/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5697110244379058
 ------------ Epoch 1/10 Batch 8600/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5684017520926373
 ------------ Epoch 1/10 Batch 8650/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5675377986205451
 ------------ Epoch 1/10 Batch 8700/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5665715830970085
 ------------ Epoch 1/10 Batch 8750/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5656949409310307
 ------------ Epoch 1/10 Batch 8800/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5646556676141071
 ------------ Epoch 1/10 Batch 8850/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5639473903707445
 ------------ Epoch 1/10 Batch 8900/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5630298247588066
 ------------ Epoch 1/10 Batch 8950/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5619487948160015
 ------------ Epoch 1/10 Batch 9000/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5610327449447164
 ------------ Epoch 1/10 Batch 9050/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5602354843290883
 ------------ Epoch 1/10 Batch 9100/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5593589095384456
 ------------ Epoch 1/10 Batch 9150/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5582903882993456
 ------------ Epoch 1/10 Batch 9200/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5573245297704378
 ------------ Epoch 1/10 Batch 9250/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5565996389755526
 ------------ Epoch 1/10 Batch 9300/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.555540538363319
 ------------ Epoch 1/10 Batch 9350/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5548432872866723
 ------------ Epoch 1/10 Batch 9400/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5538746619299847
 ------------ Epoch 1/10 Batch 9450/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5525785206822018
 ------------ Epoch 1/10 Batch 9500/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5516537987409453
 ------------ Epoch 1/10 Batch 9550/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5505839634284924
 ------------ Epoch 1/10 Batch 9600/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5496196350977213
 ------------ Epoch 1/10 Batch 9650/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5488754010279638
 ------------ Epoch 1/10 Batch 9700/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5476891180622324
 ------------ Epoch 1/10 Batch 9750/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5469401464018111
 ------------ Epoch 1/10 Batch 9800/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5464764980605936
 ------------ Epoch 1/10 Batch 9850/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5456866393384859
 ------------ Epoch 1/10 Batch 9900/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5448477648400861
 ------------ Epoch 1/10 Batch 9950/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5440653455900449
 ------------ Epoch 1/10 Batch 10000/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5433560796930454
 --------------- Epoch 1/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 49.1, 'f1': 60.1, 'turns': 1425}), ('literature', {'em': 42.9, 'f1': 53.5, 'turns': 1630}), ('mid-high_school', {'em': 45.0, 'f1': 56.5, 'turns': 1653}), ('news', {'em': 47.9, 'f1': 59.3, 'turns': 1649}), ('wikipedia', {'em': 50.3, 'f1': 62.3, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 47.0, 'f1': 58.3, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 47.0, 'f1': 58.3, 'turns': 7983})])
 ------------ Epoch 1/10 Batch 10050/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.542312273701273
 ------------ Epoch 1/10 Batch 10100/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5416747567608104
 ------------ Epoch 1/10 Batch 10150/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5408760433237577
 ------------ Epoch 1/10 Batch 10200/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5400513690349409
 ------------ Epoch 1/10 Batch 10250/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5395010980334042
 ------------ Epoch 1/10 Batch 10300/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5388308747544956
 ------------ Epoch 1/10 Batch 10350/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5381853524122189
 ------------ Epoch 1/10 Batch 10400/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5374828733731276
 ------------ Epoch 1/10 Batch 10450/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5369841333680104
 ------------ Epoch 1/10 Batch 10500/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5363155038637065
 ------------ Epoch 1/10 Batch 10550/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5358007259847005
 ------------ Epoch 1/10 Batch 10600/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5350938439207538
 ------------ Epoch 1/10 Batch 10650/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5345938864402788
 ------------ Epoch 1/10 Batch 10700/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5338197542535507
 ------------ Epoch 1/10 Batch 10750/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5331143083749122
 ------------ Epoch 1/10 Batch 10800/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5324289320116103
 ------------ Epoch 1/10 Batch 10850/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5319296939825545
 ------------ Epoch 1/10 Batch 10900/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5313544464379617
 ------------ Epoch 1/10 Batch 10950/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5305628022168801
 ------------ Epoch 1/10 Batch 11000/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5299052871585909
 ------------ Epoch 1/10 Batch 11050/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.529362570179365
 ------------ Epoch 1/10 Batch 11100/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5287873543985188
 ------------ Epoch 1/10 Batch 11150/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5279691737335627
 ------------ Epoch 1/10 Batch 11200/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5276228951240357
 ------------ Epoch 1/10 Batch 11250/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5270881252156363
 ------------ Epoch 1/10 Batch 11300/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5265043006290113
 ------------ Epoch 1/10 Batch 11350/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5258010908391836
 ------------ Epoch 1/10 Batch 11400/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5253903747263381
 ------------ Epoch 1/10 Batch 11450/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5245668090565951
 ------------ Epoch 1/10 Batch 11500/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5240054065538166
 ------------ Epoch 1/10 Batch 11550/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5235560934508453
 ------------ Epoch 1/10 Batch 11600/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5228737619911031
 --------------------- Epoch 1/10 Final Training Results ------------------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.5228785293673219
 --------------- Epoch 1/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 43.2, 'f1': 56.2, 'turns': 1425}), ('literature', {'em': 43.0, 'f1': 53.5, 'turns': 1630}), ('mid-high_school', {'em': 42.0, 'f1': 53.8, 'turns': 1653}), ('news', {'em': 46.2, 'f1': 58.0, 'turns': 1649}), ('wikipedia', {'em': 48.3, 'f1': 60.8, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 44.6, 'f1': 56.5, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 44.6, 'f1': 56.5, 'turns': 7983})])
 --------------- Epoch 2/10 Training Start --------------- 
 ------------ Epoch 2/10 Batch 50/11616 Training Results ------------ 
Total Loss: 4.444568079710007
Span Start Loss: 2.130531953573227
Span End Loss: 1.88936820268631
Type Loss: 0.4246678844466805
 ------------ Epoch 2/10 Batch 100/11616 Training Results ------------ 
Total Loss: 4.327638563513756
Span Start Loss: 2.0462968158721924
Span End Loss: 1.860913553237915
Type Loss: 0.42042813705280424
 ------------ Epoch 2/10 Batch 150/11616 Training Results ------------ 
Total Loss: 4.34947282075882
Span Start Loss: 2.080259595314662
Span End Loss: 1.8713706282774607
Type Loss: 0.3978425436715285
 ------------ Epoch 2/10 Batch 200/11616 Training Results ------------ 
Total Loss: 4.3663392651826145
Span Start Loss: 2.0668527707457542
Span End Loss: 1.9055548650026322
Type Loss: 0.39393156157806514
 ------------ Epoch 2/10 Batch 250/11616 Training Results ------------ 
Total Loss: 4.36647371083498
Span Start Loss: 2.0528353536129
Span End Loss: 1.9118294503688813
Type Loss: 0.4018088447004557
 ------------ Epoch 2/10 Batch 300/11616 Training Results ------------ 
Total Loss: 4.347333456327518
Span Start Loss: 2.0489532725016275
Span End Loss: 1.8983616969982784
Type Loss: 0.40001842033118007
 ------------ Epoch 2/10 Batch 350/11616 Training Results ------------ 
Total Loss: 4.358392618596554
Span Start Loss: 2.0493505574975694
Span End Loss: 1.9139567606789725
Type Loss: 0.3950852316511529
 ------------ Epoch 2/10 Batch 400/11616 Training Results ------------ 
Total Loss: 4.347561846114695
Span Start Loss: 2.0396736781299114
Span End Loss: 1.9177734114229679
Type Loss: 0.3901146869827062
 ------------ Epoch 2/10 Batch 450/11616 Training Results ------------ 
Total Loss: 4.356667887005541
Span Start Loss: 2.039939740366406
Span End Loss: 1.9232569862074322
Type Loss: 0.3934710969113641
 ------------ Epoch 2/10 Batch 500/11616 Training Results ------------ 
Total Loss: 4.361632632166147
Span Start Loss: 2.046485545158386
Span End Loss: 1.9237084213495255
Type Loss: 0.3914385996758938
 ------------ Epoch 2/10 Batch 550/11616 Training Results ------------ 
Total Loss: 4.3604870933023365
Span Start Loss: 2.047928264141083
Span End Loss: 1.921366926431656
Type Loss: 0.39119184038855814
 ------------ Epoch 2/10 Batch 600/11616 Training Results ------------ 
Total Loss: 4.395613225673636
Span Start Loss: 2.0624963347117107
Span End Loss: 1.9369143890341123
Type Loss: 0.3962024349812418
 ------------ Epoch 2/10 Batch 650/11616 Training Results ------------ 
Total Loss: 4.376996616789928
Span Start Loss: 2.0531665511314685
Span End Loss: 1.931513038140077
Type Loss: 0.39231695879823886
 ------------ Epoch 2/10 Batch 700/11616 Training Results ------------ 
Total Loss: 4.348724501792874
Span Start Loss: 2.0375651851296426
Span End Loss: 1.9188146515403475
Type Loss: 0.3923445955371218
 ------------ Epoch 2/10 Batch 750/11616 Training Results ------------ 
Total Loss: 4.320212331910928
Span Start Loss: 2.028531633734703
Span End Loss: 1.9018496893246968
Type Loss: 0.38983093605190516
 ------------ Epoch 2/10 Batch 800/11616 Training Results ------------ 
Total Loss: 4.311546916421503
Span Start Loss: 2.0287196119502187
Span End Loss: 1.8952092798799276
Type Loss: 0.3876179503160529
 ------------ Epoch 2/10 Batch 850/11616 Training Results ------------ 
Total Loss: 4.3122270353576715
Span Start Loss: 2.0273604677354586
Span End Loss: 1.8991748577706955
Type Loss: 0.38569163497099107
 ------------ Epoch 2/10 Batch 900/11616 Training Results ------------ 
Total Loss: 4.29431384710802
Span Start Loss: 2.0208080605334704
Span End Loss: 1.8884087108241188
Type Loss: 0.3850970018510189
 ------------ Epoch 2/10 Batch 950/11616 Training Results ------------ 
Total Loss: 4.290073610999082
Span Start Loss: 2.0193245463622245
Span End Loss: 1.8872542882593055
Type Loss: 0.3834947011521772
 ------------ Epoch 2/10 Batch 1000/11616 Training Results ------------ 
Total Loss: 4.308180153518915
Span Start Loss: 2.030554385870695
Span End Loss: 1.8914952237308025
Type Loss: 0.38613046776875853
 ------------ Epoch 2/10 Batch 1050/11616 Training Results ------------ 
Total Loss: 4.300144283402534
Span Start Loss: 2.0256494414806365
Span End Loss: 1.889962513815789
Type Loss: 0.3845322526672057
 ------------ Epoch 2/10 Batch 1100/11616 Training Results ------------ 
Total Loss: 4.299704890901392
Span Start Loss: 2.0265160060199823
Span End Loss: 1.8892189734632319
Type Loss: 0.3839698377962817
 ------------ Epoch 2/10 Batch 1150/11616 Training Results ------------ 
Total Loss: 4.290687184618867
Span Start Loss: 2.0229269602246904
Span End Loss: 1.882884888338006
Type Loss: 0.3848752626841483
 ------------ Epoch 2/10 Batch 1200/11616 Training Results ------------ 
Total Loss: 4.267218960076571
Span Start Loss: 2.0129194936404624
Span End Loss: 1.8703790728996197
Type Loss: 0.38392032123946895
 ------------ Epoch 2/10 Batch 1250/11616 Training Results ------------ 
Total Loss: 4.280028741240502
Span Start Loss: 2.0192040761232377
Span End Loss: 1.8757462222337722
Type Loss: 0.3850783712491393
 ------------ Epoch 2/10 Batch 1300/11616 Training Results ------------ 
Total Loss: 4.27737059203478
Span Start Loss: 2.019248406176384
Span End Loss: 1.8730013244656416
Type Loss: 0.3851207880804745
 ------------ Epoch 2/10 Batch 1350/11616 Training Results ------------ 
Total Loss: 4.261404186763145
Span Start Loss: 2.011378117667304
Span End Loss: 1.864853394848329
Type Loss: 0.3851726024901425
 ------------ Epoch 2/10 Batch 1400/11616 Training Results ------------ 
Total Loss: 4.250981850549579
Span Start Loss: 2.0068128960473195
Span End Loss: 1.8595061063979352
Type Loss: 0.3846627761357065
 ------------ Epoch 2/10 Batch 1450/11616 Training Results ------------ 
Total Loss: 4.259899246744041
Span Start Loss: 2.0123779098329875
Span End Loss: 1.8634150607215947
Type Loss: 0.38410620361438086
 ------------ Epoch 2/10 Batch 1500/11616 Training Results ------------ 
Total Loss: 4.269081008781989
Span Start Loss: 2.015225729862849
Span End Loss: 1.8672238805691401
Type Loss: 0.3866313246327142
 ------------ Epoch 2/10 Batch 1550/11616 Training Results ------------ 
Total Loss: 4.273470700700437
Span Start Loss: 2.0162489533039833
Span End Loss: 1.8702911676129987
Type Loss: 0.38693050575712995
 ------------ Epoch 2/10 Batch 1600/11616 Training Results ------------ 
Total Loss: 4.277456730650738
Span Start Loss: 2.0185830111056564
Span End Loss: 1.871599717065692
Type Loss: 0.3872739269875456
 ------------ Epoch 2/10 Batch 1650/11616 Training Results ------------ 
Total Loss: 4.2783260251084965
Span Start Loss: 2.0203029250376154
Span End Loss: 1.8709346597483665
Type Loss: 0.38708836530871465
 ------------ Epoch 2/10 Batch 1700/11616 Training Results ------------ 
Total Loss: 4.269135119678343
Span Start Loss: 2.015549644582412
Span End Loss: 1.8671674441940644
Type Loss: 0.38641795719371125
 ------------ Epoch 2/10 Batch 1750/11616 Training Results ------------ 
Total Loss: 4.269793457601752
Span Start Loss: 2.016634081227439
Span End Loss: 1.867084109715053
Type Loss: 0.386075192399323
 ------------ Epoch 2/10 Batch 1800/11616 Training Results ------------ 
Total Loss: 4.273695983779099
Span Start Loss: 2.016342053545846
Span End Loss: 1.8709308130542437
Type Loss: 0.3864230425645494
 ------------ Epoch 2/10 Batch 1850/11616 Training Results ------------ 
Total Loss: 4.270706457262103
Span Start Loss: 2.015813369815414
Span End Loss: 1.8702883377590696
Type Loss: 0.3846046747539092
 ------------ Epoch 2/10 Batch 1900/11616 Training Results ------------ 
Total Loss: 4.265623854532054
Span Start Loss: 2.014426455027179
Span End Loss: 1.8669855043605754
Type Loss: 0.3842118197199153
 ------------ Epoch 2/10 Batch 1950/11616 Training Results ------------ 
Total Loss: 4.260800179648093
Span Start Loss: 2.01143217591139
Span End Loss: 1.8662435632638443
Type Loss: 0.38312436656023446
 ------------ Epoch 2/10 Batch 2000/11616 Training Results ------------ 
Total Loss: 4.254345030598342
Span Start Loss: 2.008153741106391
Span End Loss: 1.863738991573453
Type Loss: 0.38245222370605914
 --------------- Epoch 2/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 53.2, 'f1': 63.5, 'turns': 1425}), ('literature', {'em': 46.9, 'f1': 57.3, 'turns': 1630}), ('mid-high_school', {'em': 49.9, 'f1': 60.4, 'turns': 1653}), ('news', {'em': 51.7, 'f1': 62.9, 'turns': 1649}), ('wikipedia', {'em': 55.1, 'f1': 67.1, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 51.3, 'f1': 62.2, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 51.3, 'f1': 62.2, 'turns': 7983})])
 ------------ Epoch 2/10 Batch 2050/11616 Training Results ------------ 
Total Loss: 4.252223400153765
Span Start Loss: 2.0073011086772126
Span End Loss: 1.8620709978807264
Type Loss: 0.38285121870477024
 ------------ Epoch 2/10 Batch 2100/11616 Training Results ------------ 
Total Loss: 4.249248047847123
Span Start Loss: 2.007231331992717
Span End Loss: 1.8600173131908688
Type Loss: 0.38199932849832946
 ------------ Epoch 2/10 Batch 2150/11616 Training Results ------------ 
Total Loss: 4.253432592784249
Span Start Loss: 2.0085069631837134
Span End Loss: 1.8628576636868854
Type Loss: 0.38206789249944134
 ------------ Epoch 2/10 Batch 2200/11616 Training Results ------------ 
Total Loss: 4.249313126470555
Span Start Loss: 2.0054238902303307
Span End Loss: 1.8607932020046494
Type Loss: 0.3830959616466002
 ------------ Epoch 2/10 Batch 2250/11616 Training Results ------------ 
Total Loss: 4.246752739118206
Span Start Loss: 2.0051931894222896
Span End Loss: 1.8585092542701298
Type Loss: 0.38305022357404234
 ------------ Epoch 2/10 Batch 2300/11616 Training Results ------------ 
Total Loss: 4.247567951322898
Span Start Loss: 2.0060517456350118
Span End Loss: 1.8580838395979093
Type Loss: 0.38343229441782056
 ------------ Epoch 2/10 Batch 2350/11616 Training Results ------------ 
Total Loss: 4.25269074512923
Span Start Loss: 2.0065050534365025
Span End Loss: 1.8621492442678897
Type Loss: 0.3840363752295045
 ------------ Epoch 2/10 Batch 2400/11616 Training Results ------------ 
Total Loss: 4.25524340076372
Span Start Loss: 2.0073517651110886
Span End Loss: 1.8629326486835878
Type Loss: 0.38495891575623925
 ------------ Epoch 2/10 Batch 2450/11616 Training Results ------------ 
Total Loss: 4.257625250153396
Span Start Loss: 2.0089386843053663
Span End Loss: 1.864327885204432
Type Loss: 0.38435861037321845
 ------------ Epoch 2/10 Batch 2500/11616 Training Results ------------ 
Total Loss: 4.259558230370283
Span Start Loss: 2.010625910151005
Span End Loss: 1.864020372581482
Type Loss: 0.38491187740713356
 ------------ Epoch 2/10 Batch 2550/11616 Training Results ------------ 
Total Loss: 4.258760629009966
Span Start Loss: 2.00866653724044
Span End Loss: 1.8650506342626085
Type Loss: 0.3850433868900234
 ------------ Epoch 2/10 Batch 2600/11616 Training Results ------------ 
Total Loss: 4.251251998400459
Span Start Loss: 2.003902164651797
Span End Loss: 1.8624109690005963
Type Loss: 0.3849387940363242
 ------------ Epoch 2/10 Batch 2650/11616 Training Results ------------ 
Total Loss: 4.2499515556783045
Span Start Loss: 2.0035912160603506
Span End Loss: 1.8613988377013297
Type Loss: 0.38496143100818375
 ------------ Epoch 2/10 Batch 2700/11616 Training Results ------------ 
Total Loss: 4.240921916205574
Span Start Loss: 1.999860825339953
Span End Loss: 1.856970632981371
Type Loss: 0.3840903869740389
 ------------ Epoch 2/10 Batch 2750/11616 Training Results ------------ 
Total Loss: 4.243440490337935
Span Start Loss: 2.001420554702932
Span End Loss: 1.8579984883828597
Type Loss: 0.384021376513622
 ------------ Epoch 2/10 Batch 2800/11616 Training Results ------------ 
Total Loss: 4.238849304350359
Span Start Loss: 2.0002692737962517
Span End Loss: 1.8558019680529834
Type Loss: 0.38277799235789906
 ------------ Epoch 2/10 Batch 2850/11616 Training Results ------------ 
Total Loss: 4.237035339129599
Span Start Loss: 1.9971481911759628
Span End Loss: 1.8580137433817512
Type Loss: 0.3818733343736906
 ------------ Epoch 2/10 Batch 2900/11616 Training Results ------------ 
Total Loss: 4.23480645664807
Span Start Loss: 1.99646770267651
Span End Loss: 1.856728118586129
Type Loss: 0.38161056612521926
 ------------ Epoch 2/10 Batch 2950/11616 Training Results ------------ 
Total Loss: 4.233343760138851
Span Start Loss: 1.9962083610437684
Span End Loss: 1.8557593309879303
Type Loss: 0.38137599875361233
 ------------ Epoch 2/10 Batch 3000/11616 Training Results ------------ 
Total Loss: 4.236165862033764
Span Start Loss: 1.9970546329220136
Span End Loss: 1.8576969099442164
Type Loss: 0.3814142499112835
 ------------ Epoch 2/10 Batch 3050/11616 Training Results ------------ 
Total Loss: 4.231214712047186
Span Start Loss: 1.9935629988009813
Span End Loss: 1.856361659276681
Type Loss: 0.3812899849411161
 ------------ Epoch 2/10 Batch 3100/11616 Training Results ------------ 
Total Loss: 4.229167375689553
Span Start Loss: 1.9927296852969354
Span End Loss: 1.8540106686853592
Type Loss: 0.38242695295582374
 ------------ Epoch 2/10 Batch 3150/11616 Training Results ------------ 
Total Loss: 4.225611669392813
Span Start Loss: 1.9910709734095466
Span End Loss: 1.8521354134120638
Type Loss: 0.3824052143339363
 ------------ Epoch 2/10 Batch 3200/11616 Training Results ------------ 
Total Loss: 4.226051655737683
Span Start Loss: 1.9902503501530737
Span End Loss: 1.8532906309515238
Type Loss: 0.3825106069323374
 ------------ Epoch 2/10 Batch 3250/11616 Training Results ------------ 
Total Loss: 4.231135888191369
Span Start Loss: 1.9934967132990178
Span End Loss: 1.8558460495655353
Type Loss: 0.3817930576130748
 ------------ Epoch 2/10 Batch 3300/11616 Training Results ------------ 
Total Loss: 4.231962358138778
Span Start Loss: 1.9935534222107945
Span End Loss: 1.85618487786163
Type Loss: 0.38222398995280715
 ------------ Epoch 2/10 Batch 3350/11616 Training Results ------------ 
Total Loss: 4.234138163301482
Span Start Loss: 1.9950571204388319
Span End Loss: 1.8566101642864854
Type Loss: 0.3824708100725243
 ------------ Epoch 2/10 Batch 3400/11616 Training Results ------------ 
Total Loss: 4.231413617642486
Span Start Loss: 1.9948623881708174
Span End Loss: 1.8545475863796823
Type Loss: 0.3820035741516553
 ------------ Epoch 2/10 Batch 3450/11616 Training Results ------------ 
Total Loss: 4.227041555120461
Span Start Loss: 1.9918713581648426
Span End Loss: 1.853060492631318
Type Loss: 0.38210963583888785
 ------------ Epoch 2/10 Batch 3500/11616 Training Results ------------ 
Total Loss: 4.228641866977726
Span Start Loss: 1.9915485696707453
Span End Loss: 1.8542689875449454
Type Loss: 0.3828242405892483
 ------------ Epoch 2/10 Batch 3550/11616 Training Results ------------ 
Total Loss: 4.229274187814182
Span Start Loss: 1.9913870965678926
Span End Loss: 1.8551039769867776
Type Loss: 0.38278304509346334
 ------------ Epoch 2/10 Batch 3600/11616 Training Results ------------ 
Total Loss: 4.223292129673064
Span Start Loss: 1.9889414463192225
Span End Loss: 1.851201572658287
Type Loss: 0.3831490419334215
 ------------ Epoch 2/10 Batch 3650/11616 Training Results ------------ 
Total Loss: 4.22389760521585
Span Start Loss: 1.988527120604907
Span End Loss: 1.8520724504042978
Type Loss: 0.3832979659047233
 ------------ Epoch 2/10 Batch 3700/11616 Training Results ------------ 
Total Loss: 4.22421440729821
Span Start Loss: 1.989252126579349
Span End Loss: 1.8515882128557644
Type Loss: 0.3833739991148783
 ------------ Epoch 2/10 Batch 3750/11616 Training Results ------------ 
Total Loss: 4.225079048534234
Span Start Loss: 1.990324860930443
Span End Loss: 1.8510212686777114
Type Loss: 0.38373284982393185
 ------------ Epoch 2/10 Batch 3800/11616 Training Results ------------ 
Total Loss: 4.227038336623656
Span Start Loss: 1.991085171338759
Span End Loss: 1.8519826678304296
Type Loss: 0.38397042866330594
 ------------ Epoch 2/10 Batch 3850/11616 Training Results ------------ 
Total Loss: 4.22699167701331
Span Start Loss: 1.9902672297149508
Span End Loss: 1.8525116947873845
Type Loss: 0.38421268351351867
 ------------ Epoch 2/10 Batch 3900/11616 Training Results ------------ 
Total Loss: 4.230368308837598
Span Start Loss: 1.99184847256312
Span End Loss: 1.8546228477282403
Type Loss: 0.3838969193979238
 ------------ Epoch 2/10 Batch 3950/11616 Training Results ------------ 
Total Loss: 4.232648470563979
Span Start Loss: 1.9923103914155236
Span End Loss: 1.8564221500520466
Type Loss: 0.3839158598859382
 ------------ Epoch 2/10 Batch 4000/11616 Training Results ------------ 
Total Loss: 4.2319888546876605
Span Start Loss: 1.9922502005770804
Span End Loss: 1.8558414436131716
Type Loss: 0.38389714078395626
 --------------- Epoch 2/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 53.4, 'f1': 64.1, 'turns': 1425}), ('literature', {'em': 48.8, 'f1': 58.9, 'turns': 1630}), ('mid-high_school', {'em': 48.8, 'f1': 60.3, 'turns': 1653}), ('news', {'em': 54.2, 'f1': 64.7, 'turns': 1649}), ('wikipedia', {'em': 56.6, 'f1': 67.6, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 52.3, 'f1': 63.1, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 52.3, 'f1': 63.1, 'turns': 7983})])
 ------------ Epoch 2/10 Batch 4050/11616 Training Results ------------ 
Total Loss: 4.230268822426413
Span Start Loss: 1.9913943384532575
Span End Loss: 1.8545672959162864
Type Loss: 0.38430711832426395
 ------------ Epoch 2/10 Batch 4100/11616 Training Results ------------ 
Total Loss: 4.223805759538237
Span Start Loss: 1.987566255040285
Span End Loss: 1.851926171547029
Type Loss: 0.38431326320190434
 ------------ Epoch 2/10 Batch 4150/11616 Training Results ------------ 
Total Loss: 4.224457691573953
Span Start Loss: 1.9877065048734825
Span End Loss: 1.8521990927946137
Type Loss: 0.3845520242503339
 ------------ Epoch 2/10 Batch 4200/11616 Training Results ------------ 
Total Loss: 4.217656904228386
Span Start Loss: 1.9842800563006173
Span End Loss: 1.8490050140803769
Type Loss: 0.38437176434788856
 ------------ Epoch 2/10 Batch 4250/11616 Training Results ------------ 
Total Loss: 4.215776351672762
Span Start Loss: 1.9825279934546527
Span End Loss: 1.8487737746729571
Type Loss: 0.38447451421114454
 ------------ Epoch 2/10 Batch 4300/11616 Training Results ------------ 
Total Loss: 4.218198694306057
Span Start Loss: 1.9831846305520036
Span End Loss: 1.8497563404953756
Type Loss: 0.3852576539962184
 ------------ Epoch 2/10 Batch 4350/11616 Training Results ------------ 
Total Loss: 4.219839218070452
Span Start Loss: 1.9837010242062054
Span End Loss: 1.851261444612481
Type Loss: 0.3848766797260735
 ------------ Epoch 2/10 Batch 4400/11616 Training Results ------------ 
Total Loss: 4.2198870723389765
Span Start Loss: 1.9838404433158312
Span End Loss: 1.8508849555389448
Type Loss: 0.3851616041279737
 ------------ Epoch 2/10 Batch 4450/11616 Training Results ------------ 
Total Loss: 4.216697209420499
Span Start Loss: 1.982087640494443
Span End Loss: 1.8494491920444402
Type Loss: 0.3851603076554667
 ------------ Epoch 2/10 Batch 4500/11616 Training Results ------------ 
Total Loss: 4.218703305307362
Span Start Loss: 1.9837574561569427
Span End Loss: 1.8495894650618234
Type Loss: 0.38535631509187324
 ------------ Epoch 2/10 Batch 4550/11616 Training Results ------------ 
Total Loss: 4.219406337446564
Span Start Loss: 1.984033423609786
Span End Loss: 1.8500260475114152
Type Loss: 0.38534679771677316
 ------------ Epoch 2/10 Batch 4600/11616 Training Results ------------ 
Total Loss: 4.218799931863728
Span Start Loss: 1.9837205224970114
Span End Loss: 1.8494359786678916
Type Loss: 0.3856433622496527
 ------------ Epoch 2/10 Batch 4650/11616 Training Results ------------ 
Total Loss: 4.215149822430585
Span Start Loss: 1.981487318469632
Span End Loss: 1.8482588277901373
Type Loss: 0.3854036075451602
 ------------ Epoch 2/10 Batch 4700/11616 Training Results ------------ 
Total Loss: 4.21258416139382
Span Start Loss: 1.9802888327710173
Span End Loss: 1.8469117858245017
Type Loss: 0.3853834740855196
 ------------ Epoch 2/10 Batch 4750/11616 Training Results ------------ 
Total Loss: 4.214922900780251
Span Start Loss: 1.9823956565480483
Span End Loss: 1.847355311073755
Type Loss: 0.3851718644089997
 ------------ Epoch 2/10 Batch 4800/11616 Training Results ------------ 
Total Loss: 4.212245012276496
Span Start Loss: 1.9811355973904332
Span End Loss: 1.8460514006453257
Type Loss: 0.38505794536011917
 ------------ Epoch 2/10 Batch 4850/11616 Training Results ------------ 
Total Loss: 4.210248089466512
Span Start Loss: 1.9800757224535204
Span End Loss: 1.845423016087296
Type Loss: 0.38474928231506617
 ------------ Epoch 2/10 Batch 4900/11616 Training Results ------------ 
Total Loss: 4.210441996081143
Span Start Loss: 1.9793022348442857
Span End Loss: 1.8463740826382928
Type Loss: 0.38476561028206224
 ------------ Epoch 2/10 Batch 4950/11616 Training Results ------------ 
Total Loss: 4.209013263158726
Span Start Loss: 1.979056725044443
Span End Loss: 1.8455451666104674
Type Loss: 0.38441130304050564
 ------------ Epoch 2/10 Batch 5000/11616 Training Results ------------ 
Total Loss: 4.2089758163839575
Span Start Loss: 1.9792019205927849
Span End Loss: 1.8450329938471317
Type Loss: 0.384740833863616
 ------------ Epoch 2/10 Batch 5050/11616 Training Results ------------ 
Total Loss: 4.209933436152959
Span Start Loss: 1.9800933984895743
Span End Loss: 1.8449726856344997
Type Loss: 0.3848672838634489
 ------------ Epoch 2/10 Batch 5100/11616 Training Results ------------ 
Total Loss: 4.2143444197727185
Span Start Loss: 1.9822573147626483
Span End Loss: 1.8469764498051475
Type Loss: 0.3851105871851392
 ------------ Epoch 2/10 Batch 5150/11616 Training Results ------------ 
Total Loss: 4.214086854139578
Span Start Loss: 1.9823855155474932
Span End Loss: 1.8466274031157632
Type Loss: 0.38507386732412485
 ------------ Epoch 2/10 Batch 5200/11616 Training Results ------------ 
Total Loss: 4.21423764819136
Span Start Loss: 1.9833266683667898
Span End Loss: 1.8463202089816333
Type Loss: 0.3845907023165805
 ------------ Epoch 2/10 Batch 5250/11616 Training Results ------------ 
Total Loss: 4.212601947159994
Span Start Loss: 1.9821677243709563
Span End Loss: 1.846073649514289
Type Loss: 0.3843605044948913
 ------------ Epoch 2/10 Batch 5300/11616 Training Results ------------ 
Total Loss: 4.210686426314543
Span Start Loss: 1.9810003407158943
Span End Loss: 1.8452583112975336
Type Loss: 0.3844277054988691
 ------------ Epoch 2/10 Batch 5350/11616 Training Results ------------ 
Total Loss: 4.208894536261247
Span Start Loss: 1.9809314645896448
Span End Loss: 1.8437087302842987
Type Loss: 0.38425427297286896
 ------------ Epoch 2/10 Batch 5400/11616 Training Results ------------ 
Total Loss: 4.210644753028949
Span Start Loss: 1.981520182567614
Span End Loss: 1.8444802664109954
Type Loss: 0.3846442352442278
 ------------ Epoch 2/10 Batch 5450/11616 Training Results ------------ 
Total Loss: 4.209721599669631
Span Start Loss: 1.9811243660078137
Span End Loss: 1.8436673601559543
Type Loss: 0.3849298048402191
 ------------ Epoch 2/10 Batch 5500/11616 Training Results ------------ 
Total Loss: 4.20813146209175
Span Start Loss: 1.98021656304056
Span End Loss: 1.8436215409961614
Type Loss: 0.3842932896319438
 ------------ Epoch 2/10 Batch 5550/11616 Training Results ------------ 
Total Loss: 4.206784196694692
Span Start Loss: 1.9796057902585278
Span End Loss: 1.843176694496258
Type Loss: 0.38400164341302334
 ------------ Epoch 2/10 Batch 5600/11616 Training Results ------------ 
Total Loss: 4.206964183066573
Span Start Loss: 1.9800414613315038
Span End Loss: 1.842833070643246
Type Loss: 0.3840895820152946
 ------------ Epoch 2/10 Batch 5650/11616 Training Results ------------ 
Total Loss: 4.203085529092139
Span Start Loss: 1.9785246254596036
Span End Loss: 1.8411969040769391
Type Loss: 0.3833639302968451
 ------------ Epoch 2/10 Batch 5700/11616 Training Results ------------ 
Total Loss: 4.201989823331435
Span Start Loss: 1.9778923767334537
Span End Loss: 1.8411055594944117
Type Loss: 0.3829918176095867
 ------------ Epoch 2/10 Batch 5750/11616 Training Results ------------ 
Total Loss: 4.202810540367728
Span Start Loss: 1.9785961344086607
Span End Loss: 1.8412407072575196
Type Loss: 0.3829736292967978
 ------------ Epoch 2/10 Batch 5800/11616 Training Results ------------ 
Total Loss: 4.200031243483054
Span Start Loss: 1.9768858177137787
Span End Loss: 1.8402683219365006
Type Loss: 0.3828770345253549
 ------------ Epoch 2/10 Batch 5850/11616 Training Results ------------ 
Total Loss: 4.2001149308732435
Span Start Loss: 1.9766623333631417
Span End Loss: 1.8407073128783804
Type Loss: 0.38274521524357236
 ------------ Epoch 2/10 Batch 5900/11616 Training Results ------------ 
Total Loss: 4.201511229069556
Span Start Loss: 1.9773347286955785
Span End Loss: 1.8413722103480565
Type Loss: 0.3828042204491794
 ------------ Epoch 2/10 Batch 5950/11616 Training Results ------------ 
Total Loss: 4.200080157177789
Span Start Loss: 1.977283701300621
Span End Loss: 1.840401915416998
Type Loss: 0.38239447086799044
 ------------ Epoch 2/10 Batch 6000/11616 Training Results ------------ 
Total Loss: 4.1996807280927895
Span Start Loss: 1.9781650001456341
Span End Loss: 1.8391686222900947
Type Loss: 0.3823470359084507
 --------------- Epoch 2/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 52.3, 'f1': 63.6, 'turns': 1425}), ('literature', {'em': 48.9, 'f1': 58.7, 'turns': 1630}), ('mid-high_school', {'em': 50.1, 'f1': 61.4, 'turns': 1653}), ('news', {'em': 53.3, 'f1': 64.7, 'turns': 1649}), ('wikipedia', {'em': 57.1, 'f1': 68.1, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 52.3, 'f1': 63.3, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 52.3, 'f1': 63.3, 'turns': 7983})])
 ------------ Epoch 2/10 Batch 6050/11616 Training Results ------------ 
Total Loss: 4.201540020921014
Span Start Loss: 1.978473194596196
Span End Loss: 1.840554676962293
Type Loss: 0.3825120798470683
 ------------ Epoch 2/10 Batch 6100/11616 Training Results ------------ 
Total Loss: 4.197967552308176
Span Start Loss: 1.9769784193009627
Span End Loss: 1.8386038396593
Type Loss: 0.3823852244123328
 ------------ Epoch 2/10 Batch 6150/11616 Training Results ------------ 
Total Loss: 4.200696229450101
Span Start Loss: 1.9786593837902797
Span End Loss: 1.8394887706806988
Type Loss: 0.3825480058613589
 ------------ Epoch 2/10 Batch 6200/11616 Training Results ------------ 
Total Loss: 4.201789567283084
Span Start Loss: 1.9795053216382381
Span End Loss: 1.839884658609667
Type Loss: 0.38239951772284847
 ------------ Epoch 2/10 Batch 6250/11616 Training Results ------------ 
Total Loss: 4.199894771099091
Span Start Loss: 1.9782750671052933
Span End Loss: 1.8393555310964584
Type Loss: 0.3822641033455729
 ------------ Epoch 2/10 Batch 6300/11616 Training Results ------------ 
Total Loss: 4.194698657261001
Span Start Loss: 1.9757132939166493
Span End Loss: 1.8367677319948637
Type Loss: 0.3822175618877546
 ------------ Epoch 2/10 Batch 6350/11616 Training Results ------------ 
Total Loss: 4.194794751293077
Span Start Loss: 1.9761714864481152
Span End Loss: 1.83645236087127
Type Loss: 0.3821708343047502
 ------------ Epoch 2/10 Batch 6400/11616 Training Results ------------ 
Total Loss: 4.196062574465759
Span Start Loss: 1.9772903450531885
Span End Loss: 1.8366687770141288
Type Loss: 0.3821033824868209
 ------------ Epoch 2/10 Batch 6450/11616 Training Results ------------ 
Total Loss: 4.193377103662306
Span Start Loss: 1.9765160440982774
Span End Loss: 1.834711496330971
Type Loss: 0.38214949321550457
 ------------ Epoch 2/10 Batch 6500/11616 Training Results ------------ 
Total Loss: 4.195639048837698
Span Start Loss: 1.9771617766939678
Span End Loss: 1.8361335434638537
Type Loss: 0.38234365832232514
 ------------ Epoch 2/10 Batch 6550/11616 Training Results ------------ 
Total Loss: 4.19523065845475
Span Start Loss: 1.9771774546017173
Span End Loss: 1.835890392765744
Type Loss: 0.38216274037900316
 ------------ Epoch 2/10 Batch 6600/11616 Training Results ------------ 
Total Loss: 4.193310893823703
Span Start Loss: 1.975939810163144
Span End Loss: 1.835452466015563
Type Loss: 0.38191854693729316
 ------------ Epoch 2/10 Batch 6650/11616 Training Results ------------ 
Total Loss: 4.191940519392938
Span Start Loss: 1.9746441757813433
Span End Loss: 1.835486137656341
Type Loss: 0.3818101355696755
 ------------ Epoch 2/10 Batch 6700/11616 Training Results ------------ 
Total Loss: 4.191081992281017
Span Start Loss: 1.9741633917280097
Span End Loss: 1.8347110474332056
Type Loss: 0.38220748272126737
 ------------ Epoch 2/10 Batch 6750/11616 Training Results ------------ 
Total Loss: 4.1933311183143545
Span Start Loss: 1.9754174351206533
Span End Loss: 1.835457866553907
Type Loss: 0.3824557459059137
 ------------ Epoch 2/10 Batch 6800/11616 Training Results ------------ 
Total Loss: 4.1936539737398135
Span Start Loss: 1.9753755738498533
Span End Loss: 1.8360562326715273
Type Loss: 0.38222209659158524
 ------------ Epoch 2/10 Batch 6850/11616 Training Results ------------ 
Total Loss: 4.1943132729643455
Span Start Loss: 1.975805007345485
Span End Loss: 1.8364325580370686
Type Loss: 0.3820756369494718
 ------------ Epoch 2/10 Batch 6900/11616 Training Results ------------ 
Total Loss: 4.195878230510415
Span Start Loss: 1.9770835980513821
Span End Loss: 1.837100428627885
Type Loss: 0.38169413338671776
 ------------ Epoch 2/10 Batch 6950/11616 Training Results ------------ 
Total Loss: 4.201593902171087
Span Start Loss: 1.98021524805388
Span End Loss: 1.8390388602061238
Type Loss: 0.3823397234626275
 ------------ Epoch 2/10 Batch 7000/11616 Training Results ------------ 
Total Loss: 4.2003921691434725
Span Start Loss: 1.9797774510085582
Span End Loss: 1.8384620463337218
Type Loss: 0.38215260152811453
 ------------ Epoch 2/10 Batch 7050/11616 Training Results ------------ 
Total Loss: 4.19943014442498
Span Start Loss: 1.9795480844188245
Span End Loss: 1.8381561021644173
Type Loss: 0.3817258875748367
 ------------ Epoch 2/10 Batch 7100/11616 Training Results ------------ 
Total Loss: 4.201706572380704
Span Start Loss: 1.9808137388002705
Span End Loss: 1.8392066426889997
Type Loss: 0.38168612073660946
 ------------ Epoch 2/10 Batch 7150/11616 Training Results ------------ 
Total Loss: 4.199573072770259
Span Start Loss: 1.9794523934426007
Span End Loss: 1.8384296372970501
Type Loss: 0.38169097159109955
 ------------ Epoch 2/10 Batch 7200/11616 Training Results ------------ 
Total Loss: 4.197274336674147
Span Start Loss: 1.978786140949362
Span End Loss: 1.8371674644947051
Type Loss: 0.3813206607649206
 ------------ Epoch 2/10 Batch 7250/11616 Training Results ------------ 
Total Loss: 4.194758128934893
Span Start Loss: 1.9774105748349222
Span End Loss: 1.8360759726886091
Type Loss: 0.3812715112703114
 ------------ Epoch 2/10 Batch 7300/11616 Training Results ------------ 
Total Loss: 4.193129231403136
Span Start Loss: 1.9770917450687657
Span End Loss: 1.8353371027724383
Type Loss: 0.38070031326085535
 ------------ Epoch 2/10 Batch 7350/11616 Training Results ------------ 
Total Loss: 4.1914418900702275
Span Start Loss: 1.9765719980812397
Span End Loss: 1.8343198689149351
Type Loss: 0.3805499530164208
 ------------ Epoch 2/10 Batch 7400/11616 Training Results ------------ 
Total Loss: 4.192066957197479
Span Start Loss: 1.9765378349979181
Span End Loss: 1.8350741648633737
Type Loss: 0.38045488721930193
 ------------ Epoch 2/10 Batch 7450/11616 Training Results ------------ 
Total Loss: 4.191116233520059
Span Start Loss: 1.9756777504106495
Span End Loss: 1.8348169200452382
Type Loss: 0.3806214930663423
 ------------ Epoch 2/10 Batch 7500/11616 Training Results ------------ 
Total Loss: 4.190583502729734
Span Start Loss: 1.9754329145789147
Span End Loss: 1.8345643292705218
Type Loss: 0.38058618905556696
 ------------ Epoch 2/10 Batch 7550/11616 Training Results ------------ 
Total Loss: 4.188735568020123
Span Start Loss: 1.9745565410718222
Span End Loss: 1.833328156838354
Type Loss: 0.38085080025618934
 ------------ Epoch 2/10 Batch 7600/11616 Training Results ------------ 
Total Loss: 4.184852292927864
Span Start Loss: 1.97244457533485
Span End Loss: 1.8315066381426235
Type Loss: 0.3809010094876009
 ------------ Epoch 2/10 Batch 7650/11616 Training Results ------------ 
Total Loss: 4.183699097128865
Span Start Loss: 1.971868908319598
Span End Loss: 1.8310864847002466
Type Loss: 0.38074363408050216
 ------------ Epoch 2/10 Batch 7700/11616 Training Results ------------ 
Total Loss: 4.183580955350167
Span Start Loss: 1.9718324149080686
Span End Loss: 1.8312180393702024
Type Loss: 0.3805304310735702
 ------------ Epoch 2/10 Batch 7750/11616 Training Results ------------ 
Total Loss: 4.183551747135578
Span Start Loss: 1.9719581208728976
Span End Loss: 1.8307826799346554
Type Loss: 0.38081087631600036
 ------------ Epoch 2/10 Batch 7800/11616 Training Results ------------ 
Total Loss: 4.181939654577619
Span Start Loss: 1.9711810750036667
Span End Loss: 1.8301491210017449
Type Loss: 0.38060938824959195
 ------------ Epoch 2/10 Batch 7850/11616 Training Results ------------ 
Total Loss: 4.181426280594556
Span Start Loss: 1.971410250508102
Span End Loss: 1.8297692159482628
Type Loss: 0.38024674379011725
 ------------ Epoch 2/10 Batch 7900/11616 Training Results ------------ 
Total Loss: 4.1789464272370065
Span Start Loss: 1.9702061715759809
Span End Loss: 1.8285757264457172
Type Loss: 0.3801644590584255
 ------------ Epoch 2/10 Batch 7950/11616 Training Results ------------ 
Total Loss: 4.176286422709624
Span Start Loss: 1.9689333906788495
Span End Loss: 1.8274640975208403
Type Loss: 0.37988886421534623
 ------------ Epoch 2/10 Batch 8000/11616 Training Results ------------ 
Total Loss: 4.174414648078382
Span Start Loss: 1.9680142520293593
Span End Loss: 1.8266239229552448
Type Loss: 0.3797764029134996
 --------------- Epoch 2/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 54.8, 'f1': 64.3, 'turns': 1425}), ('literature', {'em': 52.3, 'f1': 61.4, 'turns': 1630}), ('mid-high_school', {'em': 51.0, 'f1': 61.7, 'turns': 1653}), ('news', {'em': 54.4, 'f1': 64.7, 'turns': 1649}), ('wikipedia', {'em': 57.3, 'f1': 67.8, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 53.9, 'f1': 64.0, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 53.9, 'f1': 64.0, 'turns': 7983})])
 ------------ Epoch 2/10 Batch 8050/11616 Training Results ------------ 
Total Loss: 4.173489415793686
Span Start Loss: 1.9673412469084959
Span End Loss: 1.8262689915577077
Type Loss: 0.3798791072464415
 ------------ Epoch 2/10 Batch 8100/11616 Training Results ------------ 
Total Loss: 4.173408044322773
Span Start Loss: 1.967048965986864
Span End Loss: 1.8263900808456504
Type Loss: 0.37996892741225935
 ------------ Epoch 2/10 Batch 8150/11616 Training Results ------------ 
Total Loss: 4.173678501178882
Span Start Loss: 1.9666992079992236
Span End Loss: 1.826843988335937
Type Loss: 0.38013523487294193
 ------------ Epoch 2/10 Batch 8200/11616 Training Results ------------ 
Total Loss: 4.172513344411443
Span Start Loss: 1.9657594533846146
Span End Loss: 1.826765044902156
Type Loss: 0.37998877595723946
 ------------ Epoch 2/10 Batch 8250/11616 Training Results ------------ 
Total Loss: 4.172897256359909
Span Start Loss: 1.9663090818831415
Span End Loss: 1.826664704879125
Type Loss: 0.3799233992409074
 ------------ Epoch 2/10 Batch 8300/11616 Training Results ------------ 
Total Loss: 4.173776228214244
Span Start Loss: 1.9671894215352563
Span End Loss: 1.8269591513455632
Type Loss: 0.37962758504325544
 ------------ Epoch 2/10 Batch 8350/11616 Training Results ------------ 
Total Loss: 4.173200732888933
Span Start Loss: 1.9664706062949346
Span End Loss: 1.827145354562177
Type Loss: 0.37958470205458517
 ------------ Epoch 2/10 Batch 8400/11616 Training Results ------------ 
Total Loss: 4.173319502112766
Span Start Loss: 1.9666780124533745
Span End Loss: 1.8272795216171513
Type Loss: 0.3793618977457906
 ------------ Epoch 2/10 Batch 8450/11616 Training Results ------------ 
Total Loss: 4.17382521457397
Span Start Loss: 1.9671964855659643
Span End Loss: 1.8274563770534018
Type Loss: 0.37917228164603195
 ------------ Epoch 2/10 Batch 8500/11616 Training Results ------------ 
Total Loss: 4.173708469788818
Span Start Loss: 1.9669260729411069
Span End Loss: 1.8273698836144279
Type Loss: 0.37941244300871213
 ------------ Epoch 2/10 Batch 8550/11616 Training Results ------------ 
Total Loss: 4.173144251796586
Span Start Loss: 1.9667441648628279
Span End Loss: 1.8270179525732297
Type Loss: 0.37938206398630875
 ------------ Epoch 2/10 Batch 8600/11616 Training Results ------------ 
Total Loss: 4.1738363649349575
Span Start Loss: 1.9667365277021431
Span End Loss: 1.8276607511348502
Type Loss: 0.3794390156784983
 ------------ Epoch 2/10 Batch 8650/11616 Training Results ------------ 
Total Loss: 4.171573749847839
Span Start Loss: 1.9654658103161464
Span End Loss: 1.827102447551799
Type Loss: 0.37900542185227304
 ------------ Epoch 2/10 Batch 8700/11616 Training Results ------------ 
Total Loss: 4.168439511826326
Span Start Loss: 1.9642397224594808
Span End Loss: 1.8255321101450372
Type Loss: 0.37866760906153196
 ------------ Epoch 2/10 Batch 8750/11616 Training Results ------------ 
Total Loss: 4.1685191764065195
Span Start Loss: 1.963999466095652
Span End Loss: 1.8256266801800047
Type Loss: 0.37889295984730126
 ------------ Epoch 2/10 Batch 8800/11616 Training Results ------------ 
Total Loss: 4.1684566474350335
Span Start Loss: 1.9640516687557101
Span End Loss: 1.825904688188298
Type Loss: 0.3785002204577904
 ------------ Epoch 2/10 Batch 8850/11616 Training Results ------------ 
Total Loss: 4.168334243534985
Span Start Loss: 1.9637455523115093
Span End Loss: 1.8259809893302326
Type Loss: 0.3786076318328453
 ------------ Epoch 2/10 Batch 8900/11616 Training Results ------------ 
Total Loss: 4.16734852741441
Span Start Loss: 1.9632802853791902
Span End Loss: 1.8255133586887564
Type Loss: 0.3785548131932829
 ------------ Epoch 2/10 Batch 8950/11616 Training Results ------------ 
Total Loss: 4.167001016321462
Span Start Loss: 1.963231375007656
Span End Loss: 1.825323056685192
Type Loss: 0.3784465144674538
 ------------ Epoch 2/10 Batch 9000/11616 Training Results ------------ 
Total Loss: 4.166954225872954
Span Start Loss: 1.9630874792105621
Span End Loss: 1.8252666656341818
Type Loss: 0.3786000109174185
 ------------ Epoch 2/10 Batch 9050/11616 Training Results ------------ 
Total Loss: 4.164910566847957
Span Start Loss: 1.9621002584037202
Span End Loss: 1.8244533128204925
Type Loss: 0.378356925321959
 ------------ Epoch 2/10 Batch 9100/11616 Training Results ------------ 
Total Loss: 4.163565768366986
Span Start Loss: 1.961552790950288
Span End Loss: 1.8239565617861329
Type Loss: 0.3780563452907398
 ------------ Epoch 2/10 Batch 9150/11616 Training Results ------------ 
Total Loss: 4.162733824086971
Span Start Loss: 1.9612911299897022
Span End Loss: 1.8236122982716951
Type Loss: 0.3778303253404348
 ------------ Epoch 2/10 Batch 9200/11616 Training Results ------------ 
Total Loss: 4.1630575091132656
Span Start Loss: 1.9612164872852356
Span End Loss: 1.8236666118353606
Type Loss: 0.3781743393224947
 ------------ Epoch 2/10 Batch 9250/11616 Training Results ------------ 
Total Loss: 4.161892078470539
Span Start Loss: 1.9608661507754712
Span End Loss: 1.8228442534691578
Type Loss: 0.37818160352775376
 ------------ Epoch 2/10 Batch 9300/11616 Training Results ------------ 
Total Loss: 4.16250371162289
Span Start Loss: 1.9610331784813635
Span End Loss: 1.8232046998059879
Type Loss: 0.3782657621578584
 ------------ Epoch 2/10 Batch 9350/11616 Training Results ------------ 
Total Loss: 4.159899853847244
Span Start Loss: 1.9597631846551589
Span End Loss: 1.8221889398697226
Type Loss: 0.3779476582145388
 ------------ Epoch 2/10 Batch 9400/11616 Training Results ------------ 
Total Loss: 4.159814106879082
Span Start Loss: 1.9598062228680926
Span End Loss: 1.8219051761481355
Type Loss: 0.37810263695492546
 ------------ Epoch 2/10 Batch 9450/11616 Training Results ------------ 
Total Loss: 4.159227201042983
Span Start Loss: 1.9591046127156606
Span End Loss: 1.821958677421802
Type Loss: 0.37816384007612236
 ------------ Epoch 2/10 Batch 9500/11616 Training Results ------------ 
Total Loss: 4.1579001919062515
Span Start Loss: 1.9581409418488804
Span End Loss: 1.821397060058619
Type Loss: 0.3783621192770569
 ------------ Epoch 2/10 Batch 9550/11616 Training Results ------------ 
Total Loss: 4.157358458903447
Span Start Loss: 1.9577566545078267
Span End Loss: 1.8213650827726144
Type Loss: 0.3782366510449436
 ------------ Epoch 2/10 Batch 9600/11616 Training Results ------------ 
Total Loss: 4.157960048985357
Span Start Loss: 1.9582441783044486
Span End Loss: 1.8215280386588226
Type Loss: 0.37818776138320875
 ------------ Epoch 2/10 Batch 9650/11616 Training Results ------------ 
Total Loss: 4.158418741451644
Span Start Loss: 1.9583225697958409
Span End Loss: 1.8216644648165283
Type Loss: 0.3784316361828712
 ------------ Epoch 2/10 Batch 9700/11616 Training Results ------------ 
Total Loss: 4.158322229182597
Span Start Loss: 1.9580915884105201
Span End Loss: 1.8217410566972703
Type Loss: 0.37848951356562294
 ------------ Epoch 2/10 Batch 9750/11616 Training Results ------------ 
Total Loss: 4.157462601310168
Span Start Loss: 1.9580039569628545
Span End Loss: 1.8212363537855638
Type Loss: 0.3782222200977879
 ------------ Epoch 2/10 Batch 9800/11616 Training Results ------------ 
Total Loss: 4.158074906361954
Span Start Loss: 1.9583062317997826
Span End Loss: 1.8214196462321037
Type Loss: 0.37834895777858185
 ------------ Epoch 2/10 Batch 9850/11616 Training Results ------------ 
Total Loss: 4.158209459476059
Span Start Loss: 1.9583780142014402
Span End Loss: 1.8211330163206545
Type Loss: 0.3786983581358179
 ------------ Epoch 2/10 Batch 9900/11616 Training Results ------------ 
Total Loss: 4.158295499683931
Span Start Loss: 1.9584874093111115
Span End Loss: 1.8212168561298439
Type Loss: 0.3785911635781704
 ------------ Epoch 2/10 Batch 9950/11616 Training Results ------------ 
Total Loss: 4.1576305297226765
Span Start Loss: 1.9582030342391987
Span End Loss: 1.8209904705010467
Type Loss: 0.3784369541710271
 ------------ Epoch 2/10 Batch 10000/11616 Training Results ------------ 
Total Loss: 4.155604133300483
Span Start Loss: 1.9574418197274208
Span End Loss: 1.8198710242658853
Type Loss: 0.37829121856149284
 --------------- Epoch 2/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 53.9, 'f1': 65.3, 'turns': 1425}), ('literature', {'em': 52.6, 'f1': 63.0, 'turns': 1630}), ('mid-high_school', {'em': 51.8, 'f1': 62.9, 'turns': 1653}), ('news', {'em': 56.9, 'f1': 67.0, 'turns': 1649}), ('wikipedia', {'em': 59.0, 'f1': 69.3, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 54.9, 'f1': 65.5, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 54.9, 'f1': 65.5, 'turns': 7983})])
 ------------ Epoch 2/10 Batch 10050/11616 Training Results ------------ 
Total Loss: 4.155303467869462
Span Start Loss: 1.957392824401903
Span End Loss: 1.8196236328669448
Type Loss: 0.37828694002341423
 ------------ Epoch 2/10 Batch 10100/11616 Training Results ------------ 
Total Loss: 4.155858585821225
Span Start Loss: 1.9577320986691087
Span End Loss: 1.8198614955154977
Type Loss: 0.3782649211698018
 ------------ Epoch 2/10 Batch 10150/11616 Training Results ------------ 
Total Loss: 4.1583100865113325
Span Start Loss: 1.9590549727848598
Span End Loss: 1.821154720322252
Type Loss: 0.3781003227281233
 ------------ Epoch 2/10 Batch 10200/11616 Training Results ------------ 
Total Loss: 4.158433670484844
Span Start Loss: 1.9588309638698895
Span End Loss: 1.8216280911921285
Type Loss: 0.3779745448698454
 ------------ Epoch 2/10 Batch 10250/11616 Training Results ------------ 
Total Loss: 4.157440087802526
Span Start Loss: 1.9584736113548278
Span End Loss: 1.8208231957569354
Type Loss: 0.37814321014975627
 ------------ Epoch 2/10 Batch 10300/11616 Training Results ------------ 
Total Loss: 4.156349645258443
Span Start Loss: 1.9578713208784178
Span End Loss: 1.8202200991117838
Type Loss: 0.3782581549579079
 ------------ Epoch 2/10 Batch 10350/11616 Training Results ------------ 
Total Loss: 4.155911186416656
Span Start Loss: 1.9574326832398123
Span End Loss: 1.8206383836298172
Type Loss: 0.37784004924665904
 ------------ Epoch 2/10 Batch 10400/11616 Training Results ------------ 
Total Loss: 4.155442233648724
Span Start Loss: 1.957052110450772
Span End Loss: 1.8203758334597715
Type Loss: 0.3780142192671505
 ------------ Epoch 2/10 Batch 10450/11616 Training Results ------------ 
Total Loss: 4.155977453576606
Span Start Loss: 1.9576527189438422
Span End Loss: 1.8203323820333162
Type Loss: 0.37799228228379095
 ------------ Epoch 2/10 Batch 10500/11616 Training Results ------------ 
Total Loss: 4.155070169731265
Span Start Loss: 1.9570702294792448
Span End Loss: 1.8198596094477744
Type Loss: 0.3781402602621487
 ------------ Epoch 2/10 Batch 10550/11616 Training Results ------------ 
Total Loss: 4.154916427535186
Span Start Loss: 1.9569666986584098
Span End Loss: 1.8195803630097782
Type Loss: 0.37836929536225955
 ------------ Epoch 2/10 Batch 10600/11616 Training Results ------------ 
Total Loss: 4.1536797947970765
Span Start Loss: 1.9563502053354147
Span End Loss: 1.8190602824682334
Type Loss: 0.3782692365162075
 ------------ Epoch 2/10 Batch 10650/11616 Training Results ------------ 
Total Loss: 4.152545871106391
Span Start Loss: 1.9560043252493853
Span End Loss: 1.818179598271567
Type Loss: 0.37836187703657387
 ------------ Epoch 2/10 Batch 10700/11616 Training Results ------------ 
Total Loss: 4.150518761909454
Span Start Loss: 1.9548522520483098
Span End Loss: 1.8171215415390853
Type Loss: 0.37854489763598087
 ------------ Epoch 2/10 Batch 10750/11616 Training Results ------------ 
Total Loss: 4.1501863150679785
Span Start Loss: 1.9547264156286106
Span End Loss: 1.8169094991184944
Type Loss: 0.3785503296208416
 ------------ Epoch 2/10 Batch 10800/11616 Training Results ------------ 
Total Loss: 4.148054073571607
Span Start Loss: 1.953650467230214
Span End Loss: 1.815750635051065
Type Loss: 0.378652900475405
 ------------ Epoch 2/10 Batch 10850/11616 Training Results ------------ 
Total Loss: 4.148631573327675
Span Start Loss: 1.9537677466869354
Span End Loss: 1.8160104351576571
Type Loss: 0.378853320745571
 ------------ Epoch 2/10 Batch 10900/11616 Training Results ------------ 
Total Loss: 4.148291652588123
Span Start Loss: 1.953915349425526
Span End Loss: 1.8156544942593356
Type Loss: 0.3787217382410832
 ------------ Epoch 2/10 Batch 10950/11616 Training Results ------------ 
Total Loss: 4.147249125590608
Span Start Loss: 1.9532255567998102
Span End Loss: 1.815319775511685
Type Loss: 0.3787037225624708
 ------------ Epoch 2/10 Batch 11000/11616 Training Results ------------ 
Total Loss: 4.1441074842756445
Span Start Loss: 1.9517483398453757
Span End Loss: 1.813826796331189
Type Loss: 0.3785322774546221
 ------------ Epoch 2/10 Batch 11050/11616 Training Results ------------ 
Total Loss: 4.144183119443746
Span Start Loss: 1.9518802279935163
Span End Loss: 1.8138272624290905
Type Loss: 0.3784755582021069
 ------------ Epoch 2/10 Batch 11100/11616 Training Results ------------ 
Total Loss: 4.143967989033407
Span Start Loss: 1.9514667606004723
Span End Loss: 1.813979562792155
Type Loss: 0.37852159468784324
 ------------ Epoch 2/10 Batch 11150/11616 Training Results ------------ 
Total Loss: 4.1432586317907
Span Start Loss: 1.9507457597453497
Span End Loss: 1.814003834326171
Type Loss: 0.3785089668639071
 ------------ Epoch 2/10 Batch 11200/11616 Training Results ------------ 
Total Loss: 4.143859998382894
Span Start Loss: 1.951079246393804
Span End Loss: 1.8142395591177047
Type Loss: 0.3785411221052553
 ------------ Epoch 2/10 Batch 11250/11616 Training Results ------------ 
Total Loss: 4.143955849627654
Span Start Loss: 1.951264670777321
Span End Loss: 1.8141019928799735
Type Loss: 0.37858911521641747
 ------------ Epoch 2/10 Batch 11300/11616 Training Results ------------ 
Total Loss: 4.143729075769671
Span Start Loss: 1.951153043801278
Span End Loss: 1.8141091031126217
Type Loss: 0.3784668580878658
 ------------ Epoch 2/10 Batch 11350/11616 Training Results ------------ 
Total Loss: 4.143410625377678
Span Start Loss: 1.9509984190427259
Span End Loss: 1.814275258551085
Type Loss: 0.37813687696207793
 ------------ Epoch 2/10 Batch 11400/11616 Training Results ------------ 
Total Loss: 4.142501815062082
Span Start Loss: 1.9509624437125106
Span End Loss: 1.8135337059003742
Type Loss: 0.37800559449851057
 ------------ Epoch 2/10 Batch 11450/11616 Training Results ------------ 
Total Loss: 4.14095201577552
Span Start Loss: 1.9499997995566072
Span End Loss: 1.8131422375630604
Type Loss: 0.37780990768427347
 ------------ Epoch 2/10 Batch 11500/11616 Training Results ------------ 
Total Loss: 4.140091887058123
Span Start Loss: 1.949436656438786
Span End Loss: 1.8130010302934958
Type Loss: 0.37765412922082064
 ------------ Epoch 2/10 Batch 11550/11616 Training Results ------------ 
Total Loss: 4.138214103696924
Span Start Loss: 1.9486532556339775
Span End Loss: 1.8121267431755086
Type Loss: 0.3774340339644421
 ------------ Epoch 2/10 Batch 11600/11616 Training Results ------------ 
Total Loss: 4.137177813496312
Span Start Loss: 1.948152872819839
Span End Loss: 1.8116825006555382
Type Loss: 0.37734236904138957
 --------------------- Epoch 2/10 Final Training Results ------------------------ 
Total Loss: 4.137151786235235
Span Start Loss: 1.9479673064029044
Span End Loss: 1.811846470307641
Type Loss: 0.37733793843926583
 --------------- Epoch 2/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 54.6, 'f1': 64.9, 'turns': 1425}), ('literature', {'em': 49.9, 'f1': 59.8, 'turns': 1630}), ('mid-high_school', {'em': 51.1, 'f1': 62.2, 'turns': 1653}), ('news', {'em': 54.2, 'f1': 64.8, 'turns': 1649}), ('wikipedia', {'em': 59.1, 'f1': 69.5, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 53.8, 'f1': 64.2, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 53.8, 'f1': 64.2, 'turns': 7983})])
 --------------- Epoch 3/10 Training Start --------------- 
 ------------ Epoch 3/10 Batch 50/11616 Training Results ------------ 
Total Loss: 3.4386544704437254
Span Start Loss: 1.543994357585907
Span End Loss: 1.5500676262378692
Type Loss: 0.3445924054458737
 ------------ Epoch 3/10 Batch 100/11616 Training Results ------------ 
Total Loss: 3.2933021523058414
Span Start Loss: 1.501513936817646
Span End Loss: 1.4490505562722682
Type Loss: 0.34273757608607414
 ------------ Epoch 3/10 Batch 150/11616 Training Results ------------ 
Total Loss: 3.274193562567234
Span Start Loss: 1.4844595860441525
Span End Loss: 1.4409478035569192
Type Loss: 0.3487860978767276
 ------------ Epoch 3/10 Batch 200/11616 Training Results ------------ 
Total Loss: 3.2803749561309816
Span Start Loss: 1.490690395310521
Span End Loss: 1.4280061152577401
Type Loss: 0.3616783634154126
 ------------ Epoch 3/10 Batch 250/11616 Training Results ------------ 
Total Loss: 3.2945166692137717
Span Start Loss: 1.4903475795388221
Span End Loss: 1.4455941157341003
Type Loss: 0.3585748881585896
 ------------ Epoch 3/10 Batch 300/11616 Training Results ------------ 
Total Loss: 3.254424360146125
Span Start Loss: 1.483777600377798
Span End Loss: 1.4138046745459238
Type Loss: 0.3568420053552836
 ------------ Epoch 3/10 Batch 350/11616 Training Results ------------ 
Total Loss: 3.181872477063111
Span Start Loss: 1.4537196112104824
Span End Loss: 1.3740369483828545
Type Loss: 0.3541158440788942
 ------------ Epoch 3/10 Batch 400/11616 Training Results ------------ 
Total Loss: 3.219275768660009
Span Start Loss: 1.475744209997356
Span End Loss: 1.388161681033671
Type Loss: 0.35536980820586905
 ------------ Epoch 3/10 Batch 450/11616 Training Results ------------ 
Total Loss: 3.2235517972045473
Span Start Loss: 1.4867095912165111
Span End Loss: 1.3885420735677083
Type Loss: 0.348300064874606
 ------------ Epoch 3/10 Batch 500/11616 Training Results ------------ 
Total Loss: 3.2114295934140684
Span Start Loss: 1.4808060287237168
Span End Loss: 1.3826375195086003
Type Loss: 0.34798597696609795
 ------------ Epoch 3/10 Batch 550/11616 Training Results ------------ 
Total Loss: 3.2337272782217372
Span Start Loss: 1.4859375953132457
Span End Loss: 1.3964546623284166
Type Loss: 0.3513349538008598
 ------------ Epoch 3/10 Batch 600/11616 Training Results ------------ 
Total Loss: 3.2305183758338294
Span Start Loss: 1.4832245901723702
Span End Loss: 1.393980894908309
Type Loss: 0.35331282587566726
 ------------ Epoch 3/10 Batch 650/11616 Training Results ------------ 
Total Loss: 3.229187752650334
Span Start Loss: 1.4827450485871387
Span End Loss: 1.3913577337219165
Type Loss: 0.35508490591238323
 ------------ Epoch 3/10 Batch 700/11616 Training Results ------------ 
Total Loss: 3.226818573474884
Span Start Loss: 1.4792880920427187
Span End Loss: 1.3918820767530373
Type Loss: 0.3556483379339001
 ------------ Epoch 3/10 Batch 750/11616 Training Results ------------ 
Total Loss: 3.209981396595637
Span Start Loss: 1.4697611180146535
Span End Loss: 1.3843876235286394
Type Loss: 0.3558325913262864
 ------------ Epoch 3/10 Batch 800/11616 Training Results ------------ 
Total Loss: 3.2232777109835298
Span Start Loss: 1.4757785315066576
Span End Loss: 1.3909336633607745
Type Loss: 0.35656545314355753
 ------------ Epoch 3/10 Batch 850/11616 Training Results ------------ 
Total Loss: 3.2287682640201907
Span Start Loss: 1.4790518910569304
Span End Loss: 1.3920590999897788
Type Loss: 0.3576572116234285
 ------------ Epoch 3/10 Batch 900/11616 Training Results ------------ 
Total Loss: 3.237510339419047
Span Start Loss: 1.484774162520965
Span End Loss: 1.3939627051022319
Type Loss: 0.3587734081316739
 ------------ Epoch 3/10 Batch 950/11616 Training Results ------------ 
Total Loss: 3.2420222930218046
Span Start Loss: 1.4895179931897866
Span End Loss: 1.3934069011399621
Type Loss: 0.3590973352513423
 ------------ Epoch 3/10 Batch 1000/11616 Training Results ------------ 
Total Loss: 3.228398032858968
Span Start Loss: 1.4836711000353098
Span End Loss: 1.3873305367827415
Type Loss: 0.3573963325629011
 ------------ Epoch 3/10 Batch 1050/11616 Training Results ------------ 
Total Loss: 3.242452036979653
Span Start Loss: 1.4911133572743052
Span End Loss: 1.395430793081011
Type Loss: 0.35590782324029574
 ------------ Epoch 3/10 Batch 1100/11616 Training Results ------------ 
Total Loss: 3.243004546585408
Span Start Loss: 1.492016016529365
Span End Loss: 1.3930825953591954
Type Loss: 0.35790587079372593
 ------------ Epoch 3/10 Batch 1150/11616 Training Results ------------ 
Total Loss: 3.239751571935156
Span Start Loss: 1.4914336909029795
Span End Loss: 1.3881937893836394
Type Loss: 0.3601240291211592
 ------------ Epoch 3/10 Batch 1200/11616 Training Results ------------ 
Total Loss: 3.2456115795920293
Span Start Loss: 1.4923276467993856
Span End Loss: 1.3921597615381083
Type Loss: 0.3611241082125343
 ------------ Epoch 3/10 Batch 1250/11616 Training Results ------------ 
Total Loss: 3.239684763789177
Span Start Loss: 1.4888635729908943
Span End Loss: 1.392034500169754
Type Loss: 0.3587866271533072
 ------------ Epoch 3/10 Batch 1300/11616 Training Results ------------ 
Total Loss: 3.230624909985524
Span Start Loss: 1.4851046539957706
Span End Loss: 1.3882379024074627
Type Loss: 0.35728229167154774
 ------------ Epoch 3/10 Batch 1350/11616 Training Results ------------ 
Total Loss: 3.2241504437945507
Span Start Loss: 1.4794292374451956
Span End Loss: 1.3862331890397601
Type Loss: 0.3584879562003469
 ------------ Epoch 3/10 Batch 1400/11616 Training Results ------------ 
Total Loss: 3.2277598872248614
Span Start Loss: 1.4831952426263264
Span End Loss: 1.3852576846948692
Type Loss: 0.35930689918875164
 ------------ Epoch 3/10 Batch 1450/11616 Training Results ------------ 
Total Loss: 3.220866231209245
Span Start Loss: 1.481126045543572
Span End Loss: 1.3811282776758589
Type Loss: 0.35861184792850037
 ------------ Epoch 3/10 Batch 1500/11616 Training Results ------------ 
Total Loss: 3.218303362677495
Span Start Loss: 1.4798525478641191
Span End Loss: 1.381431207815806
Type Loss: 0.3570195467056086
 ------------ Epoch 3/10 Batch 1550/11616 Training Results ------------ 
Total Loss: 3.2056350311444652
Span Start Loss: 1.4728637225397172
Span End Loss: 1.37757240410774
Type Loss: 0.3551988442608666
 ------------ Epoch 3/10 Batch 1600/11616 Training Results ------------ 
Total Loss: 3.2156071234494448
Span Start Loss: 1.4793325199745595
Span End Loss: 1.3816168362833559
Type Loss: 0.35465770614624487
 ------------ Epoch 3/10 Batch 1650/11616 Training Results ------------ 
Total Loss: 3.2111937667835844
Span Start Loss: 1.4772327191179448
Span End Loss: 1.3807813930872714
Type Loss: 0.353179594606274
 ------------ Epoch 3/10 Batch 1700/11616 Training Results ------------ 
Total Loss: 3.209878094406689
Span Start Loss: 1.476208808720112
Span End Loss: 1.3796438658105976
Type Loss: 0.354025359518059
 ------------ Epoch 3/10 Batch 1750/11616 Training Results ------------ 
Total Loss: 3.210371587574482
Span Start Loss: 1.478035898259708
Span End Loss: 1.3799410428702832
Type Loss: 0.35239458598888346
 ------------ Epoch 3/10 Batch 1800/11616 Training Results ------------ 
Total Loss: 3.212981001743012
Span Start Loss: 1.4779812005493376
Span End Loss: 1.381133644171059
Type Loss: 0.3538660965223486
 ------------ Epoch 3/10 Batch 1850/11616 Training Results ------------ 
Total Loss: 3.2094737801197413
Span Start Loss: 1.476691307425499
Span End Loss: 1.3796993589602613
Type Loss: 0.3530830537477458
 ------------ Epoch 3/10 Batch 1900/11616 Training Results ------------ 
Total Loss: 3.215757442108895
Span Start Loss: 1.4802909017550319
Span End Loss: 1.3818018634264406
Type Loss: 0.35366461667085164
 ------------ Epoch 3/10 Batch 1950/11616 Training Results ------------ 
Total Loss: 3.217099859813849
Span Start Loss: 1.4813453280314421
Span End Loss: 1.3809824642195152
Type Loss: 0.3547720077586098
 ------------ Epoch 3/10 Batch 2000/11616 Training Results ------------ 
Total Loss: 3.2240220360085368
Span Start Loss: 1.4837472516596317
Span End Loss: 1.3846484266258776
Type Loss: 0.3556262978259474
 --------------- Epoch 3/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 56.4, 'f1': 66.3, 'turns': 1425}), ('literature', {'em': 53.2, 'f1': 63.1, 'turns': 1630}), ('mid-high_school', {'em': 52.0, 'f1': 63.7, 'turns': 1653}), ('news', {'em': 57.3, 'f1': 68.0, 'turns': 1649}), ('wikipedia', {'em': 59.4, 'f1': 70.3, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 55.6, 'f1': 66.3, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 55.6, 'f1': 66.3, 'turns': 7983})])
 ------------ Epoch 3/10 Batch 2050/11616 Training Results ------------ 
Total Loss: 3.2241351629902675
Span Start Loss: 1.4852022869121737
Span End Loss: 1.3826052566018046
Type Loss: 0.35632755966629925
 ------------ Epoch 3/10 Batch 2100/11616 Training Results ------------ 
Total Loss: 3.215712347094502
Span Start Loss: 1.4816767068562053
Span End Loss: 1.3781768801560004
Type Loss: 0.35585870070649045
 ------------ Epoch 3/10 Batch 2150/11616 Training Results ------------ 
Total Loss: 3.2130814958450404
Span Start Loss: 1.4799801728475925
Span End Loss: 1.378243327892797
Type Loss: 0.3548579356299583
 ------------ Epoch 3/10 Batch 2200/11616 Training Results ------------ 
Total Loss: 3.2168715873902496
Span Start Loss: 1.4818610766800968
Span End Loss: 1.3803724813088776
Type Loss: 0.3546379692818631
 ------------ Epoch 3/10 Batch 2250/11616 Training Results ------------ 
Total Loss: 3.2130026140146786
Span Start Loss: 1.4815765081246695
Span End Loss: 1.3763518285652
Type Loss: 0.35507421751154794
 ------------ Epoch 3/10 Batch 2300/11616 Training Results ------------ 
Total Loss: 3.2176897865598617
Span Start Loss: 1.4840064083234124
Span End Loss: 1.3784597169607877
Type Loss: 0.3552236011608139
 ------------ Epoch 3/10 Batch 2350/11616 Training Results ------------ 
Total Loss: 3.211926498945723
Span Start Loss: 1.4808999717108746
Span End Loss: 1.3755547695939845
Type Loss: 0.35547169796805433
 ------------ Epoch 3/10 Batch 2400/11616 Training Results ------------ 
Total Loss: 3.209836324552695
Span Start Loss: 1.4791942977036039
Span End Loss: 1.3752111256836603
Type Loss: 0.35543084134502956
 ------------ Epoch 3/10 Batch 2450/11616 Training Results ------------ 
Total Loss: 3.208627818311964
Span Start Loss: 1.478592920364166
Span End Loss: 1.374317637970861
Type Loss: 0.3557172004924137
 ------------ Epoch 3/10 Batch 2500/11616 Training Results ------------ 
Total Loss: 3.211405410975218
Span Start Loss: 1.479897812139988
Span End Loss: 1.3764602998822928
Type Loss: 0.3550472387216985
 ------------ Epoch 3/10 Batch 2550/11616 Training Results ------------ 
Total Loss: 3.215362494219752
Span Start Loss: 1.4814269333376604
Span End Loss: 1.378863854510527
Type Loss: 0.3550716472950344
 ------------ Epoch 3/10 Batch 2600/11616 Training Results ------------ 
Total Loss: 3.217045882000373
Span Start Loss: 1.480925413863017
Span End Loss: 1.3800760895214401
Type Loss: 0.3560443200994856
 ------------ Epoch 3/10 Batch 2650/11616 Training Results ------------ 
Total Loss: 3.2158211889132016
Span Start Loss: 1.4806080785899791
Span End Loss: 1.3791349612063957
Type Loss: 0.3560780910736407
 ------------ Epoch 3/10 Batch 2700/11616 Training Results ------------ 
Total Loss: 3.2152873480485544
Span Start Loss: 1.4797270938091809
Span End Loss: 1.379508268068786
Type Loss: 0.3560519285731156
 ------------ Epoch 3/10 Batch 2750/11616 Training Results ------------ 
Total Loss: 3.2125095869194378
Span Start Loss: 1.4781072709126906
Span End Loss: 1.3793403574201193
Type Loss: 0.3550619015625932
 ------------ Epoch 3/10 Batch 2800/11616 Training Results ------------ 
Total Loss: 3.2145548213007196
Span Start Loss: 1.4795647826897247
Span End Loss: 1.3792048171561744
Type Loss: 0.3557851645323847
 ------------ Epoch 3/10 Batch 2850/11616 Training Results ------------ 
Total Loss: 3.215790907175917
Span Start Loss: 1.478900430385481
Span End Loss: 1.3800187989677253
Type Loss: 0.35687162131593936
 ------------ Epoch 3/10 Batch 2900/11616 Training Results ------------ 
Total Loss: 3.2222025462265673
Span Start Loss: 1.4822825500214922
Span End Loss: 1.383463676538447
Type Loss: 0.3564562634378672
 ------------ Epoch 3/10 Batch 2950/11616 Training Results ------------ 
Total Loss: 3.2182760337399223
Span Start Loss: 1.4788107952778622
Span End Loss: 1.3827181829613144
Type Loss: 0.3567469995885582
 ------------ Epoch 3/10 Batch 3000/11616 Training Results ------------ 
Total Loss: 3.2179841946562133
Span Start Loss: 1.477956833511591
Span End Loss: 1.3838089023008942
Type Loss: 0.35621840341513356
 ------------ Epoch 3/10 Batch 3050/11616 Training Results ------------ 
Total Loss: 3.2176233153851306
Span Start Loss: 1.4765890669138706
Span End Loss: 1.3846380040464832
Type Loss: 0.356396189009557
 ------------ Epoch 3/10 Batch 3100/11616 Training Results ------------ 
Total Loss: 3.2209309941818636
Span Start Loss: 1.4782787599390552
Span End Loss: 1.3862298324776272
Type Loss: 0.3564223462751796
 ------------ Epoch 3/10 Batch 3150/11616 Training Results ------------ 
Total Loss: 3.2183948177903416
Span Start Loss: 1.477053533678963
Span End Loss: 1.3852749664324617
Type Loss: 0.35606626218983106
 ------------ Epoch 3/10 Batch 3200/11616 Training Results ------------ 
Total Loss: 3.2173515582457184
Span Start Loss: 1.47696368903853
Span End Loss: 1.3845796440425329
Type Loss: 0.3558081696741283
 ------------ Epoch 3/10 Batch 3250/11616 Training Results ------------ 
Total Loss: 3.2207743375805706
Span Start Loss: 1.4788413630723953
Span End Loss: 1.386493758987922
Type Loss: 0.35543915971855705
 ------------ Epoch 3/10 Batch 3300/11616 Training Results ------------ 
Total Loss: 3.218425646153363
Span Start Loss: 1.478938908134446
Span End Loss: 1.3847312050222447
Type Loss: 0.3547554771232447
 ------------ Epoch 3/10 Batch 3350/11616 Training Results ------------ 
Total Loss: 3.2210634074993987
Span Start Loss: 1.4790404088639502
Span End Loss: 1.3868947750863745
Type Loss: 0.35512816753544246
 ------------ Epoch 3/10 Batch 3400/11616 Training Results ------------ 
Total Loss: 3.220729852664997
Span Start Loss: 1.4783080557164023
Span End Loss: 1.387017930106205
Type Loss: 0.35540381127995824
 ------------ Epoch 3/10 Batch 3450/11616 Training Results ------------ 
Total Loss: 3.2208361678581308
Span Start Loss: 1.4785462234115254
Span End Loss: 1.3874380443839058
Type Loss: 0.35485184468787867
 ------------ Epoch 3/10 Batch 3500/11616 Training Results ------------ 
Total Loss: 3.219151688281979
Span Start Loss: 1.478054791616542
Span End Loss: 1.386576744556427
Type Loss: 0.35452009675359086
 ------------ Epoch 3/10 Batch 3550/11616 Training Results ------------ 
Total Loss: 3.217530072218096
Span Start Loss: 1.4775684656181804
Span End Loss: 1.386242186242426
Type Loss: 0.3537193649595367
 ------------ Epoch 3/10 Batch 3600/11616 Training Results ------------ 
Total Loss: 3.21945364959538
Span Start Loss: 1.4781177315488458
Span End Loss: 1.386935091238055
Type Loss: 0.3544007714266061
 ------------ Epoch 3/10 Batch 3650/11616 Training Results ------------ 
Total Loss: 3.216109348751911
Span Start Loss: 1.4769730421655798
Span End Loss: 1.3853088469415495
Type Loss: 0.3538274044163321
 ------------ Epoch 3/10 Batch 3700/11616 Training Results ------------ 
Total Loss: 3.218642568769487
Span Start Loss: 1.479099639271562
Span End Loss: 1.3859476773521384
Type Loss: 0.3535951971194732
 ------------ Epoch 3/10 Batch 3750/11616 Training Results ------------ 
Total Loss: 3.2205734811027846
Span Start Loss: 1.479938286650181
Span End Loss: 1.3871913834691048
Type Loss: 0.3534437555852036
 ------------ Epoch 3/10 Batch 3800/11616 Training Results ------------ 
Total Loss: 3.222719298048239
Span Start Loss: 1.4815498385813675
Span End Loss: 1.3878682136653284
Type Loss: 0.35330119044299385
 ------------ Epoch 3/10 Batch 3850/11616 Training Results ------------ 
Total Loss: 3.221150074821788
Span Start Loss: 1.4811466168621918
Span End Loss: 1.386385430825221
Type Loss: 0.35361797192581484
 ------------ Epoch 3/10 Batch 3900/11616 Training Results ------------ 
Total Loss: 3.2217232862153113
Span Start Loss: 1.4814510473876428
Span End Loss: 1.3871832833381799
Type Loss: 0.35308890007173593
 ------------ Epoch 3/10 Batch 3950/11616 Training Results ------------ 
Total Loss: 3.2231733139745797
Span Start Loss: 1.4820999261812318
Span End Loss: 1.3879079851659037
Type Loss: 0.3531653470982196
 ------------ Epoch 3/10 Batch 4000/11616 Training Results ------------ 
Total Loss: 3.2230105945654213
Span Start Loss: 1.4820153431631624
Span End Loss: 1.3876508842147888
Type Loss: 0.3533443123383913
 --------------- Epoch 3/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 56.6, 'f1': 67.3, 'turns': 1425}), ('literature', {'em': 52.2, 'f1': 62.0, 'turns': 1630}), ('mid-high_school', {'em': 53.1, 'f1': 63.3, 'turns': 1653}), ('news', {'em': 56.0, 'f1': 66.2, 'turns': 1649}), ('wikipedia', {'em': 60.0, 'f1': 70.4, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 55.5, 'f1': 65.8, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 55.5, 'f1': 65.8, 'turns': 7983})])
 ------------ Epoch 3/10 Batch 4050/11616 Training Results ------------ 
Total Loss: 3.223832176864883
Span Start Loss: 1.4811656628788255
Span End Loss: 1.3892884601671018
Type Loss: 0.35337799960931326
 ------------ Epoch 3/10 Batch 4100/11616 Training Results ------------ 
Total Loss: 3.2236699168820206
Span Start Loss: 1.4817004790466006
Span End Loss: 1.389129962212429
Type Loss: 0.35283942135050894
 ------------ Epoch 3/10 Batch 4150/11616 Training Results ------------ 
Total Loss: 3.2272637206926404
Span Start Loss: 1.483423426366714
Span End Loss: 1.3904038156694676
Type Loss: 0.3534364245019584
 ------------ Epoch 3/10 Batch 4200/11616 Training Results ------------ 
Total Loss: 3.2277838451521736
Span Start Loss: 1.4843784403233302
Span End Loss: 1.3903617682307958
Type Loss: 0.35304358241769174
 ------------ Epoch 3/10 Batch 4250/11616 Training Results ------------ 
Total Loss: 3.227839315779069
Span Start Loss: 1.4833918824686723
Span End Loss: 1.3911786658097716
Type Loss: 0.35326871334410764
 ------------ Epoch 3/10 Batch 4300/11616 Training Results ------------ 
Total Loss: 3.2261888476651768
Span Start Loss: 1.4827887903396473
Span End Loss: 1.3907923653617849
Type Loss: 0.3526076378404748
 ------------ Epoch 3/10 Batch 4350/11616 Training Results ------------ 
Total Loss: 3.2236108926178395
Span Start Loss: 1.4817714806877333
Span End Loss: 1.3891665996388458
Type Loss: 0.3526727576566667
 ------------ Epoch 3/10 Batch 4400/11616 Training Results ------------ 
Total Loss: 3.2290719005871904
Span Start Loss: 1.484610368348658
Span End Loss: 1.3914506310597061
Type Loss: 0.35301084648775444
 ------------ Epoch 3/10 Batch 4450/11616 Training Results ------------ 
Total Loss: 3.2285171104615995
Span Start Loss: 1.484635307423854
Span End Loss: 1.3909912588589646
Type Loss: 0.3528904888632425
 ------------ Epoch 3/10 Batch 4500/11616 Training Results ------------ 
Total Loss: 3.2270684290594525
Span Start Loss: 1.4839853598442343
Span End Loss: 1.3898903169764414
Type Loss: 0.3531926973375181
 ------------ Epoch 3/10 Batch 4550/11616 Training Results ------------ 
Total Loss: 3.227295045374514
Span Start Loss: 1.48445834426107
Span End Loss: 1.3894089044122906
Type Loss: 0.35342774182639936
 ------------ Epoch 3/10 Batch 4600/11616 Training Results ------------ 
Total Loss: 3.231171928200385
Span Start Loss: 1.4869929387679566
Span End Loss: 1.390034864836413
Type Loss: 0.3541440697281581
 ------------ Epoch 3/10 Batch 4650/11616 Training Results ------------ 
Total Loss: 3.235900690430595
Span Start Loss: 1.4893395645163392
Span End Loss: 1.3926645294056144
Type Loss: 0.3538965419323374
 ------------ Epoch 3/10 Batch 4700/11616 Training Results ------------ 
Total Loss: 3.2314405017710746
Span Start Loss: 1.4878003465336689
Span End Loss: 1.3900301309817649
Type Loss: 0.35360996998410593
 ------------ Epoch 3/10 Batch 4750/11616 Training Results ------------ 
Total Loss: 3.2318312547395105
Span Start Loss: 1.4875810379009498
Span End Loss: 1.3902473099827766
Type Loss: 0.3540028527994689
 ------------ Epoch 3/10 Batch 4800/11616 Training Results ------------ 
Total Loss: 3.2329660021544746
Span Start Loss: 1.4881956136381875
Span End Loss: 1.3911150496049474
Type Loss: 0.35365528461096496
 ------------ Epoch 3/10 Batch 4850/11616 Training Results ------------ 
Total Loss: 3.2308310740848176
Span Start Loss: 1.4873724477162067
Span End Loss: 1.3898405316349158
Type Loss: 0.35361804031718946
 ------------ Epoch 3/10 Batch 4900/11616 Training Results ------------ 
Total Loss: 3.231733529771469
Span Start Loss: 1.4878727929081235
Span End Loss: 1.3905513312774045
Type Loss: 0.35330935119024043
 ------------ Epoch 3/10 Batch 4950/11616 Training Results ------------ 
Total Loss: 3.2338016972246795
Span Start Loss: 1.4892223254297718
Span End Loss: 1.3912891636984517
Type Loss: 0.3532901536300778
 ------------ Epoch 3/10 Batch 5000/11616 Training Results ------------ 
Total Loss: 3.233511384308338
Span Start Loss: 1.4898407236397266
Span End Loss: 1.3903686546295881
Type Loss: 0.35330195143371823
 ------------ Epoch 3/10 Batch 5050/11616 Training Results ------------ 
Total Loss: 3.2342300527756755
Span Start Loss: 1.4904951996732467
Span End Loss: 1.3911648539829962
Type Loss: 0.3525699443672553
 ------------ Epoch 3/10 Batch 5100/11616 Training Results ------------ 
Total Loss: 3.234193703532219
Span Start Loss: 1.4904879844948358
Span End Loss: 1.3916856456124316
Type Loss: 0.35202001880050876
 ------------ Epoch 3/10 Batch 5150/11616 Training Results ------------ 
Total Loss: 3.234207323873506
Span Start Loss: 1.4905123250692793
Span End Loss: 1.3917852084237394
Type Loss: 0.35190973572794676
 ------------ Epoch 3/10 Batch 5200/11616 Training Results ------------ 
Total Loss: 3.233567153203946
Span Start Loss: 1.4904654831840443
Span End Loss: 1.3908083141007677
Type Loss: 0.3522933014811805
 ------------ Epoch 3/10 Batch 5250/11616 Training Results ------------ 
Total Loss: 3.234353179512989
Span Start Loss: 1.4912469540720894
Span End Loss: 1.3907514478124323
Type Loss: 0.3523547236443985
 ------------ Epoch 3/10 Batch 5300/11616 Training Results ------------ 
Total Loss: 3.2364166632646096
Span Start Loss: 1.4924054234207802
Span End Loss: 1.3912737786362193
Type Loss: 0.35273740782595747
 ------------ Epoch 3/10 Batch 5350/11616 Training Results ------------ 
Total Loss: 3.2368630360428976
Span Start Loss: 1.4929364432464136
Span End Loss: 1.391650194089825
Type Loss: 0.3522763454722606
 ------------ Epoch 3/10 Batch 5400/11616 Training Results ------------ 
Total Loss: 3.2398212620574567
Span Start Loss: 1.4940647697945435
Span End Loss: 1.393076284173186
Type Loss: 0.35268015456489393
 ------------ Epoch 3/10 Batch 5450/11616 Training Results ------------ 
Total Loss: 3.2419270035392103
Span Start Loss: 1.495322320422995
Span End Loss: 1.3942152224481106
Type Loss: 0.3523894068933682
 ------------ Epoch 3/10 Batch 5500/11616 Training Results ------------ 
Total Loss: 3.242184399020943
Span Start Loss: 1.4957266902869397
Span End Loss: 1.3941564386798577
Type Loss: 0.35230121623318306
 ------------ Epoch 3/10 Batch 5550/11616 Training Results ------------ 
Total Loss: 3.2433710398236375
Span Start Loss: 1.4963723115502177
Span End Loss: 1.394402944154299
Type Loss: 0.3525957302259164
 ------------ Epoch 3/10 Batch 5600/11616 Training Results ------------ 
Total Loss: 3.2419082807709594
Span Start Loss: 1.4958970394251603
Span End Loss: 1.3939286818501673
Type Loss: 0.35208250570443594
 ------------ Epoch 3/10 Batch 5650/11616 Training Results ------------ 
Total Loss: 3.238208824508222
Span Start Loss: 1.494710108561326
Span End Loss: 1.3917789624499537
Type Loss: 0.3517196997815529
 ------------ Epoch 3/10 Batch 5700/11616 Training Results ------------ 
Total Loss: 3.23880774575498
Span Start Loss: 1.4959767315392953
Span End Loss: 1.3914372237797892
Type Loss: 0.35139373689595804
 ------------ Epoch 3/10 Batch 5750/11616 Training Results ------------ 
Total Loss: 3.2402656447433906
Span Start Loss: 1.496788539370765
Span End Loss: 1.3917350145098957
Type Loss: 0.35174203703422907
 ------------ Epoch 3/10 Batch 5800/11616 Training Results ------------ 
Total Loss: 3.2394727958880107
Span Start Loss: 1.496549257957216
Span End Loss: 1.3911210288556999
Type Loss: 0.3518024551030248
 ------------ Epoch 3/10 Batch 5850/11616 Training Results ------------ 
Total Loss: 3.2376194114167975
Span Start Loss: 1.4960016059111325
Span End Loss: 1.3899602960075579
Type Loss: 0.35165745542249366
 ------------ Epoch 3/10 Batch 5900/11616 Training Results ------------ 
Total Loss: 3.2386491325636535
Span Start Loss: 1.4963957876118563
Span End Loss: 1.3908275439413422
Type Loss: 0.3514257470527822
 ------------ Epoch 3/10 Batch 5950/11616 Training Results ------------ 
Total Loss: 3.237617599745007
Span Start Loss: 1.4960308393861066
Span End Loss: 1.3901835795898898
Type Loss: 0.3514031268978695
 ------------ Epoch 3/10 Batch 6000/11616 Training Results ------------ 
Total Loss: 3.2388450234942137
Span Start Loss: 1.4961817249506713
Span End Loss: 1.3910623621729512
Type Loss: 0.35160088243801146
 --------------- Epoch 3/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 55.6, 'f1': 67.0, 'turns': 1425}), ('literature', {'em': 50.1, 'f1': 60.6, 'turns': 1630}), ('mid-high_school', {'em': 52.0, 'f1': 62.7, 'turns': 1653}), ('news', {'em': 55.2, 'f1': 66.4, 'turns': 1649}), ('wikipedia', {'em': 59.8, 'f1': 70.3, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 54.5, 'f1': 65.4, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 54.5, 'f1': 65.4, 'turns': 7983})])
 ------------ Epoch 3/10 Batch 6050/11616 Training Results ------------ 
Total Loss: 3.2378066829228698
Span Start Loss: 1.4955163756111436
Span End Loss: 1.3902221910951058
Type Loss: 0.3520680625124042
 ------------ Epoch 3/10 Batch 6100/11616 Training Results ------------ 
Total Loss: 3.2365396601804455
Span Start Loss: 1.4949943686972875
Span End Loss: 1.3895777380063397
Type Loss: 0.3519675001242488
 ------------ Epoch 3/10 Batch 6150/11616 Training Results ------------ 
Total Loss: 3.2372523541675835
Span Start Loss: 1.4952895294529636
Span End Loss: 1.3899953481760936
Type Loss: 0.3519674231151376
 ------------ Epoch 3/10 Batch 6200/11616 Training Results ------------ 
Total Loss: 3.2373996890420393
Span Start Loss: 1.4955288877866921
Span End Loss: 1.3897970371609254
Type Loss: 0.352073710616678
 ------------ Epoch 3/10 Batch 6250/11616 Training Results ------------ 
Total Loss: 3.237017075794935
Span Start Loss: 1.4954780908608436
Span End Loss: 1.3894676208007335
Type Loss: 0.35207131063908337
 ------------ Epoch 3/10 Batch 6300/11616 Training Results ------------ 
Total Loss: 3.2382718003754105
Span Start Loss: 1.4962317112372034
Span End Loss: 1.389942134738205
Type Loss: 0.35209790106153205
 ------------ Epoch 3/10 Batch 6350/11616 Training Results ------------ 
Total Loss: 3.2383800818166866
Span Start Loss: 1.496126168557978
Span End Loss: 1.3902786395270523
Type Loss: 0.35197522026728695
 ------------ Epoch 3/10 Batch 6400/11616 Training Results ------------ 
Total Loss: 3.2408662917732727
Span Start Loss: 1.4971695374930278
Span End Loss: 1.391601789999986
Type Loss: 0.35209491063142195
 ------------ Epoch 3/10 Batch 6450/11616 Training Results ------------ 
Total Loss: 3.241837589067313
Span Start Loss: 1.4974018214053886
Span End Loss: 1.3920637687890567
Type Loss: 0.35237194510884295
 ------------ Epoch 3/10 Batch 6500/11616 Training Results ------------ 
Total Loss: 3.238450077307912
Span Start Loss: 1.4958148614672515
Span End Loss: 1.3900776246430782
Type Loss: 0.352557537305527
 ------------ Epoch 3/10 Batch 6550/11616 Training Results ------------ 
Total Loss: 3.2369879780777993
Span Start Loss: 1.495320238866879
Span End Loss: 1.389345418625433
Type Loss: 0.3523222668830106
 ------------ Epoch 3/10 Batch 6600/11616 Training Results ------------ 
Total Loss: 3.235221024359943
Span Start Loss: 1.494791785040588
Span End Loss: 1.3883512988743005
Type Loss: 0.3520778866893979
 ------------ Epoch 3/10 Batch 6650/11616 Training Results ------------ 
Total Loss: 3.235139749008686
Span Start Loss: 1.4946450514508818
Span End Loss: 1.3884748221866619
Type Loss: 0.3520198216482549
 ------------ Epoch 3/10 Batch 6700/11616 Training Results ------------ 
Total Loss: 3.2349778489001206
Span Start Loss: 1.494607805321688
Span End Loss: 1.388568244283991
Type Loss: 0.35180174569891237
 ------------ Epoch 3/10 Batch 6750/11616 Training Results ------------ 
Total Loss: 3.234946205886426
Span Start Loss: 1.494620201566705
Span End Loss: 1.3884977208342817
Type Loss: 0.35182822979793504
 ------------ Epoch 3/10 Batch 6800/11616 Training Results ------------ 
Total Loss: 3.2361130751285923
Span Start Loss: 1.4950856744859586
Span End Loss: 1.3896092829250675
Type Loss: 0.35141806394820485
 ------------ Epoch 3/10 Batch 6850/11616 Training Results ------------ 
Total Loss: 3.2359856357485275
Span Start Loss: 1.49503807013161
Span End Loss: 1.3896733192338127
Type Loss: 0.35127419289310263
 ------------ Epoch 3/10 Batch 6900/11616 Training Results ------------ 
Total Loss: 3.2352939454163763
Span Start Loss: 1.4954374964222097
Span End Loss: 1.388938692951764
Type Loss: 0.35091770236555864
 ------------ Epoch 3/10 Batch 6950/11616 Training Results ------------ 
Total Loss: 3.236892358052859
Span Start Loss: 1.495994941649034
Span End Loss: 1.3900499256791399
Type Loss: 0.3508474370204716
 ------------ Epoch 3/10 Batch 7000/11616 Training Results ------------ 
Total Loss: 3.237911345974675
Span Start Loss: 1.4968187527326602
Span End Loss: 1.3905853546570455
Type Loss: 0.3505071847883186
 ------------ Epoch 3/10 Batch 7050/11616 Training Results ------------ 
Total Loss: 3.2364735753982203
Span Start Loss: 1.496629218185413
Span End Loss: 1.3896280249435429
Type Loss: 0.3502162784837345
 ------------ Epoch 3/10 Batch 7100/11616 Training Results ------------ 
Total Loss: 3.2383551434994162
Span Start Loss: 1.4973418539563115
Span End Loss: 1.3905796342091241
Type Loss: 0.35043360142273383
 ------------ Epoch 3/10 Batch 7150/11616 Training Results ------------ 
Total Loss: 3.236789524310327
Span Start Loss: 1.4962745035059386
Span End Loss: 1.3899910199965213
Type Loss: 0.3505239468047044
 ------------ Epoch 3/10 Batch 7200/11616 Training Results ------------ 
Total Loss: 3.237730681901384
Span Start Loss: 1.4967639406304807
Span End Loss: 1.3901890044152323
Type Loss: 0.3507776827018501
 ------------ Epoch 3/10 Batch 7250/11616 Training Results ------------ 
Total Loss: 3.238593778790071
Span Start Loss: 1.497246046550315
Span End Loss: 1.390727073680738
Type Loss: 0.3506206045266369
 ------------ Epoch 3/10 Batch 7300/11616 Training Results ------------ 
Total Loss: 3.2391163307356914
Span Start Loss: 1.497572182669011
Span End Loss: 1.39079828611894
Type Loss: 0.35074580795169896
 ------------ Epoch 3/10 Batch 7350/11616 Training Results ------------ 
Total Loss: 3.238299682741465
Span Start Loss: 1.49739649991904
Span End Loss: 1.3900949076781062
Type Loss: 0.3508082208992774
 ------------ Epoch 3/10 Batch 7400/11616 Training Results ------------ 
Total Loss: 3.2399833628505066
Span Start Loss: 1.498478227792358
Span End Loss: 1.3908322565489122
Type Loss: 0.3506728242717784
 ------------ Epoch 3/10 Batch 7450/11616 Training Results ------------ 
Total Loss: 3.2385544563149846
Span Start Loss: 1.498023820846473
Span End Loss: 1.3897979086507486
Type Loss: 0.3507326724537767
 ------------ Epoch 3/10 Batch 7500/11616 Training Results ------------ 
Total Loss: 3.24004606291155
Span Start Loss: 1.4991964730451504
Span End Loss: 1.3903014463335275
Type Loss: 0.35054808897102874
 ------------ Epoch 3/10 Batch 7550/11616 Training Results ------------ 
Total Loss: 3.241351548996785
Span Start Loss: 1.4997730359484422
Span End Loss: 1.3913394948178963
Type Loss: 0.3502389637741032
 ------------ Epoch 3/10 Batch 7600/11616 Training Results ------------ 
Total Loss: 3.242754940732725
Span Start Loss: 1.500693696804933
Span End Loss: 1.3917615844319133
Type Loss: 0.3502996050546828
 ------------ Epoch 3/10 Batch 7650/11616 Training Results ------------ 
Total Loss: 3.2459055076091508
Span Start Loss: 1.5025728448358626
Span End Loss: 1.3930830159423009
Type Loss: 0.35024959214469964
 ------------ Epoch 3/10 Batch 7700/11616 Training Results ------------ 
Total Loss: 3.2454531086100773
Span Start Loss: 1.5023676728999074
Span End Loss: 1.3930240720191172
Type Loss: 0.35006130890703047
 ------------ Epoch 3/10 Batch 7750/11616 Training Results ------------ 
Total Loss: 3.245657725598543
Span Start Loss: 1.5024783142180211
Span End Loss: 1.393210402260865
Type Loss: 0.34996895449416293
 ------------ Epoch 3/10 Batch 7800/11616 Training Results ------------ 
Total Loss: 3.2455790822179273
Span Start Loss: 1.5024300120799587
Span End Loss: 1.3930635819708308
Type Loss: 0.3500854335441135
 ------------ Epoch 3/10 Batch 7850/11616 Training Results ------------ 
Total Loss: 3.2467909510205883
Span Start Loss: 1.5031902293784982
Span End Loss: 1.3932598827171858
Type Loss: 0.350340784315708
 ------------ Epoch 3/10 Batch 7900/11616 Training Results ------------ 
Total Loss: 3.2460388415363393
Span Start Loss: 1.5028183326983375
Span End Loss: 1.3928354948391266
Type Loss: 0.3503849593104347
 ------------ Epoch 3/10 Batch 7950/11616 Training Results ------------ 
Total Loss: 3.2456755912763144
Span Start Loss: 1.5030635374925048
Span End Loss: 1.392197964770614
Type Loss: 0.35041403451430725
 ------------ Epoch 3/10 Batch 8000/11616 Training Results ------------ 
Total Loss: 3.246196704073809
Span Start Loss: 1.503132211244665
Span End Loss: 1.392738790968433
Type Loss: 0.35032564725377596
 --------------- Epoch 3/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 56.9, 'f1': 66.9, 'turns': 1425}), ('literature', {'em': 53.7, 'f1': 63.0, 'turns': 1630}), ('mid-high_school', {'em': 54.7, 'f1': 65.5, 'turns': 1653}), ('news', {'em': 58.1, 'f1': 68.8, 'turns': 1649}), ('wikipedia', {'em': 59.8, 'f1': 70.8, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 56.6, 'f1': 67.0, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 56.6, 'f1': 67.0, 'turns': 7983})])
 ------------ Epoch 3/10 Batch 8050/11616 Training Results ------------ 
Total Loss: 3.2457301127568163
Span Start Loss: 1.5025739133163638
Span End Loss: 1.39279956002228
Type Loss: 0.35035658468185743
 ------------ Epoch 3/10 Batch 8100/11616 Training Results ------------ 
Total Loss: 3.2451539756477246
Span Start Loss: 1.5022073620585379
Span End Loss: 1.392592968997764
Type Loss: 0.35035358971007813
 ------------ Epoch 3/10 Batch 8150/11616 Training Results ------------ 
Total Loss: 3.2474867526327538
Span Start Loss: 1.5035104361258766
Span End Loss: 1.3936616632927414
Type Loss: 0.35031459834449497
 ------------ Epoch 3/10 Batch 8200/11616 Training Results ------------ 
Total Loss: 3.247910970590282
Span Start Loss: 1.5035214009544835
Span End Loss: 1.3939656074763072
Type Loss: 0.3504239072357645
 ------------ Epoch 3/10 Batch 8250/11616 Training Results ------------ 
Total Loss: 3.24891137433323
Span Start Loss: 1.504491822754795
Span End Loss: 1.3941920175353686
Type Loss: 0.35022747908555196
 ------------ Epoch 3/10 Batch 8300/11616 Training Results ------------ 
Total Loss: 3.250245768096612
Span Start Loss: 1.505141010534871
Span End Loss: 1.3948718765431858
Type Loss: 0.3502328260660351
 ------------ Epoch 3/10 Batch 8350/11616 Training Results ------------ 
Total Loss: 3.2515328207906493
Span Start Loss: 1.5063820942495754
Span End Loss: 1.3950948317125886
Type Loss: 0.35005583961745224
 ------------ Epoch 3/10 Batch 8400/11616 Training Results ------------ 
Total Loss: 3.2515247805088405
Span Start Loss: 1.5063608691122916
Span End Loss: 1.3952206025041995
Type Loss: 0.3499432538848902
 ------------ Epoch 3/10 Batch 8450/11616 Training Results ------------ 
Total Loss: 3.252481097489948
Span Start Loss: 1.506657970596171
Span End Loss: 1.395749783246122
Type Loss: 0.35007328870292953
 ------------ Epoch 3/10 Batch 8500/11616 Training Results ------------ 
Total Loss: 3.2527914814344223
Span Start Loss: 1.5066724917406544
Span End Loss: 1.3958969968189212
Type Loss: 0.35022193768269877
 ------------ Epoch 3/10 Batch 8550/11616 Training Results ------------ 
Total Loss: 3.253445083743822
Span Start Loss: 1.5068578906409573
Span End Loss: 1.3962758486086164
Type Loss: 0.3503112893189952
 ------------ Epoch 3/10 Batch 8600/11616 Training Results ------------ 
Total Loss: 3.2530321587613504
Span Start Loss: 1.5066103114300342
Span End Loss: 1.39646506087378
Type Loss: 0.34995673135052935
 ------------ Epoch 3/10 Batch 8650/11616 Training Results ------------ 
Total Loss: 3.253320242796158
Span Start Loss: 1.5069209461138082
Span End Loss: 1.3967518734897493
Type Loss: 0.3496473679568812
 ------------ Epoch 3/10 Batch 8700/11616 Training Results ------------ 
Total Loss: 3.2532701754578572
Span Start Loss: 1.5069341361702515
Span End Loss: 1.39685901133836
Type Loss: 0.3494769727749129
 ------------ Epoch 3/10 Batch 8750/11616 Training Results ------------ 
Total Loss: 3.254023378921407
Span Start Loss: 1.5069483665611063
Span End Loss: 1.397696253803798
Type Loss: 0.34937870348564215
 ------------ Epoch 3/10 Batch 8800/11616 Training Results ------------ 
Total Loss: 3.2538179425819016
Span Start Loss: 1.5070125215644525
Span End Loss: 1.3973167901079764
Type Loss: 0.349488575917203
 ------------ Epoch 3/10 Batch 8850/11616 Training Results ------------ 
Total Loss: 3.2536919727121707
Span Start Loss: 1.5068345360272686
Span End Loss: 1.3976961646123796
Type Loss: 0.3491612172911817
 ------------ Epoch 3/10 Batch 8900/11616 Training Results ------------ 
Total Loss: 3.2537543441514285
Span Start Loss: 1.5066195541963483
Span End Loss: 1.397766265010231
Type Loss: 0.34936847049470865
 ------------ Epoch 3/10 Batch 8950/11616 Training Results ------------ 
Total Loss: 3.253746013797029
Span Start Loss: 1.5064513666145296
Span End Loss: 1.3980395149768399
Type Loss: 0.3492550775897819
 ------------ Epoch 3/10 Batch 9000/11616 Training Results ------------ 
Total Loss: 3.254527545020812
Span Start Loss: 1.5068207634033428
Span End Loss: 1.3986355599678224
Type Loss: 0.34907116696486873
 ------------ Epoch 3/10 Batch 9050/11616 Training Results ------------ 
Total Loss: 3.253873451338124
Span Start Loss: 1.5065828200178581
Span End Loss: 1.3985104310726593
Type Loss: 0.34878014547359304
 ------------ Epoch 3/10 Batch 9100/11616 Training Results ------------ 
Total Loss: 3.254553000886853
Span Start Loss: 1.5066668720240448
Span End Loss: 1.3990219719413217
Type Loss: 0.3488641022539769
 ------------ Epoch 3/10 Batch 9150/11616 Training Results ------------ 
Total Loss: 3.253574303342182
Span Start Loss: 1.5063275469204442
Span End Loss: 1.3986727434217605
Type Loss: 0.3485739584628472
 ------------ Epoch 3/10 Batch 9200/11616 Training Results ------------ 
Total Loss: 3.253696662264512
Span Start Loss: 1.506275725958302
Span End Loss: 1.3989683788091591
Type Loss: 0.34845250290079527
 ------------ Epoch 3/10 Batch 9250/11616 Training Results ------------ 
Total Loss: 3.254033643333493
Span Start Loss: 1.5063313945475463
Span End Loss: 1.3988588243062432
Type Loss: 0.3488433699047042
 ------------ Epoch 3/10 Batch 9300/11616 Training Results ------------ 
Total Loss: 3.2564002872675015
Span Start Loss: 1.5074475128972722
Span End Loss: 1.4000850948098527
Type Loss: 0.3488676247429303
 ------------ Epoch 3/10 Batch 9350/11616 Training Results ------------ 
Total Loss: 3.256639754632737
Span Start Loss: 1.5074932783308195
Span End Loss: 1.4004261804471678
Type Loss: 0.3487202412185862
 ------------ Epoch 3/10 Batch 9400/11616 Training Results ------------ 
Total Loss: 3.256405151623519
Span Start Loss: 1.5078432092839416
Span End Loss: 1.400171954249765
Type Loss: 0.3483899333612359
 ------------ Epoch 3/10 Batch 9450/11616 Training Results ------------ 
Total Loss: 3.2572233172990974
Span Start Loss: 1.508567950265906
Span End Loss: 1.4004903931892108
Type Loss: 0.3481649191487364
 ------------ Epoch 3/10 Batch 9500/11616 Training Results ------------ 
Total Loss: 3.2573297563705004
Span Start Loss: 1.5088098056198735
Span End Loss: 1.4003452610781317
Type Loss: 0.34817463488396455
 ------------ Epoch 3/10 Batch 9550/11616 Training Results ------------ 
Total Loss: 3.2576432017638735
Span Start Loss: 1.5089114295570325
Span End Loss: 1.4004340790063923
Type Loss: 0.34829763846783973
 ------------ Epoch 3/10 Batch 9600/11616 Training Results ------------ 
Total Loss: 3.2555207986811485
Span Start Loss: 1.5080403686927941
Span End Loss: 1.3992955189508696
Type Loss: 0.3481848563311117
 ------------ Epoch 3/10 Batch 9650/11616 Training Results ------------ 
Total Loss: 3.254481087644329
Span Start Loss: 1.5078401725488315
Span End Loss: 1.39854351101144
Type Loss: 0.3480973493289492
 ------------ Epoch 3/10 Batch 9700/11616 Training Results ------------ 
Total Loss: 3.257010110632814
Span Start Loss: 1.509436858286409
Span End Loss: 1.3995709828156786
Type Loss: 0.3480022149171066
 ------------ Epoch 3/10 Batch 9750/11616 Training Results ------------ 
Total Loss: 3.2578907938530812
Span Start Loss: 1.5098888696760704
Span End Loss: 1.4002018111210603
Type Loss: 0.34780005825358706
 ------------ Epoch 3/10 Batch 9800/11616 Training Results ------------ 
Total Loss: 3.2567233792639203
Span Start Loss: 1.5096347956488632
Span End Loss: 1.399232333074419
Type Loss: 0.3478561957190954
 ------------ Epoch 3/10 Batch 9850/11616 Training Results ------------ 
Total Loss: 3.256788326637243
Span Start Loss: 1.509634020892043
Span End Loss: 1.399335356421277
Type Loss: 0.347818894504196
 ------------ Epoch 3/10 Batch 9900/11616 Training Results ------------ 
Total Loss: 3.256314504425032
Span Start Loss: 1.5096806612467826
Span End Loss: 1.398688150155123
Type Loss: 0.3479456381720839
 ------------ Epoch 3/10 Batch 9950/11616 Training Results ------------ 
Total Loss: 3.256979384891052
Span Start Loss: 1.510003343536776
Span End Loss: 1.398886886930945
Type Loss: 0.34808909953979017
 ------------ Epoch 3/10 Batch 10000/11616 Training Results ------------ 
Total Loss: 3.25864854683727
Span Start Loss: 1.510991382522136
Span End Loss: 1.3997178579807281
Type Loss: 0.3479392512806691
 --------------- Epoch 3/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 57.9, 'f1': 68.4, 'turns': 1425}), ('literature', {'em': 53.5, 'f1': 63.7, 'turns': 1630}), ('mid-high_school', {'em': 52.9, 'f1': 64.4, 'turns': 1653}), ('news', {'em': 58.7, 'f1': 68.8, 'turns': 1649}), ('wikipedia', {'em': 60.9, 'f1': 72.0, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 56.8, 'f1': 67.4, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 56.8, 'f1': 67.4, 'turns': 7983})])
 ------------ Epoch 3/10 Batch 10050/11616 Training Results ------------ 
Total Loss: 3.259846174783671
Span Start Loss: 1.5116544372295562
Span End Loss: 1.4000653410610275
Type Loss: 0.34812634134591
 ------------ Epoch 3/10 Batch 10100/11616 Training Results ------------ 
Total Loss: 3.2595501576024706
Span Start Loss: 1.5118427899664286
Span End Loss: 1.3999317636525277
Type Loss: 0.3477755488262576
 ------------ Epoch 3/10 Batch 10150/11616 Training Results ------------ 
Total Loss: 3.2585312890154974
Span Start Loss: 1.5109757290780543
Span End Loss: 1.3998238604468078
Type Loss: 0.3477316443103691
 ------------ Epoch 3/10 Batch 10200/11616 Training Results ------------ 
Total Loss: 3.258909640095982
Span Start Loss: 1.5112538687307753
Span End Loss: 1.3998796760217815
Type Loss: 0.34777604012996616
 ------------ Epoch 3/10 Batch 10250/11616 Training Results ------------ 
Total Loss: 3.260422222788741
Span Start Loss: 1.511980277809428
Span End Loss: 1.4004366813694558
Type Loss: 0.34800520850181943
 ------------ Epoch 3/10 Batch 10300/11616 Training Results ------------ 
Total Loss: 3.261054061888491
Span Start Loss: 1.5124834792569135
Span End Loss: 1.4007532999642844
Type Loss: 0.3478172276661849
 ------------ Epoch 3/10 Batch 10350/11616 Training Results ------------ 
Total Loss: 3.2627064707846456
Span Start Loss: 1.5134864192891524
Span End Loss: 1.4013555041373065
Type Loss: 0.34786449238594963
 ------------ Epoch 3/10 Batch 10400/11616 Training Results ------------ 
Total Loss: 3.262806100068757
Span Start Loss: 1.5135389409593951
Span End Loss: 1.401515141375936
Type Loss: 0.34775196265653135
 ------------ Epoch 3/10 Batch 10450/11616 Training Results ------------ 
Total Loss: 3.2631053883778423
Span Start Loss: 1.5132911975111403
Span End Loss: 1.4019219758265327
Type Loss: 0.34789215986960026
 ------------ Epoch 3/10 Batch 10500/11616 Training Results ------------ 
Total Loss: 3.263772526106664
Span Start Loss: 1.5137526050813142
Span End Loss: 1.4021204111803145
Type Loss: 0.34789945478355955
 ------------ Epoch 3/10 Batch 10550/11616 Training Results ------------ 
Total Loss: 3.263852114989577
Span Start Loss: 1.5139830971872918
Span End Loss: 1.402072174379611
Type Loss: 0.3477967884272372
 ------------ Epoch 3/10 Batch 10600/11616 Training Results ------------ 
Total Loss: 3.2642259492415864
Span Start Loss: 1.514152167224378
Span End Loss: 1.4022766203784718
Type Loss: 0.34779710651858586
 ------------ Epoch 3/10 Batch 10650/11616 Training Results ------------ 
Total Loss: 3.2639622374249737
Span Start Loss: 1.5139382937410628
Span End Loss: 1.4023210909500927
Type Loss: 0.34770279763633727
 ------------ Epoch 3/10 Batch 10700/11616 Training Results ------------ 
Total Loss: 3.2658762053838957
Span Start Loss: 1.5150377243255901
Span End Loss: 1.402865838946026
Type Loss: 0.3479725869778091
 ------------ Epoch 3/10 Batch 10750/11616 Training Results ------------ 
Total Loss: 3.2662216794975967
Span Start Loss: 1.5151112266473992
Span End Loss: 1.4027010328520175
Type Loss: 0.3484093648373734
 ------------ Epoch 3/10 Batch 10800/11616 Training Results ------------ 
Total Loss: 3.267833769756059
Span Start Loss: 1.516019707934724
Span End Loss: 1.403324309559884
Type Loss: 0.3484896971511275
 ------------ Epoch 3/10 Batch 10850/11616 Training Results ------------ 
Total Loss: 3.2685046587465547
Span Start Loss: 1.5163940582160027
Span End Loss: 1.40362652803621
Type Loss: 0.3484840174333497
 ------------ Epoch 3/10 Batch 10900/11616 Training Results ------------ 
Total Loss: 3.269508641595961
Span Start Loss: 1.516510275763656
Span End Loss: 1.4038073237554742
Type Loss: 0.3491909870086665
 ------------ Epoch 3/10 Batch 10950/11616 Training Results ------------ 
Total Loss: 3.2693283564764073
Span Start Loss: 1.516448396049678
Span End Loss: 1.4036224757917395
Type Loss: 0.349257429679473
 ------------ Epoch 3/10 Batch 11000/11616 Training Results ------------ 
Total Loss: 3.267960594310002
Span Start Loss: 1.5157856900610707
Span End Loss: 1.4028323474404487
Type Loss: 0.34934250185689464
 ------------ Epoch 3/10 Batch 11050/11616 Training Results ------------ 
Total Loss: 3.2668929688396497
Span Start Loss: 1.5155345078156544
Span End Loss: 1.4022516590046668
Type Loss: 0.3491067471687273
 ------------ Epoch 3/10 Batch 11100/11616 Training Results ------------ 
Total Loss: 3.2674590921012667
Span Start Loss: 1.515709430594165
Span End Loss: 1.4025065046454872
Type Loss: 0.34924310197986597
 ------------ Epoch 3/10 Batch 11150/11616 Training Results ------------ 
Total Loss: 3.2677877617456987
Span Start Loss: 1.5159805690787833
Span End Loss: 1.4023250293183753
Type Loss: 0.3494821084583086
 ------------ Epoch 3/10 Batch 11200/11616 Training Results ------------ 
Total Loss: 3.269325970272933
Span Start Loss: 1.5167138215872857
Span End Loss: 1.4029206907868919
Type Loss: 0.3496914029660236
 ------------ Epoch 3/10 Batch 11250/11616 Training Results ------------ 
Total Loss: 3.269828554266029
Span Start Loss: 1.517050931345092
Span End Loss: 1.40320540604194
Type Loss: 0.3495721618387434
 ------------ Epoch 3/10 Batch 11300/11616 Training Results ------------ 
Total Loss: 3.2686021508566574
Span Start Loss: 1.5164466007251656
Span End Loss: 1.4026773266705264
Type Loss: 0.3494781684549113
 ------------ Epoch 3/10 Batch 11350/11616 Training Results ------------ 
Total Loss: 3.2685463641642998
Span Start Loss: 1.5164345180476289
Span End Loss: 1.402609549682374
Type Loss: 0.34950224136695723
 ------------ Epoch 3/10 Batch 11400/11616 Training Results ------------ 
Total Loss: 3.2695882984355356
Span Start Loss: 1.5171544964990595
Span End Loss: 1.402900871315873
Type Loss: 0.3495328756346645
 ------------ Epoch 3/10 Batch 11450/11616 Training Results ------------ 
Total Loss: 3.269755398537394
Span Start Loss: 1.5173642588039153
Span End Loss: 1.4027947770582927
Type Loss: 0.34959630768144234
 ------------ Epoch 3/10 Batch 11500/11616 Training Results ------------ 
Total Loss: 3.2689399149495624
Span Start Loss: 1.5170724059252636
Span End Loss: 1.4023950137933312
Type Loss: 0.34947244028520325
 ------------ Epoch 3/10 Batch 11550/11616 Training Results ------------ 
Total Loss: 3.2681533701298555
Span Start Loss: 1.516852912292594
Span End Loss: 1.402101644643651
Type Loss: 0.3491987581696216
 ------------ Epoch 3/10 Batch 11600/11616 Training Results ------------ 
Total Loss: 3.2688676844575797
Span Start Loss: 1.5174291972083778
Span End Loss: 1.402354616835906
Type Loss: 0.3490838153342363
 --------------------- Epoch 3/10 Final Training Results ------------------------ 
Total Loss: 3.2702814387580887
Span Start Loss: 1.5180943137016814
Span End Loss: 1.403040211806497
Type Loss: 0.3491468582024603
 --------------- Epoch 3/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 56.6, 'f1': 67.2, 'turns': 1425}), ('literature', {'em': 51.2, 'f1': 60.8, 'turns': 1630}), ('mid-high_school', {'em': 51.8, 'f1': 62.8, 'turns': 1653}), ('news', {'em': 56.4, 'f1': 66.9, 'turns': 1649}), ('wikipedia', {'em': 57.2, 'f1': 68.8, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 54.6, 'f1': 65.3, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 54.6, 'f1': 65.3, 'turns': 7983})])
 --------------- Epoch 4/10 Training Start --------------- 
 ------------ Epoch 4/10 Batch 50/11616 Training Results ------------ 
Total Loss: 2.835409423708916
Span Start Loss: 1.250551359653473
Span End Loss: 1.1370083406567573
Type Loss: 0.44784967597573994
 ------------ Epoch 4/10 Batch 100/11616 Training Results ------------ 
Total Loss: 2.668961700797081
Span Start Loss: 1.1762397316098214
Span End Loss: 1.0932429434359074
Type Loss: 0.3994789702352136
 ------------ Epoch 4/10 Batch 150/11616 Training Results ------------ 
Total Loss: 2.604837344586849
Span Start Loss: 1.1602233383059501
Span End Loss: 1.0615490412712096
Type Loss: 0.38306491790339353
 ------------ Epoch 4/10 Batch 200/11616 Training Results ------------ 
Total Loss: 2.5419163044542072
Span Start Loss: 1.127553926333785
Span End Loss: 1.049459633305669
Type Loss: 0.36490269950125365
 ------------ Epoch 4/10 Batch 250/11616 Training Results ------------ 
Total Loss: 2.5039080199599266
Span Start Loss: 1.1057578638792038
Span End Loss: 1.0428477905392646
Type Loss: 0.3553023238815367
 ------------ Epoch 4/10 Batch 300/11616 Training Results ------------ 
Total Loss: 2.4923508169750375
Span Start Loss: 1.10247563076516
Span End Loss: 1.0383614182968934
Type Loss: 0.351513727887844
 ------------ Epoch 4/10 Batch 350/11616 Training Results ------------ 
Total Loss: 2.5004923558660916
Span Start Loss: 1.098543342437063
Span End Loss: 1.0450301548412868
Type Loss: 0.35691881312589563
 ------------ Epoch 4/10 Batch 400/11616 Training Results ------------ 
Total Loss: 2.4736759996041657
Span Start Loss: 1.0870592275634408
Span End Loss: 1.0340226117894054
Type Loss: 0.3525941122951917
 ------------ Epoch 4/10 Batch 450/11616 Training Results ------------ 
Total Loss: 2.4609252277347777
Span Start Loss: 1.0838421810335583
Span End Loss: 1.0254560386637848
Type Loss: 0.3516269643087354
 ------------ Epoch 4/10 Batch 500/11616 Training Results ------------ 
Total Loss: 2.4659429417550562
Span Start Loss: 1.085980996966362
Span End Loss: 1.0260049109905958
Type Loss: 0.35395699019171295
 ------------ Epoch 4/10 Batch 550/11616 Training Results ------------ 
Total Loss: 2.477159252085469
Span Start Loss: 1.09600339141759
Span End Loss: 1.0307855175706473
Type Loss: 0.3503702996755865
 ------------ Epoch 4/10 Batch 600/11616 Training Results ------------ 
Total Loss: 2.457414644708236
Span Start Loss: 1.0899466764430206
Span End Loss: 1.020122777633369
Type Loss: 0.3473451465756322
 ------------ Epoch 4/10 Batch 650/11616 Training Results ------------ 
Total Loss: 2.45858100475027
Span Start Loss: 1.0919881012806525
Span End Loss: 1.0217066458440744
Type Loss: 0.34488621366281924
 ------------ Epoch 4/10 Batch 700/11616 Training Results ------------ 
Total Loss: 2.4442103618489845
Span Start Loss: 1.0855586253106595
Span End Loss: 1.0155917669513397
Type Loss: 0.34305992496998183
 ------------ Epoch 4/10 Batch 750/11616 Training Results ------------ 
Total Loss: 2.4527272674938043
Span Start Loss: 1.088617191846172
Span End Loss: 1.0200377972722054
Type Loss: 0.3440722321979702
 ------------ Epoch 4/10 Batch 800/11616 Training Results ------------ 
Total Loss: 2.445049327099696
Span Start Loss: 1.0871229801280424
Span End Loss: 1.0145551034808158
Type Loss: 0.3433711982378736
 ------------ Epoch 4/10 Batch 850/11616 Training Results ------------ 
Total Loss: 2.4557138830861627
Span Start Loss: 1.0954762825501316
Span End Loss: 1.0157302399593242
Type Loss: 0.34450731579214333
 ------------ Epoch 4/10 Batch 900/11616 Training Results ------------ 
Total Loss: 2.4539551167438427
Span Start Loss: 1.096183897252712
Span End Loss: 1.0164530552923678
Type Loss: 0.34131811997542777
 ------------ Epoch 4/10 Batch 950/11616 Training Results ------------ 
Total Loss: 2.4548645528523547
Span Start Loss: 1.0969178675311175
Span End Loss: 1.0153727272937172
Type Loss: 0.3425739141710495
 ------------ Epoch 4/10 Batch 1000/11616 Training Results ------------ 
Total Loss: 2.4667130044847725
Span Start Loss: 1.1020956609509884
Span End Loss: 1.022872795909643
Type Loss: 0.3417445039190352
 ------------ Epoch 4/10 Batch 1050/11616 Training Results ------------ 
Total Loss: 2.4568613693118095
Span Start Loss: 1.0992826175512302
Span End Loss: 1.0169294535617033
Type Loss: 0.34064925634080456
 ------------ Epoch 4/10 Batch 1100/11616 Training Results ------------ 
Total Loss: 2.45666232644157
Span Start Loss: 1.0953718671981585
Span End Loss: 1.021640625209971
Type Loss: 0.3396497933261774
 ------------ Epoch 4/10 Batch 1150/11616 Training Results ------------ 
Total Loss: 2.4680038305080454
Span Start Loss: 1.1024602314927008
Span End Loss: 1.0291178662362306
Type Loss: 0.33642569219612556
 ------------ Epoch 4/10 Batch 1200/11616 Training Results ------------ 
Total Loss: 2.4775947354113064
Span Start Loss: 1.106117974789813
Span End Loss: 1.0346890330314635
Type Loss: 0.3367876869160682
 ------------ Epoch 4/10 Batch 1250/11616 Training Results ------------ 
Total Loss: 2.480616782784462
Span Start Loss: 1.1071365949243308
Span End Loss: 1.0360328443050384
Type Loss: 0.33744730129987
 ------------ Epoch 4/10 Batch 1300/11616 Training Results ------------ 
Total Loss: 2.4789893899972624
Span Start Loss: 1.1052716779909455
Span End Loss: 1.0350964727080785
Type Loss: 0.3386211972927245
 ------------ Epoch 4/10 Batch 1350/11616 Training Results ------------ 
Total Loss: 2.4729517456833965
Span Start Loss: 1.1002013197182505
Span End Loss: 1.0356278784738646
Type Loss: 0.33712250468217664
 ------------ Epoch 4/10 Batch 1400/11616 Training Results ------------ 
Total Loss: 2.477868462726474
Span Start Loss: 1.1030813411809504
Span End Loss: 1.0372389867050307
Type Loss: 0.33754809249325524
 ------------ Epoch 4/10 Batch 1450/11616 Training Results ------------ 
Total Loss: 2.474321699091073
Span Start Loss: 1.1002088453826206
Span End Loss: 1.0359588167996243
Type Loss: 0.3381539952703591
 ------------ Epoch 4/10 Batch 1500/11616 Training Results ------------ 
Total Loss: 2.4755422900617123
Span Start Loss: 1.1039182421043514
Span End Loss: 1.0342445054749647
Type Loss: 0.3373795009888709
 ------------ Epoch 4/10 Batch 1550/11616 Training Results ------------ 
Total Loss: 2.472689921480994
Span Start Loss: 1.1030793229490519
Span End Loss: 1.0327739761721704
Type Loss: 0.3368365809669898
 ------------ Epoch 4/10 Batch 1600/11616 Training Results ------------ 
Total Loss: 2.478059369768016
Span Start Loss: 1.1058618610235862
Span End Loss: 1.0364939648658038
Type Loss: 0.33570350177702496
 ------------ Epoch 4/10 Batch 1650/11616 Training Results ------------ 
Total Loss: 2.4774530729335367
Span Start Loss: 1.105762887332927
Span End Loss: 1.0371692367214145
Type Loss: 0.3345209065401419
 ------------ Epoch 4/10 Batch 1700/11616 Training Results ------------ 
Total Loss: 2.482801338570083
Span Start Loss: 1.1088226915698718
Span End Loss: 1.0390809275209905
Type Loss: 0.3348976765408674
 ------------ Epoch 4/10 Batch 1750/11616 Training Results ------------ 
Total Loss: 2.4820265416375227
Span Start Loss: 1.109884696558118
Span End Loss: 1.0386542820845333
Type Loss: 0.3334875202849507
 ------------ Epoch 4/10 Batch 1800/11616 Training Results ------------ 
Total Loss: 2.478094202838838
Span Start Loss: 1.1077799028137492
Span End Loss: 1.0359510480239988
Type Loss: 0.33436320926580165
 ------------ Epoch 4/10 Batch 1850/11616 Training Results ------------ 
Total Loss: 2.4795137038706123
Span Start Loss: 1.1101392937975156
Span End Loss: 1.035602699957184
Type Loss: 0.33377166792347623
 ------------ Epoch 4/10 Batch 1900/11616 Training Results ------------ 
Total Loss: 2.485139786040312
Span Start Loss: 1.1113012599925463
Span End Loss: 1.0400767296748725
Type Loss: 0.3337617542222142
 ------------ Epoch 4/10 Batch 1950/11616 Training Results ------------ 
Total Loss: 2.486107069043777
Span Start Loss: 1.1129840694310573
Span End Loss: 1.0399291821511891
Type Loss: 0.3331937750772788
 ------------ Epoch 4/10 Batch 2000/11616 Training Results ------------ 
Total Loss: 2.480847050752491
Span Start Loss: 1.1134308855962007
Span End Loss: 1.0348070418946445
Type Loss: 0.3326090809945017
 --------------- Epoch 4/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 56.7, 'f1': 67.6, 'turns': 1425}), ('literature', {'em': 53.7, 'f1': 63.8, 'turns': 1630}), ('mid-high_school', {'em': 55.8, 'f1': 66.6, 'turns': 1653}), ('news', {'em': 55.8, 'f1': 66.9, 'turns': 1649}), ('wikipedia', {'em': 60.6, 'f1': 71.9, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 56.5, 'f1': 67.3, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 56.5, 'f1': 67.3, 'turns': 7983})])
 ------------ Epoch 4/10 Batch 2050/11616 Training Results ------------ 
Total Loss: 2.4837833355294494
Span Start Loss: 1.1143830967658177
Span End Loss: 1.0364167232011876
Type Loss: 0.3329834727134283
 ------------ Epoch 4/10 Batch 2100/11616 Training Results ------------ 
Total Loss: 2.4796355524056013
Span Start Loss: 1.1111335599972378
Span End Loss: 1.0358475222722405
Type Loss: 0.3326544273777732
 ------------ Epoch 4/10 Batch 2150/11616 Training Results ------------ 
Total Loss: 2.473871493634096
Span Start Loss: 1.1073885720673688
Span End Loss: 1.033760825017857
Type Loss: 0.33272205423477086
 ------------ Epoch 4/10 Batch 2200/11616 Training Results ------------ 
Total Loss: 2.4754604178904134
Span Start Loss: 1.108850357984616
Span End Loss: 1.0344813573462042
Type Loss: 0.3321286597560075
 ------------ Epoch 4/10 Batch 2250/11616 Training Results ------------ 
Total Loss: 2.4767773283355767
Span Start Loss: 1.1084927802731595
Span End Loss: 1.0349738290044996
Type Loss: 0.3333106762924128
 ------------ Epoch 4/10 Batch 2300/11616 Training Results ------------ 
Total Loss: 2.478277867303594
Span Start Loss: 1.1077691376581789
Span End Loss: 1.0365873687358007
Type Loss: 0.3339213184323972
 ------------ Epoch 4/10 Batch 2350/11616 Training Results ------------ 
Total Loss: 2.482171540992691
Span Start Loss: 1.1097708868964555
Span End Loss: 1.0377298409127174
Type Loss: 0.3346707708016038
 ------------ Epoch 4/10 Batch 2400/11616 Training Results ------------ 
Total Loss: 2.4791216160946834
Span Start Loss: 1.109075323981233
Span End Loss: 1.0362258178616564
Type Loss: 0.3338204320326137
 ------------ Epoch 4/10 Batch 2450/11616 Training Results ------------ 
Total Loss: 2.481677880898422
Span Start Loss: 1.11055322711413
Span End Loss: 1.0375759437254497
Type Loss: 0.333548667833057
 ------------ Epoch 4/10 Batch 2500/11616 Training Results ------------ 
Total Loss: 2.480034962579608
Span Start Loss: 1.1101750725015997
Span End Loss: 1.0375187774777412
Type Loss: 0.3323410698328167
 ------------ Epoch 4/10 Batch 2550/11616 Training Results ------------ 
Total Loss: 2.4824628575029326
Span Start Loss: 1.11276216531355
Span End Loss: 1.037509459672021
Type Loss: 0.33219118965534017
 ------------ Epoch 4/10 Batch 2600/11616 Training Results ------------ 
Total Loss: 2.4808747697908147
Span Start Loss: 1.1119263046048582
Span End Loss: 1.037438942961968
Type Loss: 0.33150947867319563
 ------------ Epoch 4/10 Batch 2650/11616 Training Results ------------ 
Total Loss: 2.478536896744989
Span Start Loss: 1.11156471307407
Span End Loss: 1.0359268749322532
Type Loss: 0.33104526550913194
 ------------ Epoch 4/10 Batch 2700/11616 Training Results ------------ 
Total Loss: 2.479816805736886
Span Start Loss: 1.110896361755828
Span End Loss: 1.0373545471164916
Type Loss: 0.3315658534090552
 ------------ Epoch 4/10 Batch 2750/11616 Training Results ------------ 
Total Loss: 2.4834407380765136
Span Start Loss: 1.1133103313649242
Span End Loss: 1.038214423418045
Type Loss: 0.3319159400984645
 ------------ Epoch 4/10 Batch 2800/11616 Training Results ------------ 
Total Loss: 2.4809085366183092
Span Start Loss: 1.1113222551971143
Span End Loss: 1.03848882909332
Type Loss: 0.3310974096440311
 ------------ Epoch 4/10 Batch 2850/11616 Training Results ------------ 
Total Loss: 2.484035582879656
Span Start Loss: 1.1128535708249139
Span End Loss: 1.0399564118866336
Type Loss: 0.3312255572991674
 ------------ Epoch 4/10 Batch 2900/11616 Training Results ------------ 
Total Loss: 2.4834988670230937
Span Start Loss: 1.113199401553078
Span End Loss: 1.0386022667740953
Type Loss: 0.3316971560908032
 ------------ Epoch 4/10 Batch 2950/11616 Training Results ------------ 
Total Loss: 2.4809604292325043
Span Start Loss: 1.1119210942152697
Span End Loss: 1.0377882840067654
Type Loss: 0.33125100840514493
 ------------ Epoch 4/10 Batch 3000/11616 Training Results ------------ 
Total Loss: 2.480186126641929
Span Start Loss: 1.112446407262236
Span End Loss: 1.0371083424737055
Type Loss: 0.33063133431288105
 ------------ Epoch 4/10 Batch 3050/11616 Training Results ------------ 
Total Loss: 2.4748691041938593
Span Start Loss: 1.1092218365776734
Span End Loss: 1.0349009370559552
Type Loss: 0.33074628814993823
 ------------ Epoch 4/10 Batch 3100/11616 Training Results ------------ 
Total Loss: 2.475564550452175
Span Start Loss: 1.1089294627501118
Span End Loss: 1.034576388385988
Type Loss: 0.3320586566596983
 ------------ Epoch 4/10 Batch 3150/11616 Training Results ------------ 
Total Loss: 2.4776286213525704
Span Start Loss: 1.1107128786473046
Span End Loss: 1.0346296656037133
Type Loss: 0.3322860347509148
 ------------ Epoch 4/10 Batch 3200/11616 Training Results ------------ 
Total Loss: 2.4750609300215727
Span Start Loss: 1.1093901567021385
Span End Loss: 1.0340618108352646
Type Loss: 0.3316089202481089
 ------------ Epoch 4/10 Batch 3250/11616 Training Results ------------ 
Total Loss: 2.4751520083386165
Span Start Loss: 1.1092128621385648
Span End Loss: 1.034839462706676
Type Loss: 0.33109964136722
 ------------ Epoch 4/10 Batch 3300/11616 Training Results ------------ 
Total Loss: 2.4735187112720625
Span Start Loss: 1.1083416968629216
Span End Loss: 1.0346327576537926
Type Loss: 0.3305442149389648
 ------------ Epoch 4/10 Batch 3350/11616 Training Results ------------ 
Total Loss: 2.4736151973369407
Span Start Loss: 1.108385958222311
Span End Loss: 1.0343701248382455
Type Loss: 0.3308590724697309
 ------------ Epoch 4/10 Batch 3400/11616 Training Results ------------ 
Total Loss: 2.474874011591515
Span Start Loss: 1.109144574456355
Span End Loss: 1.035382276326418
Type Loss: 0.33034711921560195
 ------------ Epoch 4/10 Batch 3450/11616 Training Results ------------ 
Total Loss: 2.4785115288975446
Span Start Loss: 1.1105448128023874
Span End Loss: 1.0370899075183315
Type Loss: 0.3308767666913353
 ------------ Epoch 4/10 Batch 3500/11616 Training Results ------------ 
Total Loss: 2.4762787496404988
Span Start Loss: 1.10957399577541
Span End Loss: 1.0360377416653292
Type Loss: 0.33066697014336077
 ------------ Epoch 4/10 Batch 3550/11616 Training Results ------------ 
Total Loss: 2.4753692680891133
Span Start Loss: 1.1084372626790697
Span End Loss: 1.0358851914380638
Type Loss: 0.33104677198008753
 ------------ Epoch 4/10 Batch 3600/11616 Training Results ------------ 
Total Loss: 2.4708697790590426
Span Start Loss: 1.1064236218171815
Span End Loss: 1.0334807499953442
Type Loss: 0.3309653654922214
 ------------ Epoch 4/10 Batch 3650/11616 Training Results ------------ 
Total Loss: 2.4715071584978334
Span Start Loss: 1.10649300374601
Span End Loss: 1.0344184462134152
Type Loss: 0.33059566620380093
 ------------ Epoch 4/10 Batch 3700/11616 Training Results ------------ 
Total Loss: 2.4702846642482923
Span Start Loss: 1.1060418587963323
Span End Loss: 1.0338013166974525
Type Loss: 0.33044144712187146
 ------------ Epoch 4/10 Batch 3750/11616 Training Results ------------ 
Total Loss: 2.4739446154435476
Span Start Loss: 1.1080844958821932
Span End Loss: 1.0361939555545647
Type Loss: 0.32966612230738
 ------------ Epoch 4/10 Batch 3800/11616 Training Results ------------ 
Total Loss: 2.473622604214439
Span Start Loss: 1.1088882603496313
Span End Loss: 1.0356896458036806
Type Loss: 0.3290446561976875
 ------------ Epoch 4/10 Batch 3850/11616 Training Results ------------ 
Total Loss: 2.4761705548829074
Span Start Loss: 1.1110121229600596
Span End Loss: 1.0363132338051673
Type Loss: 0.3288451561781687
 ------------ Epoch 4/10 Batch 3900/11616 Training Results ------------ 
Total Loss: 2.4775200612174393
Span Start Loss: 1.112246437252332
Span End Loss: 1.0365811244150003
Type Loss: 0.3286924577709765
 ------------ Epoch 4/10 Batch 3950/11616 Training Results ------------ 
Total Loss: 2.4815758765403984
Span Start Loss: 1.113828361294692
Span End Loss: 1.0388776184825959
Type Loss: 0.32886985435374555
 ------------ Epoch 4/10 Batch 4000/11616 Training Results ------------ 
Total Loss: 2.4838984457124025
Span Start Loss: 1.1150239625535905
Span End Loss: 1.0399982645753771
Type Loss: 0.32887617640988903
 --------------- Epoch 4/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 57.5, 'f1': 67.5, 'turns': 1425}), ('literature', {'em': 53.3, 'f1': 63.3, 'turns': 1630}), ('mid-high_school', {'em': 54.9, 'f1': 66.2, 'turns': 1653}), ('news', {'em': 58.7, 'f1': 69.1, 'turns': 1649}), ('wikipedia', {'em': 61.2, 'f1': 72.1, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 57.1, 'f1': 67.7, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 57.1, 'f1': 67.7, 'turns': 7983})])
 ------------ Epoch 4/10 Batch 4050/11616 Training Results ------------ 
Total Loss: 2.487733846360151
Span Start Loss: 1.1170966068793227
Span End Loss: 1.0414816227536878
Type Loss: 0.32915557425653125
 ------------ Epoch 4/10 Batch 4100/11616 Training Results ------------ 
Total Loss: 2.4882507215458447
Span Start Loss: 1.1170743142795272
Span End Loss: 1.0421201617852216
Type Loss: 0.32905620316952106
 ------------ Epoch 4/10 Batch 4150/11616 Training Results ------------ 
Total Loss: 2.48578945873193
Span Start Loss: 1.1159691149750388
Span End Loss: 1.0407742730626859
Type Loss: 0.32904602832418967
 ------------ Epoch 4/10 Batch 4200/11616 Training Results ------------ 
Total Loss: 2.4856211546719784
Span Start Loss: 1.115548275882999
Span End Loss: 1.0412290773877786
Type Loss: 0.32884375861402426
 ------------ Epoch 4/10 Batch 4250/11616 Training Results ------------ 
Total Loss: 2.484697843667339
Span Start Loss: 1.1159448805731886
Span End Loss: 1.0401538829330135
Type Loss: 0.32859903780918787
 ------------ Epoch 4/10 Batch 4300/11616 Training Results ------------ 
Total Loss: 2.4873438525858313
Span Start Loss: 1.1173147056234438
Span End Loss: 1.041602070672914
Type Loss: 0.328427033762036
 ------------ Epoch 4/10 Batch 4350/11616 Training Results ------------ 
Total Loss: 2.491303067731446
Span Start Loss: 1.1195345180986942
Span End Loss: 1.0428505102810504
Type Loss: 0.32891799659029336
 ------------ Epoch 4/10 Batch 4400/11616 Training Results ------------ 
Total Loss: 2.4925313519821928
Span Start Loss: 1.1205910565263846
Span End Loss: 1.0428011710620062
Type Loss: 0.3291390822089108
 ------------ Epoch 4/10 Batch 4450/11616 Training Results ------------ 
Total Loss: 2.494224236369803
Span Start Loss: 1.1220427550425691
Span End Loss: 1.043398281506608
Type Loss: 0.3287831580486107
 ------------ Epoch 4/10 Batch 4500/11616 Training Results ------------ 
Total Loss: 2.495554412321912
Span Start Loss: 1.1214675951931212
Span End Loss: 1.0441162286814716
Type Loss: 0.3299705466137578
 ------------ Epoch 4/10 Batch 4550/11616 Training Results ------------ 
Total Loss: 2.496409143245482
Span Start Loss: 1.1222635107905
Span End Loss: 1.0443859968620997
Type Loss: 0.32975959378059255
 ------------ Epoch 4/10 Batch 4600/11616 Training Results ------------ 
Total Loss: 2.498083081059968
Span Start Loss: 1.1234973585573227
Span End Loss: 1.0451163627434037
Type Loss: 0.32946931766771265
 ------------ Epoch 4/10 Batch 4650/11616 Training Results ------------ 
Total Loss: 2.497527392509003
Span Start Loss: 1.1232721828485048
Span End Loss: 1.0450567951958667
Type Loss: 0.32919837254209705
 ------------ Epoch 4/10 Batch 4700/11616 Training Results ------------ 
Total Loss: 2.4974174143746497
Span Start Loss: 1.123381611460701
Span End Loss: 1.0450313134967013
Type Loss: 0.3290044475852096
 ------------ Epoch 4/10 Batch 4750/11616 Training Results ------------ 
Total Loss: 2.4945569225949678
Span Start Loss: 1.1214132100218221
Span End Loss: 1.0440205307759736
Type Loss: 0.32912314004941207
 ------------ Epoch 4/10 Batch 4800/11616 Training Results ------------ 
Total Loss: 2.494316251463412
Span Start Loss: 1.1212236697366462
Span End Loss: 1.0438657298513379
Type Loss: 0.3292268101061927
 ------------ Epoch 4/10 Batch 4850/11616 Training Results ------------ 
Total Loss: 2.4946317549794914
Span Start Loss: 1.1210792201265847
Span End Loss: 1.0441558838720175
Type Loss: 0.3293966091495276
 ------------ Epoch 4/10 Batch 4900/11616 Training Results ------------ 
Total Loss: 2.496074498866742
Span Start Loss: 1.1222844269324321
Span End Loss: 1.044702944737308
Type Loss: 0.32908708498281025
 ------------ Epoch 4/10 Batch 4950/11616 Training Results ------------ 
Total Loss: 2.501021785623949
Span Start Loss: 1.1246932349271244
Span End Loss: 1.0471927050448426
Type Loss: 0.32913580342995546
 ------------ Epoch 4/10 Batch 5000/11616 Training Results ------------ 
Total Loss: 2.5026745804063975
Span Start Loss: 1.125864294859767
Span End Loss: 1.0481384498745203
Type Loss: 0.32867179341893643
 ------------ Epoch 4/10 Batch 5050/11616 Training Results ------------ 
Total Loss: 2.503420539963806
Span Start Loss: 1.126551257988604
Span End Loss: 1.0480955795884723
Type Loss: 0.32877365970062
 ------------ Epoch 4/10 Batch 5100/11616 Training Results ------------ 
Total Loss: 2.503579998991507
Span Start Loss: 1.126301951598303
Span End Loss: 1.0482811661166889
Type Loss: 0.328996838552366
 ------------ Epoch 4/10 Batch 5150/11616 Training Results ------------ 
Total Loss: 2.5030859871215903
Span Start Loss: 1.1266372803721614
Span End Loss: 1.0476783807315293
Type Loss: 0.3287702829882647
 ------------ Epoch 4/10 Batch 5200/11616 Training Results ------------ 
Total Loss: 2.502985537621694
Span Start Loss: 1.1262536585990053
Span End Loss: 1.0482162990550008
Type Loss: 0.32851553719634047
 ------------ Epoch 4/10 Batch 5250/11616 Training Results ------------ 
Total Loss: 2.5036223579836743
Span Start Loss: 1.1268607349083537
Span End Loss: 1.048525702517657
Type Loss: 0.32823587811064153
 ------------ Epoch 4/10 Batch 5300/11616 Training Results ------------ 
Total Loss: 2.504247275441182
Span Start Loss: 1.1267567489703871
Span End Loss: 1.0495561810048684
Type Loss: 0.32793430293084314
 ------------ Epoch 4/10 Batch 5350/11616 Training Results ------------ 
Total Loss: 2.5062480629430475
Span Start Loss: 1.1278173437491756
Span End Loss: 1.0506600197717966
Type Loss: 0.32777065644127745
 ------------ Epoch 4/10 Batch 5400/11616 Training Results ------------ 
Total Loss: 2.504599301279005
Span Start Loss: 1.126875865371139
Span End Loss: 1.050166264925566
Type Loss: 0.32755712818492344
 ------------ Epoch 4/10 Batch 5450/11616 Training Results ------------ 
Total Loss: 2.5060234791060925
Span Start Loss: 1.1274156730874962
Span End Loss: 1.0509997993475253
Type Loss: 0.32760796361560124
 ------------ Epoch 4/10 Batch 5500/11616 Training Results ------------ 
Total Loss: 2.510566434802657
Span Start Loss: 1.1292102655456824
Span End Loss: 1.0536473571536216
Type Loss: 0.3277087691270492
 ------------ Epoch 4/10 Batch 5550/11616 Training Results ------------ 
Total Loss: 2.50986800244464
Span Start Loss: 1.129212191063273
Span End Loss: 1.0532814264445154
Type Loss: 0.3273743418589033
 ------------ Epoch 4/10 Batch 5600/11616 Training Results ------------ 
Total Loss: 2.511830974759401
Span Start Loss: 1.1299634936518435
Span End Loss: 1.0543250473761665
Type Loss: 0.32754239046497136
 ------------ Epoch 4/10 Batch 5650/11616 Training Results ------------ 
Total Loss: 2.510910554299444
Span Start Loss: 1.1293909355035399
Span End Loss: 1.0543297232212745
Type Loss: 0.32718985226054764
 ------------ Epoch 4/10 Batch 5700/11616 Training Results ------------ 
Total Loss: 2.513056921818408
Span Start Loss: 1.1305076963271488
Span End Loss: 1.0554177400979556
Type Loss: 0.32713144227551916
 ------------ Epoch 4/10 Batch 5750/11616 Training Results ------------ 
Total Loss: 2.5151687837845604
Span Start Loss: 1.1314907977801303
Span End Loss: 1.0563058761189814
Type Loss: 0.3273720665734423
 ------------ Epoch 4/10 Batch 5800/11616 Training Results ------------ 
Total Loss: 2.516173164935076
Span Start Loss: 1.132087889525654
Span End Loss: 1.0567404964424927
Type Loss: 0.32734473568242817
 ------------ Epoch 4/10 Batch 5850/11616 Training Results ------------ 
Total Loss: 2.5171673320966144
Span Start Loss: 1.132520781782193
Span End Loss: 1.0568838402743523
Type Loss: 0.32776266675108134
 ------------ Epoch 4/10 Batch 5900/11616 Training Results ------------ 
Total Loss: 2.516504409227331
Span Start Loss: 1.1329327312970565
Span End Loss: 1.0561176799742853
Type Loss: 0.3274539546941612
 ------------ Epoch 4/10 Batch 5950/11616 Training Results ------------ 
Total Loss: 2.5178884341811933
Span Start Loss: 1.133757510105101
Span End Loss: 1.0564345375755253
Type Loss: 0.32769634339567116
 ------------ Epoch 4/10 Batch 6000/11616 Training Results ------------ 
Total Loss: 2.517531302071487
Span Start Loss: 1.1335126553447916
Span End Loss: 1.0562358152366553
Type Loss: 0.3277827884190095
 --------------- Epoch 4/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 56.4, 'f1': 67.1, 'turns': 1425}), ('literature', {'em': 53.4, 'f1': 63.8, 'turns': 1630}), ('mid-high_school', {'em': 54.5, 'f1': 65.5, 'turns': 1653}), ('news', {'em': 58.2, 'f1': 69.0, 'turns': 1649}), ('wikipedia', {'em': 60.9, 'f1': 72.6, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 56.7, 'f1': 67.6, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 56.7, 'f1': 67.6, 'turns': 7983})])
 ------------ Epoch 4/10 Batch 6050/11616 Training Results ------------ 
Total Loss: 2.5199167067200436
Span Start Loss: 1.1350157760092912
Span End Loss: 1.0570277371088213
Type Loss: 0.32787315059807304
 ------------ Epoch 4/10 Batch 6100/11616 Training Results ------------ 
Total Loss: 2.521772888898239
Span Start Loss: 1.1354614344431606
Span End Loss: 1.0583400780521333
Type Loss: 0.3279713335671447
 ------------ Epoch 4/10 Batch 6150/11616 Training Results ------------ 
Total Loss: 2.5250692874284053
Span Start Loss: 1.1369639283900217
Span End Loss: 1.059940718492054
Type Loss: 0.32816459741488824
 ------------ Epoch 4/10 Batch 6200/11616 Training Results ------------ 
Total Loss: 2.52542852443012
Span Start Loss: 1.1370128894570253
Span End Loss: 1.0605285013199695
Type Loss: 0.32788709051621656
 ------------ Epoch 4/10 Batch 6250/11616 Training Results ------------ 
Total Loss: 2.5269622631281616
Span Start Loss: 1.1382429093119502
Span End Loss: 1.0610143451115488
Type Loss: 0.3277049656222761
 ------------ Epoch 4/10 Batch 6300/11616 Training Results ------------ 
Total Loss: 2.526376511472913
Span Start Loss: 1.1377432020147524
Span End Loss: 1.0611680873705163
Type Loss: 0.3274651789305998
 ------------ Epoch 4/10 Batch 6350/11616 Training Results ------------ 
Total Loss: 2.5275183232603813
Span Start Loss: 1.1376752253049822
Span End Loss: 1.0616036351080838
Type Loss: 0.3282394197769463
 ------------ Epoch 4/10 Batch 6400/11616 Training Results ------------ 
Total Loss: 2.5292270115198336
Span Start Loss: 1.1389593621433596
Span End Loss: 1.0615809678993537
Type Loss: 0.3286866384766472
 ------------ Epoch 4/10 Batch 6450/11616 Training Results ------------ 
Total Loss: 2.531125902456715
Span Start Loss: 1.1398461955089787
Span End Loss: 1.0624336931326015
Type Loss: 0.3288459707286873
 ------------ Epoch 4/10 Batch 6500/11616 Training Results ------------ 
Total Loss: 2.5320600508755216
Span Start Loss: 1.1403499677098141
Span End Loss: 1.062953097337427
Type Loss: 0.3287569427614888
 ------------ Epoch 4/10 Batch 6550/11616 Training Results ------------ 
Total Loss: 2.5311475200080915
Span Start Loss: 1.1399521059534363
Span End Loss: 1.0623476197254909
Type Loss: 0.32884775133291616
 ------------ Epoch 4/10 Batch 6600/11616 Training Results ------------ 
Total Loss: 2.530284498544466
Span Start Loss: 1.1398117752359107
Span End Loss: 1.0616085324602935
Type Loss: 0.3288641477812251
 ------------ Epoch 4/10 Batch 6650/11616 Training Results ------------ 
Total Loss: 2.5297275704800857
Span Start Loss: 1.1395951818957701
Span End Loss: 1.0613376202941276
Type Loss: 0.3287947252416577
 ------------ Epoch 4/10 Batch 6700/11616 Training Results ------------ 
Total Loss: 2.5308788196614653
Span Start Loss: 1.1400901800647045
Span End Loss: 1.0617084467686269
Type Loss: 0.32908014996018986
 ------------ Epoch 4/10 Batch 6750/11616 Training Results ------------ 
Total Loss: 2.533313952058002
Span Start Loss: 1.1411472046383553
Span End Loss: 1.0628570137517872
Type Loss: 0.3293096906295805
 ------------ Epoch 4/10 Batch 6800/11616 Training Results ------------ 
Total Loss: 2.5342896250816174
Span Start Loss: 1.1415821329687776
Span End Loss: 1.0634639573335538
Type Loss: 0.32924349157509447
 ------------ Epoch 4/10 Batch 6850/11616 Training Results ------------ 
Total Loss: 2.534120219121993
Span Start Loss: 1.141704575990184
Span End Loss: 1.063475580906944
Type Loss: 0.32894001907009607
 ------------ Epoch 4/10 Batch 6900/11616 Training Results ------------ 
Total Loss: 2.5334566955468145
Span Start Loss: 1.141293487875736
Span End Loss: 1.0634103454517174
Type Loss: 0.3287528187915197
 ------------ Epoch 4/10 Batch 6950/11616 Training Results ------------ 
Total Loss: 2.5333405036961647
Span Start Loss: 1.1415954821731011
Span End Loss: 1.0633369598240197
Type Loss: 0.328408018095191
 ------------ Epoch 4/10 Batch 7000/11616 Training Results ------------ 
Total Loss: 2.5334565070830286
Span Start Loss: 1.141950656327552
Span End Loss: 1.0633244131673127
Type Loss: 0.3281813938902425
 ------------ Epoch 4/10 Batch 7050/11616 Training Results ------------ 
Total Loss: 2.534184252225021
Span Start Loss: 1.1423503642880641
Span End Loss: 1.0636043324546083
Type Loss: 0.32822951173238085
 ------------ Epoch 4/10 Batch 7100/11616 Training Results ------------ 
Total Loss: 2.5349779583435033
Span Start Loss: 1.142558779167405
Span End Loss: 1.0639568186165687
Type Loss: 0.32846231678579474
 ------------ Epoch 4/10 Batch 7150/11616 Training Results ------------ 
Total Loss: 2.5348880172948114
Span Start Loss: 1.1424579334183484
Span End Loss: 1.063882612081935
Type Loss: 0.3285474281101585
 ------------ Epoch 4/10 Batch 7200/11616 Training Results ------------ 
Total Loss: 2.534918251294746
Span Start Loss: 1.1424412348770743
Span End Loss: 1.0640291639502781
Type Loss: 0.32844780870609813
 ------------ Epoch 4/10 Batch 7250/11616 Training Results ------------ 
Total Loss: 2.533691394033617
Span Start Loss: 1.1419086063943547
Span End Loss: 1.0635225723162294
Type Loss: 0.3282601714190738
 ------------ Epoch 4/10 Batch 7300/11616 Training Results ------------ 
Total Loss: 2.5337405481898827
Span Start Loss: 1.1419516148252336
Span End Loss: 1.0635307771602824
Type Loss: 0.3282581122574231
 ------------ Epoch 4/10 Batch 7350/11616 Training Results ------------ 
Total Loss: 2.5354043996683795
Span Start Loss: 1.1431240744440228
Span End Loss: 1.06409421549185
Type Loss: 0.3281860655553791
 ------------ Epoch 4/10 Batch 7400/11616 Training Results ------------ 
Total Loss: 2.5358115329759547
Span Start Loss: 1.1431655338232884
Span End Loss: 1.0639242394774446
Type Loss: 0.3287217155046056
 ------------ Epoch 4/10 Batch 7450/11616 Training Results ------------ 
Total Loss: 2.534950549733019
Span Start Loss: 1.1429911818887863
Span End Loss: 1.0634941439063057
Type Loss: 0.3284651799832514
 ------------ Epoch 4/10 Batch 7500/11616 Training Results ------------ 
Total Loss: 2.5358129656836392
Span Start Loss: 1.143381929600984
Span End Loss: 1.064020536077271
Type Loss: 0.3284104559727013
 ------------ Epoch 4/10 Batch 7550/11616 Training Results ------------ 
Total Loss: 2.537770083963575
Span Start Loss: 1.1443009780265934
Span End Loss: 1.0650262721566273
Type Loss: 0.32844278964701273
 ------------ Epoch 4/10 Batch 7600/11616 Training Results ------------ 
Total Loss: 2.5378361767574558
Span Start Loss: 1.1442508839646746
Span End Loss: 1.065229486646866
Type Loss: 0.32835576198627486
 ------------ Epoch 4/10 Batch 7650/11616 Training Results ------------ 
Total Loss: 2.537833007222979
Span Start Loss: 1.1442635852300556
Span End Loss: 1.0653466614781348
Type Loss: 0.3282227163876291
 ------------ Epoch 4/10 Batch 7700/11616 Training Results ------------ 
Total Loss: 2.537684334357354
Span Start Loss: 1.1438225440862406
Span End Loss: 1.0655565720131355
Type Loss: 0.32830517401193066
 ------------ Epoch 4/10 Batch 7750/11616 Training Results ------------ 
Total Loss: 2.538244688854583
Span Start Loss: 1.144326577696829
Span End Loss: 1.0658642469598882
Type Loss: 0.3280538200036653
 ------------ Epoch 4/10 Batch 7800/11616 Training Results ------------ 
Total Loss: 2.538751303836799
Span Start Loss: 1.144734846470782
Span End Loss: 1.066053282362767
Type Loss: 0.3279631308492464
 ------------ Epoch 4/10 Batch 7850/11616 Training Results ------------ 
Total Loss: 2.5396960930293724
Span Start Loss: 1.1449573994771975
Span End Loss: 1.0667238849251988
Type Loss: 0.32801476452001344
 ------------ Epoch 4/10 Batch 7900/11616 Training Results ------------ 
Total Loss: 2.5402825817677983
Span Start Loss: 1.1451925837491415
Span End Loss: 1.0671034258229164
Type Loss: 0.32798652826867336
 ------------ Epoch 4/10 Batch 7950/11616 Training Results ------------ 
Total Loss: 2.5393539773007423
Span Start Loss: 1.1443888442384653
Span End Loss: 1.067317107902055
Type Loss: 0.32764798120437366
 ------------ Epoch 4/10 Batch 8000/11616 Training Results ------------ 
Total Loss: 2.5411417305073702
Span Start Loss: 1.1454265376876573
Span End Loss: 1.0680961353380698
Type Loss: 0.32761901354626755
 --------------- Epoch 4/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 58.8, 'f1': 69.6, 'turns': 1425}), ('literature', {'em': 53.8, 'f1': 63.8, 'turns': 1630}), ('mid-high_school', {'em': 55.1, 'f1': 66.0, 'turns': 1653}), ('news', {'em': 59.0, 'f1': 69.4, 'turns': 1649}), ('wikipedia', {'em': 61.2, 'f1': 72.5, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 57.5, 'f1': 68.2, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 57.5, 'f1': 68.2, 'turns': 7983})])
 ------------ Epoch 4/10 Batch 8050/11616 Training Results ------------ 
Total Loss: 2.5423147313134824
Span Start Loss: 1.1464171119476234
Span End Loss: 1.0684305528502749
Type Loss: 0.3274670222790345
 ------------ Epoch 4/10 Batch 8100/11616 Training Results ------------ 
Total Loss: 2.541246993975415
Span Start Loss: 1.1461655307522067
Span End Loss: 1.0673602954374144
Type Loss: 0.32772112356107913
 ------------ Epoch 4/10 Batch 8150/11616 Training Results ------------ 
Total Loss: 2.5412649454126144
Span Start Loss: 1.1463721451919777
Span End Loss: 1.0669485907606278
Type Loss: 0.32794416523075726
 ------------ Epoch 4/10 Batch 8200/11616 Training Results ------------ 
Total Loss: 2.541575922523966
Span Start Loss: 1.1469682930925542
Span End Loss: 1.0668825965604132
Type Loss: 0.327724988571618
 ------------ Epoch 4/10 Batch 8250/11616 Training Results ------------ 
Total Loss: 2.542369768192822
Span Start Loss: 1.147610854832286
Span End Loss: 1.0671817656242937
Type Loss: 0.32757710346105423
 ------------ Epoch 4/10 Batch 8300/11616 Training Results ------------ 
Total Loss: 2.543642490031759
Span Start Loss: 1.1483699275577912
Span End Loss: 1.067850043790931
Type Loss: 0.3274224742903677
 ------------ Epoch 4/10 Batch 8350/11616 Training Results ------------ 
Total Loss: 2.5437567947042026
Span Start Loss: 1.1483056797388338
Span End Loss: 1.0679661550504451
Type Loss: 0.3274849155159321
 ------------ Epoch 4/10 Batch 8400/11616 Training Results ------------ 
Total Loss: 2.5451569658460182
Span Start Loss: 1.148851758769181
Span End Loss: 1.0688285021406287
Type Loss: 0.32747666056600533
 ------------ Epoch 4/10 Batch 8450/11616 Training Results ------------ 
Total Loss: 2.545414881785152
Span Start Loss: 1.1489168857388508
Span End Loss: 1.0689223849590448
Type Loss: 0.3275755667186083
 ------------ Epoch 4/10 Batch 8500/11616 Training Results ------------ 
Total Loss: 2.544218245799489
Span Start Loss: 1.1484087289806235
Span End Loss: 1.0683220601092804
Type Loss: 0.32748741243757745
 ------------ Epoch 4/10 Batch 8550/11616 Training Results ------------ 
Total Loss: 2.5444572947287596
Span Start Loss: 1.1484960547758387
Span End Loss: 1.0684729940790618
Type Loss: 0.32748820175226145
 ------------ Epoch 4/10 Batch 8600/11616 Training Results ------------ 
Total Loss: 2.5452494465702666
Span Start Loss: 1.1488986552374567
Span End Loss: 1.0689561943408794
Type Loss: 0.32739455290203695
 ------------ Epoch 4/10 Batch 8650/11616 Training Results ------------ 
Total Loss: 2.54579204496836
Span Start Loss: 1.149096895189945
Span End Loss: 1.06931619797591
Type Loss: 0.327378907628099
 ------------ Epoch 4/10 Batch 8700/11616 Training Results ------------ 
Total Loss: 2.545992182771108
Span Start Loss: 1.1491964773668897
Span End Loss: 1.069283837230084
Type Loss: 0.32751182390676664
 ------------ Epoch 4/10 Batch 8750/11616 Training Results ------------ 
Total Loss: 2.54687138131048
Span Start Loss: 1.1500887511157565
Span End Loss: 1.069622386839773
Type Loss: 0.3271601990214416
 ------------ Epoch 4/10 Batch 8800/11616 Training Results ------------ 
Total Loss: 2.5483374333368953
Span Start Loss: 1.1508107790357265
Span End Loss: 1.0703564381271347
Type Loss: 0.32717017185683783
 ------------ Epoch 4/10 Batch 8850/11616 Training Results ------------ 
Total Loss: 2.5487999704198145
Span Start Loss: 1.1509719029765597
Span End Loss: 1.070636866960864
Type Loss: 0.32719115600221405
 ------------ Epoch 4/10 Batch 8900/11616 Training Results ------------ 
Total Loss: 2.548111380010927
Span Start Loss: 1.150619011223944
Span End Loss: 1.0700947632291093
Type Loss: 0.3273975610634668
 ------------ Epoch 4/10 Batch 8950/11616 Training Results ------------ 
Total Loss: 2.5485423506825495
Span Start Loss: 1.151041264784361
Span End Loss: 1.069988432555933
Type Loss: 0.3275126088987099
 ------------ Epoch 4/10 Batch 9000/11616 Training Results ------------ 
Total Loss: 2.548223062523951
Span Start Loss: 1.1517354743089527
Span End Loss: 1.0694231854240515
Type Loss: 0.32706435821433033
 ------------ Epoch 4/10 Batch 9050/11616 Training Results ------------ 
Total Loss: 2.547573803754225
Span Start Loss: 1.1513983168751414
Span End Loss: 1.0691410542104538
Type Loss: 0.3270343880334776
 ------------ Epoch 4/10 Batch 9100/11616 Training Results ------------ 
Total Loss: 2.5473582357925535
Span Start Loss: 1.1511083033760743
Span End Loss: 1.0692756666711807
Type Loss: 0.3269742210610555
 ------------ Epoch 4/10 Batch 9150/11616 Training Results ------------ 
Total Loss: 2.548065243362159
Span Start Loss: 1.1515493928227838
Span End Loss: 1.0694662957986603
Type Loss: 0.3270495099274065
 ------------ Epoch 4/10 Batch 9200/11616 Training Results ------------ 
Total Loss: 2.5485419571986827
Span Start Loss: 1.152207703886594
Span End Loss: 1.0693624697020276
Type Loss: 0.32697173874916347
 ------------ Epoch 4/10 Batch 9250/11616 Training Results ------------ 
Total Loss: 2.547786178772111
Span Start Loss: 1.1518424273916597
Span End Loss: 1.06935523869802
Type Loss: 0.32658846794723256
 ------------ Epoch 4/10 Batch 9300/11616 Training Results ------------ 
Total Loss: 2.548122036374945
Span Start Loss: 1.15213858133943
Span End Loss: 1.069454456227481
Type Loss: 0.32652895381764296
 ------------ Epoch 4/10 Batch 9350/11616 Training Results ------------ 
Total Loss: 2.5491568442553603
Span Start Loss: 1.152898181622081
Span End Loss: 1.0698533596355408
Type Loss: 0.3264052580843834
 ------------ Epoch 4/10 Batch 9400/11616 Training Results ------------ 
Total Loss: 2.550883166696084
Span Start Loss: 1.1541547263899143
Span End Loss: 1.070416895026896
Type Loss: 0.3263115004359923
 ------------ Epoch 4/10 Batch 9450/11616 Training Results ------------ 
Total Loss: 2.5530973410342264
Span Start Loss: 1.1549952771914778
Span End Loss: 1.0715763188222491
Type Loss: 0.32652570025285793
 ------------ Epoch 4/10 Batch 9500/11616 Training Results ------------ 
Total Loss: 2.552633648312798
Span Start Loss: 1.154923837999176
Span End Loss: 1.071301292770199
Type Loss: 0.3264084729369926
 ------------ Epoch 4/10 Batch 9550/11616 Training Results ------------ 
Total Loss: 2.5536821831914924
Span Start Loss: 1.1556430903952861
Span End Loss: 1.071819025931285
Type Loss: 0.3262200221990806
 ------------ Epoch 4/10 Batch 9600/11616 Training Results ------------ 
Total Loss: 2.5553816944147303
Span Start Loss: 1.1567555995551326
Span End Loss: 1.0723498788640913
Type Loss: 0.3262761712555463
 ------------ Epoch 4/10 Batch 9650/11616 Training Results ------------ 
Total Loss: 2.555326894460333
Span Start Loss: 1.156530481390269
Span End Loss: 1.072339932856203
Type Loss: 0.3264564354157973
 ------------ Epoch 4/10 Batch 9700/11616 Training Results ------------ 
Total Loss: 2.5557082281611168
Span Start Loss: 1.156863873252434
Span End Loss: 1.072277800248187
Type Loss: 0.32656650972158946
 ------------ Epoch 4/10 Batch 9750/11616 Training Results ------------ 
Total Loss: 2.5556397544669034
Span Start Loss: 1.1568302024406119
Span End Loss: 1.072391972989035
Type Loss: 0.3264175341759737
 ------------ Epoch 4/10 Batch 9800/11616 Training Results ------------ 
Total Loss: 2.556503086630255
Span Start Loss: 1.157623579765347
Span End Loss: 1.0727147572877227
Type Loss: 0.3261647046815452
 ------------ Epoch 4/10 Batch 9850/11616 Training Results ------------ 
Total Loss: 2.558642670604859
Span Start Loss: 1.1589289001782004
Span End Loss: 1.073247495536612
Type Loss: 0.326466229937895
 ------------ Epoch 4/10 Batch 9900/11616 Training Results ------------ 
Total Loss: 2.5593293159240575
Span Start Loss: 1.1595501819239797
Span End Loss: 1.073200351467375
Type Loss: 0.3265787376337355
 ------------ Epoch 4/10 Batch 9950/11616 Training Results ------------ 
Total Loss: 2.5590926156290363
Span Start Loss: 1.1595237863769559
Span End Loss: 1.072923836065035
Type Loss: 0.32664494822915624
 ------------ Epoch 4/10 Batch 10000/11616 Training Results ------------ 
Total Loss: 2.559205768758431
Span Start Loss: 1.1597522837178782
Span End Loss: 1.0728373722845688
Type Loss: 0.3266160676142201
 --------------- Epoch 4/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 58.8, 'f1': 69.0, 'turns': 1425}), ('literature', {'em': 56.8, 'f1': 65.5, 'turns': 1630}), ('mid-high_school', {'em': 56.8, 'f1': 67.0, 'turns': 1653}), ('news', {'em': 60.8, 'f1': 70.5, 'turns': 1649}), ('wikipedia', {'em': 63.5, 'f1': 73.8, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 59.4, 'f1': 69.1, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 59.4, 'f1': 69.1, 'turns': 7983})])
 ------------ Epoch 4/10 Batch 10050/11616 Training Results ------------ 
Total Loss: 2.5595746639930637
Span Start Loss: 1.1596765362758616
Span End Loss: 1.0732794787866338
Type Loss: 0.32661860387624053
 ------------ Epoch 4/10 Batch 10100/11616 Training Results ------------ 
Total Loss: 2.5596249256744095
Span Start Loss: 1.1594907794570053
Span End Loss: 1.0733040788823205
Type Loss: 0.3268300223706456
 ------------ Epoch 4/10 Batch 10150/11616 Training Results ------------ 
Total Loss: 2.5603606936067638
Span Start Loss: 1.1600326441501823
Span End Loss: 1.0735424357705883
Type Loss: 0.3267855685385944
 ------------ Epoch 4/10 Batch 10200/11616 Training Results ------------ 
Total Loss: 2.55984030269342
Span Start Loss: 1.1597956032862924
Span End Loss: 1.0733047241916625
Type Loss: 0.32673993001692
 ------------ Epoch 4/10 Batch 10250/11616 Training Results ------------ 
Total Loss: 2.5595637394215274
Span Start Loss: 1.1596432997353556
Span End Loss: 1.0734577062444353
Type Loss: 0.32646268807569656
 ------------ Epoch 4/10 Batch 10300/11616 Training Results ------------ 
Total Loss: 2.5609647836554252
Span Start Loss: 1.160627959320159
Span End Loss: 1.073873245510119
Type Loss: 0.3264635334339318
 ------------ Epoch 4/10 Batch 10350/11616 Training Results ------------ 
Total Loss: 2.561654840760689
Span Start Loss: 1.1608363794072887
Span End Loss: 1.0742720966287642
Type Loss: 0.32654631937993467
 ------------ Epoch 4/10 Batch 10400/11616 Training Results ------------ 
Total Loss: 2.5621019140870955
Span Start Loss: 1.1609218550761804
Span End Loss: 1.074783215484976
Type Loss: 0.3263967981506497
 ------------ Epoch 4/10 Batch 10450/11616 Training Results ------------ 
Total Loss: 2.5629172271215603
Span Start Loss: 1.1612072644480749
Span End Loss: 1.0751992832696395
Type Loss: 0.3265106339030788
 ------------ Epoch 4/10 Batch 10500/11616 Training Results ------------ 
Total Loss: 2.5636276580020785
Span Start Loss: 1.1616162423266188
Span End Loss: 1.075657191139779
Type Loss: 0.32635417908483316
 ------------ Epoch 4/10 Batch 10550/11616 Training Results ------------ 
Total Loss: 2.563729027554534
Span Start Loss: 1.1615037118175655
Span End Loss: 1.0759731595587179
Type Loss: 0.3262521104613469
 ------------ Epoch 4/10 Batch 10600/11616 Training Results ------------ 
Total Loss: 2.5649209898720793
Span Start Loss: 1.1621200159981073
Span End Loss: 1.0765763961434154
Type Loss: 0.326224531955525
 ------------ Epoch 4/10 Batch 10650/11616 Training Results ------------ 
Total Loss: 2.566697792553216
Span Start Loss: 1.1628390722825004
Span End Loss: 1.077945570427522
Type Loss: 0.32591310402806933
 ------------ Epoch 4/10 Batch 10700/11616 Training Results ------------ 
Total Loss: 2.5677775104173293
Span Start Loss: 1.1630547545420302
Span End Loss: 1.078566221115223
Type Loss: 0.3261564889682509
 ------------ Epoch 4/10 Batch 10750/11616 Training Results ------------ 
Total Loss: 2.567106951234299
Span Start Loss: 1.1627346938656167
Span End Loss: 1.0783345528877406
Type Loss: 0.3260376587080401
 ------------ Epoch 4/10 Batch 10800/11616 Training Results ------------ 
Total Loss: 2.5674653019152442
Span Start Loss: 1.1629752338996708
Span End Loss: 1.0784377008781734
Type Loss: 0.32605232120595046
 ------------ Epoch 4/10 Batch 10850/11616 Training Results ------------ 
Total Loss: 2.5673289511696122
Span Start Loss: 1.1627743662902357
Span End Loss: 1.07829023073569
Type Loss: 0.3262643082457067
 ------------ Epoch 4/10 Batch 10900/11616 Training Results ------------ 
Total Loss: 2.567341289930729
Span Start Loss: 1.1628460573153827
Span End Loss: 1.0781325100876664
Type Loss: 0.3263626766556857
 ------------ Epoch 4/10 Batch 10950/11616 Training Results ------------ 
Total Loss: 2.568417263223224
Span Start Loss: 1.1635018779713338
Span End Loss: 1.0785473078797192
Type Loss: 0.3263680314942854
 ------------ Epoch 4/10 Batch 11000/11616 Training Results ------------ 
Total Loss: 2.5693356089446353
Span Start Loss: 1.1640201949953017
Span End Loss: 1.079060698932714
Type Loss: 0.3262546692466871
 ------------ Epoch 4/10 Batch 11050/11616 Training Results ------------ 
Total Loss: 2.570630719221083
Span Start Loss: 1.1649176945717694
Span End Loss: 1.0796493728204348
Type Loss: 0.3260636061032164
 ------------ Epoch 4/10 Batch 11100/11616 Training Results ------------ 
Total Loss: 2.5704834209788623
Span Start Loss: 1.1648422090335002
Span End Loss: 1.0797067562452107
Type Loss: 0.3259344100772596
 ------------ Epoch 4/10 Batch 11150/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.32594326790656075
 ------------ Epoch 4/10 Batch 11200/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.3260715292266104
 ------------ Epoch 4/10 Batch 11250/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.32606379788865647
 ------------ Epoch 4/10 Batch 11300/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.3263583928843614
 ------------ Epoch 4/10 Batch 11350/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.3264191868015881
 ------------ Epoch 4/10 Batch 11400/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.3264180007028769
 ------------ Epoch 4/10 Batch 11450/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.3263289499251115
 ------------ Epoch 4/10 Batch 11500/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.32623436267649675
 ------------ Epoch 4/10 Batch 11550/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.32619305335186843
 ------------ Epoch 4/10 Batch 11600/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.32586161101798944
 --------------------- Epoch 4/10 Final Training Results ------------------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.32583344178309553
 --------------- Epoch 4/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 57.1, 'f1': 67.5, 'turns': 1425}), ('literature', {'em': 51.6, 'f1': 61.9, 'turns': 1630}), ('mid-high_school', {'em': 53.4, 'f1': 64.4, 'turns': 1653}), ('news', {'em': 56.3, 'f1': 67.4, 'turns': 1649}), ('wikipedia', {'em': 59.1, 'f1': 70.3, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 55.4, 'f1': 66.3, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 55.4, 'f1': 66.3, 'turns': 7983})])
 --------------- Epoch 5/10 Training Start --------------- 
 ------------ Epoch 5/10 Batch 50/11616 Training Results ------------ 
Total Loss: 1.9868811815977097
Span Start Loss: 0.8713433167338371
Span End Loss: 0.8539445784687996
Type Loss: 0.26159324750304225
 ------------ Epoch 5/10 Batch 100/11616 Training Results ------------ 
Total Loss: 1.9656235553324222
Span Start Loss: 0.8470723751187325
Span End Loss: 0.8313357803970576
Type Loss: 0.28721535954624416
 ------------ Epoch 5/10 Batch 150/11616 Training Results ------------ 
Total Loss: 1.9413132319847743
Span Start Loss: 0.8294534362355868
Span End Loss: 0.7999940386911233
Type Loss: 0.3118657229964932
 ------------ Epoch 5/10 Batch 200/11616 Training Results ------------ 
Total Loss: 1.8926992867141963
Span Start Loss: 0.8145418856292963
Span End Loss: 0.7827029361948371
Type Loss: 0.29545443275012073
 ------------ Epoch 5/10 Batch 250/11616 Training Results ------------ 
Total Loss: 1.9159166219830512
Span Start Loss: 0.8222378541976213
Span End Loss: 0.7939959205091
Type Loss: 0.29968281562626364
 ------------ Epoch 5/10 Batch 300/11616 Training Results ------------ 
Total Loss: 1.8904428221285343
Span Start Loss: 0.8057209807137649
Span End Loss: 0.7817010244478783
Type Loss: 0.30302078815797967
 ------------ Epoch 5/10 Batch 350/11616 Training Results ------------ 
Total Loss: 1.8874567019087927
Span Start Loss: 0.8120690308298384
Span End Loss: 0.7725528818156038
Type Loss: 0.30283476009964944
 ------------ Epoch 5/10 Batch 400/11616 Training Results ------------ 
Total Loss: 1.8929853269830346
Span Start Loss: 0.8027265286818147
Span End Loss: 0.7814564964734018
Type Loss: 0.3088022705353797
 ------------ Epoch 5/10 Batch 450/11616 Training Results ------------ 
Total Loss: 1.8767224638826316
Span Start Loss: 0.7945006497452657
Span End Loss: 0.7746506149901284
Type Loss: 0.3075711670476529
 ------------ Epoch 5/10 Batch 500/11616 Training Results ------------ 
Total Loss: 1.8784375251084566
Span Start Loss: 0.7992527353242039
Span End Loss: 0.7738277685940266
Type Loss: 0.30535698984190823
 ------------ Epoch 5/10 Batch 550/11616 Training Results ------------ 
Total Loss: 1.878623932193626
Span Start Loss: 0.8017229753190821
Span End Loss: 0.7704223370822993
Type Loss: 0.3064785899120298
 ------------ Epoch 5/10 Batch 600/11616 Training Results ------------ 
Total Loss: 1.867701768875122
Span Start Loss: 0.7969334346180161
Span End Loss: 0.7669031385580699
Type Loss: 0.30386516558937726
 ------------ Epoch 5/10 Batch 650/11616 Training Results ------------ 
Total Loss: 1.8714363395021512
Span Start Loss: 0.8033024514982333
Span End Loss: 0.7643620646114533
Type Loss: 0.30377179331790943
 ------------ Epoch 5/10 Batch 700/11616 Training Results ------------ 
Total Loss: 1.869802211251642
Span Start Loss: 0.8054350198805332
Span End Loss: 0.7643985312325614
Type Loss: 0.2999686282500625
 ------------ Epoch 5/10 Batch 750/11616 Training Results ------------ 
Total Loss: 1.8915828758726518
Span Start Loss: 0.8176115035911401
Span End Loss: 0.7725330874721209
Type Loss: 0.30143825288365284
 ------------ Epoch 5/10 Batch 800/11616 Training Results ------------ 
Total Loss: 1.892257210263051
Span Start Loss: 0.8181451522465796
Span End Loss: 0.7712520067021251
Type Loss: 0.3028600190533325
 ------------ Epoch 5/10 Batch 850/11616 Training Results ------------ 
Total Loss: 1.885102154270691
Span Start Loss: 0.8155274435965454
Span End Loss: 0.7679130194292348
Type Loss: 0.30166165818624635
 ------------ Epoch 5/10 Batch 900/11616 Training Results ------------ 
Total Loss: 1.882408782839775
Span Start Loss: 0.8105463203787804
Span End Loss: 0.7694544096953339
Type Loss: 0.3024080203038951
 ------------ Epoch 5/10 Batch 950/11616 Training Results ------------ 
Total Loss: 1.8951543806414854
Span Start Loss: 0.8185509782558993
Span End Loss: 0.7746469066801824
Type Loss: 0.3019564617523237
 ------------ Epoch 5/10 Batch 1000/11616 Training Results ------------ 
Total Loss: 1.8834309665858746
Span Start Loss: 0.8126440923064947
Span End Loss: 0.7711463722288608
Type Loss: 0.2996404674630612
 ------------ Epoch 5/10 Batch 1050/11616 Training Results ------------ 
Total Loss: 1.8964703848674185
Span Start Loss: 0.8195674321339244
Span End Loss: 0.7748788760247685
Type Loss: 0.30202404103641
 ------------ Epoch 5/10 Batch 1100/11616 Training Results ------------ 
Total Loss: 1.8932582441378722
Span Start Loss: 0.8180273069441318
Span End Loss: 0.7743677003410729
Type Loss: 0.300863201291385
 ------------ Epoch 5/10 Batch 1150/11616 Training Results ------------ 
Total Loss: 1.8995012534701305
Span Start Loss: 0.8205271428823471
Span End Loss: 0.7756520742177964
Type Loss: 0.3033220014737352
 ------------ Epoch 5/10 Batch 1200/11616 Training Results ------------ 
Total Loss: 1.9070238080496589
Span Start Loss: 0.8245739758263032
Span End Loss: 0.7783113239084681
Type Loss: 0.30413847368520996
 ------------ Epoch 5/10 Batch 1250/11616 Training Results ------------ 
Total Loss: 1.9039772251844407
Span Start Loss: 0.8239527132064104
Span End Loss: 0.7767379298210144
Type Loss: 0.30328654724955556
 ------------ Epoch 5/10 Batch 1300/11616 Training Results ------------ 
Total Loss: 1.9081902221418343
Span Start Loss: 0.8264629479354391
Span End Loss: 0.7775899145465631
Type Loss: 0.30413732444151087
 ------------ Epoch 5/10 Batch 1350/11616 Training Results ------------ 
Total Loss: 1.9117294872386588
Span Start Loss: 0.8287239105144033
Span End Loss: 0.7780129871600204
Type Loss: 0.3049925543947352
 ------------ Epoch 5/10 Batch 1400/11616 Training Results ------------ 
Total Loss: 1.9174174358403044
Span Start Loss: 0.8291244230126696
Span End Loss: 0.781054977471275
Type Loss: 0.3072380002508206
 ------------ Epoch 5/10 Batch 1450/11616 Training Results ------------ 
Total Loss: 1.9189155250145444
Span Start Loss: 0.8303331176114493
Span End Loss: 0.7816931393074579
Type Loss: 0.3068892331074538
 ------------ Epoch 5/10 Batch 1500/11616 Training Results ------------ 
Total Loss: 1.9127144132430354
Span Start Loss: 0.8288456178978085
Span End Loss: 0.7780724977577727
Type Loss: 0.3057962625212967
 ------------ Epoch 5/10 Batch 1550/11616 Training Results ------------ 
Total Loss: 1.911700253945685
Span Start Loss: 0.8284021660733607
Span End Loss: 0.777175937556932
Type Loss: 0.30612211501646425
 ------------ Epoch 5/10 Batch 1600/11616 Training Results ------------ 
Total Loss: 1.9123860194697044
Span Start Loss: 0.8290153303882107
Span End Loss: 0.7767880906327628
Type Loss: 0.3065825631376356
 ------------ Epoch 5/10 Batch 1650/11616 Training Results ------------ 
Total Loss: 1.9096739353894285
Span Start Loss: 0.8272370901613525
Span End Loss: 0.7750351988072648
Type Loss: 0.3074016111682762
 ------------ Epoch 5/10 Batch 1700/11616 Training Results ------------ 
Total Loss: 1.9108319664154858
Span Start Loss: 0.827452635515262
Span End Loss: 0.7766680697680396
Type Loss: 0.30671122642343535
 ------------ Epoch 5/10 Batch 1750/11616 Training Results ------------ 
Total Loss: 1.9115240793462311
Span Start Loss: 0.8272955715698855
Span End Loss: 0.7755577171700342
Type Loss: 0.3086707560271025
 ------------ Epoch 5/10 Batch 1800/11616 Training Results ------------ 
Total Loss: 1.9120243558349708
Span Start Loss: 0.827297291206196
Span End Loss: 0.7757418827298614
Type Loss: 0.3089851477380014
 ------------ Epoch 5/10 Batch 1850/11616 Training Results ------------ 
Total Loss: 1.9089759376906865
Span Start Loss: 0.8254357318469399
Span End Loss: 0.774645506699343
Type Loss: 0.3088946649281157
 ------------ Epoch 5/10 Batch 1900/11616 Training Results ------------ 
Total Loss: 1.9117430194526126
Span Start Loss: 0.8274805723689497
Span End Loss: 0.7756364399194717
Type Loss: 0.30862597303563044
 ------------ Epoch 5/10 Batch 1950/11616 Training Results ------------ 
Total Loss: 1.9123915386792176
Span Start Loss: 0.8266395871045116
Span End Loss: 0.7766525550492299
Type Loss: 0.309099361864038
 ------------ Epoch 5/10 Batch 2000/11616 Training Results ------------ 
Total Loss: 1.916531449193135
Span Start Loss: 0.8291416224362329
Span End Loss: 0.7781954783387482
Type Loss: 0.30919431417807935
 --------------- Epoch 5/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 55.5, 'f1': 68.2, 'turns': 1425}), ('literature', {'em': 51.6, 'f1': 62.1, 'turns': 1630}), ('mid-high_school', {'em': 53.3, 'f1': 65.5, 'turns': 1653}), ('news', {'em': 56.6, 'f1': 68.1, 'turns': 1649}), ('wikipedia', {'em': 60.7, 'f1': 72.5, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 55.6, 'f1': 67.2, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 55.6, 'f1': 67.2, 'turns': 7983})])
 ------------ Epoch 5/10 Batch 2050/11616 Training Results ------------ 
Total Loss: 1.922651517291258
Span Start Loss: 0.8325402607686999
Span End Loss: 0.7811445928673919
Type Loss: 0.30896662861381363
 ------------ Epoch 5/10 Batch 2100/11616 Training Results ------------ 
Total Loss: 1.92614226971886
Span Start Loss: 0.8334909372892053
Span End Loss: 0.7844058521091938
Type Loss: 0.30824544517855557
 ------------ Epoch 5/10 Batch 2150/11616 Training Results ------------ 
Total Loss: 1.9237812466832787
Span Start Loss: 0.8319307674649496
Span End Loss: 0.7836901032439498
Type Loss: 0.3081603406239734
 ------------ Epoch 5/10 Batch 2200/11616 Training Results ------------ 
Total Loss: 1.9219547094015235
Span Start Loss: 0.8296809616794979
Span End Loss: 0.7832636702500961
Type Loss: 0.3090100420732051
 ------------ Epoch 5/10 Batch 2250/11616 Training Results ------------ 
Total Loss: 1.9197100755323966
Span Start Loss: 0.8290176021034519
Span End Loss: 0.7822754502892494
Type Loss: 0.3084169878611962
 ------------ Epoch 5/10 Batch 2300/11616 Training Results ------------ 
Total Loss: 1.9200757643045938
Span Start Loss: 0.8299422191804194
Span End Loss: 0.7822674112281074
Type Loss: 0.30786609917312213
 ------------ Epoch 5/10 Batch 2350/11616 Training Results ------------ 
Total Loss: 1.9165915869731218
Span Start Loss: 0.8284153539267309
Span End Loss: 0.7810736949710135
Type Loss: 0.3071025034761492
 ------------ Epoch 5/10 Batch 2400/11616 Training Results ------------ 
Total Loss: 1.917811926927728
Span Start Loss: 0.829128753625943
Span End Loss: 0.7810015438341846
Type Loss: 0.3076815946819261
 ------------ Epoch 5/10 Batch 2450/11616 Training Results ------------ 
Total Loss: 1.9173584619727062
Span Start Loss: 0.8282942574509249
Span End Loss: 0.7823342124570389
Type Loss: 0.3067299577004599
 ------------ Epoch 5/10 Batch 2500/11616 Training Results ------------ 
Total Loss: 1.9194168057665228
Span Start Loss: 0.8289642252080143
Span End Loss: 0.7845918139904737
Type Loss: 0.30586073191054164
 ------------ Epoch 5/10 Batch 2550/11616 Training Results ------------ 
Total Loss: 1.919649564591693
Span Start Loss: 0.8297755846593018
Span End Loss: 0.7842273851790849
Type Loss: 0.30564655986742356
 ------------ Epoch 5/10 Batch 2600/11616 Training Results ------------ 
Total Loss: 1.9197456130471366
Span Start Loss: 0.8297663910751446
Span End Loss: 0.7841135561638154
Type Loss: 0.3058656311919913
 ------------ Epoch 5/10 Batch 2650/11616 Training Results ------------ 
Total Loss: 1.9212640432113746
Span Start Loss: 0.8289756660704624
Span End Loss: 0.7856247194659598
Type Loss: 0.306663623080009
 ------------ Epoch 5/10 Batch 2700/11616 Training Results ------------ 
Total Loss: 1.9228236108604404
Span Start Loss: 0.8311318672096564
Span End Loss: 0.7850087664372943
Type Loss: 0.3066829424113449
 ------------ Epoch 5/10 Batch 2750/11616 Training Results ------------ 
Total Loss: 1.9232486180825668
Span Start Loss: 0.8316881404092366
Span End Loss: 0.7844570408856327
Type Loss: 0.3071034021482549
 ------------ Epoch 5/10 Batch 2800/11616 Training Results ------------ 
Total Loss: 1.9193276009256286
Span Start Loss: 0.829605935833949
Span End Loss: 0.7823358988322849
Type Loss: 0.3073857318180879
 ------------ Epoch 5/10 Batch 2850/11616 Training Results ------------ 
Total Loss: 1.9163264379558855
Span Start Loss: 0.8277740308701208
Span End Loss: 0.7809504986573991
Type Loss: 0.30760187421941704
 ------------ Epoch 5/10 Batch 2900/11616 Training Results ------------ 
Total Loss: 1.9168535503719388
Span Start Loss: 0.8289653686620295
Span End Loss: 0.7806537923454467
Type Loss: 0.3072343551878143
 ------------ Epoch 5/10 Batch 2950/11616 Training Results ------------ 
Total Loss: 1.9172123554399458
Span Start Loss: 0.8297408491278352
Span End Loss: 0.7804218634994606
Type Loss: 0.30704960797266184
 ------------ Epoch 5/10 Batch 3000/11616 Training Results ------------ 
Total Loss: 1.9163956420868635
Span Start Loss: 0.8295067324942599
Span End Loss: 0.779493273569271
Type Loss: 0.30739560146598766
 ------------ Epoch 5/10 Batch 3050/11616 Training Results ------------ 
Total Loss: 1.9166159185107614
Span Start Loss: 0.8297185982476737
Span End Loss: 0.7793419395173427
Type Loss: 0.3075553462761225
 ------------ Epoch 5/10 Batch 3100/11616 Training Results ------------ 
Total Loss: 1.918513406344479
Span Start Loss: 0.8300479272374462
Span End Loss: 0.7813266561306532
Type Loss: 0.307138788701005
 ------------ Epoch 5/10 Batch 3150/11616 Training Results ------------ 
Total Loss: 1.9169981802503269
Span Start Loss: 0.8300631987936203
Span End Loss: 0.7798270016890906
Type Loss: 0.3071079453941257
 ------------ Epoch 5/10 Batch 3200/11616 Training Results ------------ 
Total Loss: 1.9197937688149977
Span Start Loss: 0.8311792853783118
Span End Loss: 0.7814978325442644
Type Loss: 0.30711661616602215
 ------------ Epoch 5/10 Batch 3250/11616 Training Results ------------ 
Total Loss: 1.917761692376091
Span Start Loss: 0.8308252997381183
Span End Loss: 0.7802157499131102
Type Loss: 0.30672060810860535
 ------------ Epoch 5/10 Batch 3300/11616 Training Results ------------ 
Total Loss: 1.9176123864102093
Span Start Loss: 0.8311632897349244
Span End Loss: 0.7795800810087133
Type Loss: 0.3068689810521336
 ------------ Epoch 5/10 Batch 3350/11616 Training Results ------------ 
Total Loss: 1.9191799903371887
Span Start Loss: 0.8321609349821263
Span End Loss: 0.7798867191335381
Type Loss: 0.3071323017384023
 ------------ Epoch 5/10 Batch 3400/11616 Training Results ------------ 
Total Loss: 1.9175754985772073
Span Start Loss: 0.8320395919357372
Span End Loss: 0.7788728532112916
Type Loss: 0.30666301921448286
 ------------ Epoch 5/10 Batch 3450/11616 Training Results ------------ 
Total Loss: 1.9234998223520277
Span Start Loss: 0.8343945080846332
Span End Loss: 0.7817202882582078
Type Loss: 0.30738499192224034
 ------------ Epoch 5/10 Batch 3500/11616 Training Results ------------ 
Total Loss: 1.9264102157684309
Span Start Loss: 0.8358439522444138
Span End Loss: 0.7830056139331844
Type Loss: 0.30756061567259685
 ------------ Epoch 5/10 Batch 3550/11616 Training Results ------------ 
Total Loss: 1.92776798474222
Span Start Loss: 0.8362922129418019
Span End Loss: 0.7833703467853262
Type Loss: 0.30810539136365267
 ------------ Epoch 5/10 Batch 3600/11616 Training Results ------------ 
Total Loss: 1.9291693387915276
Span Start Loss: 0.837936153153682
Span End Loss: 0.7833403353056767
Type Loss: 0.3078928164868719
 ------------ Epoch 5/10 Batch 3650/11616 Training Results ------------ 
Total Loss: 1.9292281951449097
Span Start Loss: 0.8382346830042462
Span End Loss: 0.7833504809546348
Type Loss: 0.3076429974747031
 ------------ Epoch 5/10 Batch 3700/11616 Training Results ------------ 
Total Loss: 1.9269976791850216
Span Start Loss: 0.8373031528762265
Span End Loss: 0.7821002709608827
Type Loss: 0.3075942217123166
 ------------ Epoch 5/10 Batch 3750/11616 Training Results ------------ 
Total Loss: 1.9283063520342112
Span Start Loss: 0.8377520142858227
Span End Loss: 0.7831234794780612
Type Loss: 0.30743082469825944
 ------------ Epoch 5/10 Batch 3800/11616 Training Results ------------ 
Total Loss: 1.927128917562138
Span Start Loss: 0.838264853694035
Span End Loss: 0.7825621533732077
Type Loss: 0.3063018769283142
 ------------ Epoch 5/10 Batch 3850/11616 Training Results ------------ 
Total Loss: 1.9289536994918213
Span Start Loss: 0.8386969239395353
Span End Loss: 0.784351082303404
Type Loss: 0.30590565964797295
 ------------ Epoch 5/10 Batch 3900/11616 Training Results ------------ 
Total Loss: 1.9297152382202256
Span Start Loss: 0.8388788691492608
Span End Loss: 0.7849248261926457
Type Loss: 0.30591150924300725
 ------------ Epoch 5/10 Batch 3950/11616 Training Results ------------ 
Total Loss: 1.930040274640616
Span Start Loss: 0.8386282372403937
Span End Loss: 0.7846902550176917
Type Loss: 0.30672174860711526
 ------------ Epoch 5/10 Batch 4000/11616 Training Results ------------ 
Total Loss: 1.9314083378063516
Span Start Loss: 0.8405205152188427
Span End Loss: 0.7845158929792233
Type Loss: 0.30637189608230253
 --------------- Epoch 5/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 56.5, 'f1': 68.0, 'turns': 1425}), ('literature', {'em': 51.8, 'f1': 61.7, 'turns': 1630}), ('mid-high_school', {'em': 55.0, 'f1': 66.6, 'turns': 1653}), ('news', {'em': 58.4, 'f1': 68.8, 'turns': 1649}), ('wikipedia', {'em': 60.5, 'f1': 72.3, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 56.4, 'f1': 67.5, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 56.4, 'f1': 67.5, 'turns': 7983})])
 ------------ Epoch 5/10 Batch 4050/11616 Training Results ------------ 
Total Loss: 1.932684654323959
Span Start Loss: 0.840952985710292
Span End Loss: 0.7850323516723734
Type Loss: 0.30669928339045543
 ------------ Epoch 5/10 Batch 4100/11616 Training Results ------------ 
Total Loss: 1.9324252015706607
Span Start Loss: 0.8408341372617316
Span End Loss: 0.7849125716494533
Type Loss: 0.30667845878045913
 ------------ Epoch 5/10 Batch 4150/11616 Training Results ------------ 
Total Loss: 1.9349999434393212
Span Start Loss: 0.8413932966023504
Span End Loss: 0.787026675907693
Type Loss: 0.30657993687316776
 ------------ Epoch 5/10 Batch 4200/11616 Training Results ------------ 
Total Loss: 1.9377996405098765
Span Start Loss: 0.8423365700249339
Span End Loss: 0.7889554491998362
Type Loss: 0.30650758735502937
 ------------ Epoch 5/10 Batch 4250/11616 Training Results ------------ 
Total Loss: 1.9383553132020375
Span Start Loss: 0.8425095173054759
Span End Loss: 0.7891910188412842
Type Loss: 0.30665474321189173
 ------------ Epoch 5/10 Batch 4300/11616 Training Results ------------ 
Total Loss: 1.9412913514127912
Span Start Loss: 0.8433888977640417
Span End Loss: 0.7918986927150468
Type Loss: 0.30600372708056034
 ------------ Epoch 5/10 Batch 4350/11616 Training Results ------------ 
Total Loss: 1.9439262146965184
Span Start Loss: 0.844507109836686
Span End Loss: 0.793263300477859
Type Loss: 0.3061557704966044
 ------------ Epoch 5/10 Batch 4400/11616 Training Results ------------ 
Total Loss: 1.943199032337659
Span Start Loss: 0.844123137368482
Span End Loss: 0.7929082317366688
Type Loss: 0.3061676292987117
 ------------ Epoch 5/10 Batch 4450/11616 Training Results ------------ 
Total Loss: 1.942228826985098
Span Start Loss: 0.8435930387813891
Span End Loss: 0.7927566350146793
Type Loss: 0.30587911906202186
 ------------ Epoch 5/10 Batch 4500/11616 Training Results ------------ 
Total Loss: 1.94199573814455
Span Start Loss: 0.8434922011763686
Span End Loss: 0.7922676323581901
Type Loss: 0.3062358705815342
 ------------ Epoch 5/10 Batch 4550/11616 Training Results ------------ 
Total Loss: 1.9440144558652088
Span Start Loss: 0.8445399209075085
Span End Loss: 0.7932173301848573
Type Loss: 0.30625717047315376
 ------------ Epoch 5/10 Batch 4600/11616 Training Results ------------ 
Total Loss: 1.9467546000550298
Span Start Loss: 0.8453473274025094
Span End Loss: 0.7947967670951038
Type Loss: 0.306610470905738
 ------------ Epoch 5/10 Batch 4650/11616 Training Results ------------ 
Total Loss: 1.9479434422307438
Span Start Loss: 0.8458849672576593
Span End Loss: 0.7952498620456104
Type Loss: 0.3068085782253934
 ------------ Epoch 5/10 Batch 4700/11616 Training Results ------------ 
Total Loss: 1.944749969176631
Span Start Loss: 0.8439966658903088
Span End Loss: 0.7938648102428526
Type Loss: 0.3068884582789813
 ------------ Epoch 5/10 Batch 4750/11616 Training Results ------------ 
Total Loss: 1.9473799783501187
Span Start Loss: 0.8446638066082408
Span End Loss: 0.7952853759319375
Type Loss: 0.3074307608216217
 ------------ Epoch 5/10 Batch 4800/11616 Training Results ------------ 
Total Loss: 1.9473354125589444
Span Start Loss: 0.8441469439805951
Span End Loss: 0.7955914560034095
Type Loss: 0.30759697764374627
 ------------ Epoch 5/10 Batch 4850/11616 Training Results ------------ 
Total Loss: 1.9456657236345958
Span Start Loss: 0.8430265884366386
Span End Loss: 0.7951440655705087
Type Loss: 0.307495034741449
 ------------ Epoch 5/10 Batch 4900/11616 Training Results ------------ 
Total Loss: 1.9454580427394534
Span Start Loss: 0.8431430070440532
Span End Loss: 0.7948392877480661
Type Loss: 0.3074757129927071
 ------------ Epoch 5/10 Batch 4950/11616 Training Results ------------ 
Total Loss: 1.9444072805834238
Span Start Loss: 0.8423361542937581
Span End Loss: 0.7943105010368456
Type Loss: 0.30776059046763965
 ------------ Epoch 5/10 Batch 5000/11616 Training Results ------------ 
Total Loss: 1.9496750483438372
Span Start Loss: 0.8458019513275474
Span End Loss: 0.7961454005535692
Type Loss: 0.3077276615336537
 ------------ Epoch 5/10 Batch 5050/11616 Training Results ------------ 
Total Loss: 1.9509396480894325
Span Start Loss: 0.8458638681482413
Span End Loss: 0.7973021625908147
Type Loss: 0.30777358247059405
 ------------ Epoch 5/10 Batch 5100/11616 Training Results ------------ 
Total Loss: 1.9517260143423782
Span Start Loss: 0.8462046542204916
Span End Loss: 0.7981178087971228
Type Loss: 0.30740351645458563
 ------------ Epoch 5/10 Batch 5150/11616 Training Results ------------ 
Total Loss: 1.9533009660113785
Span Start Loss: 0.8472316574149774
Span End Loss: 0.7985109469334333
Type Loss: 0.30755832680365414
 ------------ Epoch 5/10 Batch 5200/11616 Training Results ------------ 
Total Loss: 1.9535818114088705
Span Start Loss: 0.8470636203394343
Span End Loss: 0.799166750050723
Type Loss: 0.30735140626533675
 ------------ Epoch 5/10 Batch 5250/11616 Training Results ------------ 
Total Loss: 1.9544288671229566
Span Start Loss: 0.8476287036137212
Span End Loss: 0.7991790865861944
Type Loss: 0.30762104228458237
 ------------ Epoch 5/10 Batch 5300/11616 Training Results ------------ 
Total Loss: 1.9546174480105347
Span Start Loss: 0.8477922933249963
Span End Loss: 0.7992244012362131
Type Loss: 0.3076007185609273
 ------------ Epoch 5/10 Batch 5350/11616 Training Results ------------ 
Total Loss: 1.9538779879632955
Span Start Loss: 0.8473544497213492
Span End Loss: 0.7988545375411338
Type Loss: 0.30766896582791736
 ------------ Epoch 5/10 Batch 5400/11616 Training Results ------------ 
Total Loss: 1.9543270613860202
Span Start Loss: 0.8482762242112033
Span End Loss: 0.7986021167346863
Type Loss: 0.3074486854582749
 ------------ Epoch 5/10 Batch 5450/11616 Training Results ------------ 
Total Loss: 1.9538430981679793
Span Start Loss: 0.8486742457204045
Span End Loss: 0.7981720192884223
Type Loss: 0.3069967981798728
 ------------ Epoch 5/10 Batch 5500/11616 Training Results ------------ 
Total Loss: 1.9516421767561274
Span Start Loss: 0.8478206149451435
Span End Loss: 0.7970072195531631
Type Loss: 0.306814307404682
 ------------ Epoch 5/10 Batch 5550/11616 Training Results ------------ 
Total Loss: 1.9508650086585198
Span Start Loss: 0.8476162344838182
Span End Loss: 0.796821863472898
Type Loss: 0.3064268759273932
 ------------ Epoch 5/10 Batch 5600/11616 Training Results ------------ 
Total Loss: 1.9506045055276315
Span Start Loss: 0.8468484584178909
Span End Loss: 0.7967015580768099
Type Loss: 0.30705445437276335
 ------------ Epoch 5/10 Batch 5650/11616 Training Results ------------ 
Total Loss: 1.9501646279959024
Span Start Loss: 0.8468871516889307
Span End Loss: 0.7963289883180595
Type Loss: 0.30694845326623954
 ------------ Epoch 5/10 Batch 5700/11616 Training Results ------------ 
Total Loss: 1.9511938558924093
Span Start Loss: 0.8473481058580964
Span End Loss: 0.7967391223357501
Type Loss: 0.30710659285043285
 ------------ Epoch 5/10 Batch 5750/11616 Training Results ------------ 
Total Loss: 1.9521252591454465
Span Start Loss: 0.8480652462195443
Span End Loss: 0.7972999667475081
Type Loss: 0.30676001130935293
 ------------ Epoch 5/10 Batch 5800/11616 Training Results ------------ 
Total Loss: 1.9520794603202878
Span Start Loss: 0.8480412615389274
Span End Loss: 0.7973598857135672
Type Loss: 0.30667827844732154
 ------------ Epoch 5/10 Batch 5850/11616 Training Results ------------ 
Total Loss: 1.9508960658350052
Span Start Loss: 0.8474297924979757
Span End Loss: 0.796566420605836
Type Loss: 0.30689981831173996
 ------------ Epoch 5/10 Batch 5900/11616 Training Results ------------ 
Total Loss: 1.9520936087089575
Span Start Loss: 0.8479117918156592
Span End Loss: 0.7973462001027508
Type Loss: 0.30683558259585525
 ------------ Epoch 5/10 Batch 5950/11616 Training Results ------------ 
Total Loss: 1.9528970504696128
Span Start Loss: 0.847836500050954
Span End Loss: 0.798155990965007
Type Loss: 0.3069045250870663
 ------------ Epoch 5/10 Batch 6000/11616 Training Results ------------ 
Total Loss: 1.9549296547845005
Span Start Loss: 0.849112701222611
Span End Loss: 0.7989037435223969
Type Loss: 0.3069131754868043
 --------------- Epoch 5/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 55.8, 'f1': 67.9, 'turns': 1425}), ('literature', {'em': 52.8, 'f1': 62.9, 'turns': 1630}), ('mid-high_school', {'em': 53.8, 'f1': 65.9, 'turns': 1653}), ('news', {'em': 57.2, 'f1': 68.4, 'turns': 1649}), ('wikipedia', {'em': 61.1, 'f1': 73.0, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 56.1, 'f1': 67.6, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 56.1, 'f1': 67.6, 'turns': 7983})])
 ------------ Epoch 5/10 Batch 6050/11616 Training Results ------------ 
Total Loss: 1.9549396485121042
Span Start Loss: 0.8493361437810231
Span End Loss: 0.7990646105039526
Type Loss: 0.30653885975741774
 ------------ Epoch 5/10 Batch 6100/11616 Training Results ------------ 
Total Loss: 1.9538165516754398
Span Start Loss: 0.848911190421061
Span End Loss: 0.7985191755505187
Type Loss: 0.30638615106415673
 ------------ Epoch 5/10 Batch 6150/11616 Training Results ------------ 
Total Loss: 1.9542225658742152
Span Start Loss: 0.8492616061390779
Span End Loss: 0.7987180580860533
Type Loss: 0.30624286721831534
 ------------ Epoch 5/10 Batch 6200/11616 Training Results ------------ 
Total Loss: 1.9547615970124401
Span Start Loss: 0.8499628009031256
Span End Loss: 0.798833308661687
Type Loss: 0.3059654529841857
 ------------ Epoch 5/10 Batch 6250/11616 Training Results ------------ 
Total Loss: 1.9550279755860567
Span Start Loss: 0.8504019721707702
Span End Loss: 0.7989141291947662
Type Loss: 0.30571183987125755
 ------------ Epoch 5/10 Batch 6300/11616 Training Results ------------ 
Total Loss: 1.9549624052223942
Span Start Loss: 0.8503511223513337
Span End Loss: 0.7989135005509865
Type Loss: 0.30569774801635907
 ------------ Epoch 5/10 Batch 6350/11616 Training Results ------------ 
Total Loss: 1.95474296733737
Span Start Loss: 0.8501477040769899
Span End Loss: 0.7990156658664053
Type Loss: 0.30557956333690156
 ------------ Epoch 5/10 Batch 6400/11616 Training Results ------------ 
Total Loss: 1.9561646664282306
Span Start Loss: 0.8506986590832821
Span End Loss: 0.79967112139464
Type Loss: 0.305794851861865
 ------------ Epoch 5/10 Batch 6450/11616 Training Results ------------ 
Total Loss: 1.9550652487737725
Span Start Loss: 0.8500654274776809
Span End Loss: 0.7991790860990972
Type Loss: 0.30582070124530514
 ------------ Epoch 5/10 Batch 6500/11616 Training Results ------------ 
Total Loss: 1.9567759220531353
Span Start Loss: 0.8515803604077261
Span End Loss: 0.7993558966914335
Type Loss: 0.30583963082100335
 ------------ Epoch 5/10 Batch 6550/11616 Training Results ------------ 
Total Loss: 1.9573421366285277
Span Start Loss: 0.8518068357366528
Span End Loss: 0.7995162773526181
Type Loss: 0.30601898944702083
 ------------ Epoch 5/10 Batch 6600/11616 Training Results ------------ 
Total Loss: 1.9567763716208213
Span Start Loss: 0.851820628603243
Span End Loss: 0.7992947136335583
Type Loss: 0.3056609952452621
 ------------ Epoch 5/10 Batch 6650/11616 Training Results ------------ 
Total Loss: 1.9556124180219228
Span Start Loss: 0.8514367399065006
Span End Loss: 0.7988242049735124
Type Loss: 0.3053514390357567
 ------------ Epoch 5/10 Batch 6700/11616 Training Results ------------ 
Total Loss: 1.9568519506770283
Span Start Loss: 0.8519101617304581
Span End Loss: 0.7996776449887444
Type Loss: 0.30526410986905667
 ------------ Epoch 5/10 Batch 6750/11616 Training Results ------------ 
Total Loss: 1.9563515856254985
Span Start Loss: 0.8517823208335925
Span End Loss: 0.7993967012041421
Type Loss: 0.30517252943940737
 ------------ Epoch 5/10 Batch 6800/11616 Training Results ------------ 
Total Loss: 1.9550954262479483
Span Start Loss: 0.8509424825338647
Span End Loss: 0.7993660858959671
Type Loss: 0.3047868238248424
 ------------ Epoch 5/10 Batch 6850/11616 Training Results ------------ 
Total Loss: 1.9557932595334893
Span Start Loss: 0.8511640278056917
Span End Loss: 0.7998319659454164
Type Loss: 0.30479723183356605
 ------------ Epoch 5/10 Batch 6900/11616 Training Results ------------ 
Total Loss: 1.956857411953472
Span Start Loss: 0.8517327372845857
Span End Loss: 0.8006553038578157
Type Loss: 0.30446933680389454
 ------------ Epoch 5/10 Batch 6950/11616 Training Results ------------ 
Total Loss: 1.9590947927719504
Span Start Loss: 0.8528362324745428
Span End Loss: 0.8020082375376398
Type Loss: 0.3042502885890736
 ------------ Epoch 5/10 Batch 7000/11616 Training Results ------------ 
Total Loss: 1.958790754288701
Span Start Loss: 0.8526513329464942
Span End Loss: 0.8017595261134474
Type Loss: 0.3043798610812851
 ------------ Epoch 5/10 Batch 7050/11616 Training Results ------------ 
Total Loss: 1.9599286718848854
Span Start Loss: 0.853416046345646
Span End Loss: 0.80194249338728
Type Loss: 0.30457009799408574
 ------------ Epoch 5/10 Batch 7100/11616 Training Results ------------ 
Total Loss: 1.9601323841598777
Span Start Loss: 0.8533021899572456
Span End Loss: 0.8018752341503671
Type Loss: 0.304954925872333
 ------------ Epoch 5/10 Batch 7150/11616 Training Results ------------ 
Total Loss: 1.959388094750995
Span Start Loss: 0.8527453406673524
Span End Loss: 0.8016208467415007
Type Loss: 0.3050218731547882
 ------------ Epoch 5/10 Batch 7200/11616 Training Results ------------ 
Total Loss: 1.9602706377562653
Span Start Loss: 0.8534540159656252
Span End Loss: 0.8020026453383939
Type Loss: 0.30481394224256897
 ------------ Epoch 5/10 Batch 7250/11616 Training Results ------------ 
Total Loss: 1.9608713152503658
Span Start Loss: 0.8536982691254595
Span End Loss: 0.8021716034172167
Type Loss: 0.3050014085304634
 ------------ Epoch 5/10 Batch 7300/11616 Training Results ------------ 
Total Loss: 1.963099284685689
Span Start Loss: 0.8544864305830879
Span End Loss: 0.8034413382264288
Type Loss: 0.3051714817265508
 ------------ Epoch 5/10 Batch 7350/11616 Training Results ------------ 
Total Loss: 1.9647400294348192
Span Start Loss: 0.8551439534814483
Span End Loss: 0.8041742006467567
Type Loss: 0.30542184106137626
 ------------ Epoch 5/10 Batch 7400/11616 Training Results ------------ 
Total Loss: 1.9666763155255467
Span Start Loss: 0.8563780175801367
Span End Loss: 0.8046527618075394
Type Loss: 0.30564550193838425
 ------------ Epoch 5/10 Batch 7450/11616 Training Results ------------ 
Total Loss: 1.9685880771231832
Span Start Loss: 0.8572935396630392
Span End Loss: 0.8057256915581557
Type Loss: 0.3055688116810091
 ------------ Epoch 5/10 Batch 7500/11616 Training Results ------------ 
Total Loss: 1.9678698223692674
Span Start Loss: 0.8571936861661573
Span End Loss: 0.8053493210729211
Type Loss: 0.30532678084727377
 ------------ Epoch 5/10 Batch 7550/11616 Training Results ------------ 
Total Loss: 1.9684003926973963
Span Start Loss: 0.8577890183404088
Span End Loss: 0.805292476239608
Type Loss: 0.30531886392655405
 ------------ Epoch 5/10 Batch 7600/11616 Training Results ------------ 
Total Loss: 1.9691981667272844
Span Start Loss: 0.858116474916031
Span End Loss: 0.8057880990066272
Type Loss: 0.3052935587215899
 ------------ Epoch 5/10 Batch 7650/11616 Training Results ------------ 
Total Loss: 1.9692226684719227
Span Start Loss: 0.8582514875014525
Span End Loss: 0.805780686270202
Type Loss: 0.30519046036252645
 ------------ Epoch 5/10 Batch 7700/11616 Training Results ------------ 
Total Loss: 1.9703124309865783
Span Start Loss: 0.8589335270688034
Span End Loss: 0.8063170168961266
Type Loss: 0.3050618527128736
 ------------ Epoch 5/10 Batch 7750/11616 Training Results ------------ 
Total Loss: 1.9707280058786274
Span Start Loss: 0.8587223548151313
Span End Loss: 0.8067953006692471
Type Loss: 0.305210316082703
 ------------ Epoch 5/10 Batch 7800/11616 Training Results ------------ 
Total Loss: 1.9699963661352506
Span Start Loss: 0.8587980703811329
Span End Loss: 0.8066522808793264
Type Loss: 0.3045459805439131
 ------------ Epoch 5/10 Batch 7850/11616 Training Results ------------ 
Total Loss: 1.971358007682594
Span Start Loss: 0.8598060750024049
Span End Loss: 0.8070899235177192
Type Loss: 0.30446197480206516
 ------------ Epoch 5/10 Batch 7900/11616 Training Results ------------ 
Total Loss: 1.971330694412176
Span Start Loss: 0.8597959097118789
Span End Loss: 0.8073641461124526
Type Loss: 0.30417060430344384
 ------------ Epoch 5/10 Batch 7950/11616 Training Results ------------ 
Total Loss: 1.9714220330037129
Span Start Loss: 0.8601441288999511
Span End Loss: 0.8072430003223554
Type Loss: 0.30403486939773855
 ------------ Epoch 5/10 Batch 8000/11616 Training Results ------------ 
Total Loss: 1.9735647139616777
Span Start Loss: 0.8615788057392929
Span End Loss: 0.8077757436521351
Type Loss: 0.3042101301677176
 --------------- Epoch 5/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 57.3, 'f1': 68.2, 'turns': 1425}), ('literature', {'em': 54.6, 'f1': 64.3, 'turns': 1630}), ('mid-high_school', {'em': 56.4, 'f1': 67.4, 'turns': 1653}), ('news', {'em': 57.7, 'f1': 69.1, 'turns': 1649}), ('wikipedia', {'em': 60.9, 'f1': 71.8, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 57.4, 'f1': 68.2, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 57.4, 'f1': 68.2, 'turns': 7983})])
 ------------ Epoch 5/10 Batch 8050/11616 Training Results ------------ 
Total Loss: 1.9744780659779722
Span Start Loss: 0.86212496057361
Span End Loss: 0.8081076675653458
Type Loss: 0.30424540356594076
 ------------ Epoch 5/10 Batch 8100/11616 Training Results ------------ 
Total Loss: 1.9753513122425863
Span Start Loss: 0.8628277928218889
Span End Loss: 0.8083641392654842
Type Loss: 0.30415934601309996
 ------------ Epoch 5/10 Batch 8150/11616 Training Results ------------ 
Total Loss: 1.9768055206758937
Span Start Loss: 0.8635881996550169
Span End Loss: 0.8090409848020852
Type Loss: 0.3041763019197809
 ------------ Epoch 5/10 Batch 8200/11616 Training Results ------------ 
Total Loss: 1.9770854601647887
Span Start Loss: 0.8636301147531172
Span End Loss: 0.8090699767357693
Type Loss: 0.30438533430799797
 ------------ Epoch 5/10 Batch 8250/11616 Training Results ------------ 
Total Loss: 1.9774592519449916
Span Start Loss: 0.8638676343824375
Span End Loss: 0.8091292356035926
Type Loss: 0.3044623475435444
 ------------ Epoch 5/10 Batch 8300/11616 Training Results ------------ 
Total Loss: 1.9779074496095888
Span Start Loss: 0.8639500422032932
Span End Loss: 0.8091837740770306
Type Loss: 0.30477359889136874
 ------------ Epoch 5/10 Batch 8350/11616 Training Results ------------ 
Total Loss: 1.9785723047353283
Span Start Loss: 0.8640645025637068
Span End Loss: 0.8095126841435889
Type Loss: 0.3049950837820746
 ------------ Epoch 5/10 Batch 8400/11616 Training Results ------------ 
Total Loss: 1.9782509283395484
Span Start Loss: 0.8638722621172755
Span End Loss: 0.8092069691233337
Type Loss: 0.30517166278053387
 ------------ Epoch 5/10 Batch 8450/11616 Training Results ------------ 
Total Loss: 1.980857415560126
Span Start Loss: 0.8652123731050706
Span End Loss: 0.8105838303823443
Type Loss: 0.30506117785399084
 ------------ Epoch 5/10 Batch 8500/11616 Training Results ------------ 
Total Loss: 1.9812437736692237
Span Start Loss: 0.8651584292871549
Span End Loss: 0.8108801399495672
Type Loss: 0.3052051702507169
 ------------ Epoch 5/10 Batch 8550/11616 Training Results ------------ 
Total Loss: 1.982224562560475
Span Start Loss: 0.8654184083607898
Span End Loss: 0.8112275240984228
Type Loss: 0.3055785958623045
 ------------ Epoch 5/10 Batch 8600/11616 Training Results ------------ 
Total Loss: 1.9833074919370457
Span Start Loss: 0.8661020396851263
Span End Loss: 0.8116580540730163
Type Loss: 0.3055473638360042
 ------------ Epoch 5/10 Batch 8650/11616 Training Results ------------ 
Total Loss: 1.9829475688678078
Span Start Loss: 0.8660365680353231
Span End Loss: 0.811444371823286
Type Loss: 0.30546659474982474
 ------------ Epoch 5/10 Batch 8700/11616 Training Results ------------ 
Total Loss: 1.9835354678086594
Span Start Loss: 0.866231119383726
Span End Loss: 0.8119375954739664
Type Loss: 0.30536671871892124
 ------------ Epoch 5/10 Batch 8750/11616 Training Results ------------ 
Total Loss: 1.9832398656404444
Span Start Loss: 0.8657799129364746
Span End Loss: 0.8121786204389163
Type Loss: 0.30528129797189363
 ------------ Epoch 5/10 Batch 8800/11616 Training Results ------------ 
Total Loss: 1.9828437186364847
Span Start Loss: 0.8653425654224967
Span End Loss: 0.8121936939775266
Type Loss: 0.3053074249315647
 ------------ Epoch 5/10 Batch 8850/11616 Training Results ------------ 
Total Loss: 1.983873988878466
Span Start Loss: 0.8657364289098456
Span End Loss: 0.812584248636402
Type Loss: 0.3055532770990631
 ------------ Epoch 5/10 Batch 8900/11616 Training Results ------------ 
Total Loss: 1.9838544298057559
Span Start Loss: 0.8655750066935598
Span End Loss: 0.8126271434864971
Type Loss: 0.3056522452744938
 ------------ Epoch 5/10 Batch 8950/11616 Training Results ------------ 
Total Loss: 1.984517439435023
Span Start Loss: 0.8658967502363151
Span End Loss: 0.8131272173352415
Type Loss: 0.305493437531014
 ------------ Epoch 5/10 Batch 9000/11616 Training Results ------------ 
Total Loss: 1.9844131552769493
Span Start Loss: 0.8656290168627683
Span End Loss: 0.8130822134936849
Type Loss: 0.3057018907051016
 ------------ Epoch 5/10 Batch 9050/11616 Training Results ------------ 
Total Loss: 1.9840161443590212
Span Start Loss: 0.8656628154882986
Span End Loss: 0.8127786708487331
Type Loss: 0.3055746238208916
 ------------ Epoch 5/10 Batch 9100/11616 Training Results ------------ 
Total Loss: 1.9843256871256936
Span Start Loss: 0.8657547029294074
Span End Loss: 0.8128279867383478
Type Loss: 0.3057429631379341
 ------------ Epoch 5/10 Batch 9150/11616 Training Results ------------ 
Total Loss: 1.9836262225748493
Span Start Loss: 0.8658650726517612
Span End Loss: 0.8120150171041163
Type Loss: 0.30574609851306866
 ------------ Epoch 5/10 Batch 9200/11616 Training Results ------------ 
Total Loss: 1.9843690298548293
Span Start Loss: 0.8662452757210755
Span End Loss: 0.8124463915792496
Type Loss: 0.3056773280293134
 ------------ Epoch 5/10 Batch 9250/11616 Training Results ------------ 
Total Loss: 1.9842210764226478
Span Start Loss: 0.8662822547327425
Span End Loss: 0.8123788736961983
Type Loss: 0.30555991342276134
 ------------ Epoch 5/10 Batch 9300/11616 Training Results ------------ 
Total Loss: 1.9848235742509446
Span Start Loss: 0.8667512812647689
Span End Loss: 0.8124379304744384
Type Loss: 0.3056343279307788
 ------------ Epoch 5/10 Batch 9350/11616 Training Results ------------ 
Total Loss: 1.9849106666925835
Span Start Loss: 0.8664707503533539
Span End Loss: 0.8128558741497962
Type Loss: 0.3055840076532534
 ------------ Epoch 5/10 Batch 9400/11616 Training Results ------------ 
Total Loss: 1.9852281591092098
Span Start Loss: 0.8664892583576209
Span End Loss: 0.813115491000341
Type Loss: 0.3056233751467191
 ------------ Epoch 5/10 Batch 9450/11616 Training Results ------------ 
Total Loss: 1.9865146361853159
Span Start Loss: 0.8672525562588421
Span End Loss: 0.8136814218668869
Type Loss: 0.30558062354610277
 ------------ Epoch 5/10 Batch 9500/11616 Training Results ------------ 
Total Loss: 1.986728117235593
Span Start Loss: 0.8672639997568178
Span End Loss: 0.8138562982901931
Type Loss: 0.3056077847147949
 ------------ Epoch 5/10 Batch 9550/11616 Training Results ------------ 
Total Loss: 1.987249588620171
Span Start Loss: 0.8673952029027162
Span End Loss: 0.8141182887558069
Type Loss: 0.30573606248150204
 ------------ Epoch 5/10 Batch 9600/11616 Training Results ------------ 
Total Loss: 1.9878169672146517
Span Start Loss: 0.8679407681258938
Span End Loss: 0.8143230962819265
Type Loss: 0.30555306830021434
 ------------ Epoch 5/10 Batch 9650/11616 Training Results ------------ 
Total Loss: 1.988734715598891
Span Start Loss: 0.8681138535513702
Span End Loss: 0.8149640526506722
Type Loss: 0.30565677486422804
 ------------ Epoch 5/10 Batch 9700/11616 Training Results ------------ 
Total Loss: 1.9901920000772886
Span Start Loss: 0.868709831534088
Span End Loss: 0.8157102623170952
Type Loss: 0.3057718716360648
 ------------ Epoch 5/10 Batch 9750/11616 Training Results ------------ 
Total Loss: 1.990155535593438
Span Start Loss: 0.8685927317469165
Span End Loss: 0.8158108119838513
Type Loss: 0.30575195730788013
 ------------ Epoch 5/10 Batch 9800/11616 Training Results ------------ 
Total Loss: 1.9899514015241317
Span Start Loss: 0.8682629199810706
Span End Loss: 0.8158120343450229
Type Loss: 0.305876412587668
 ------------ Epoch 5/10 Batch 9850/11616 Training Results ------------ 
Total Loss: 1.9911667595820757
Span Start Loss: 0.8689065135083147
Span End Loss: 0.8162951327153117
Type Loss: 0.3059650787656777
 ------------ Epoch 5/10 Batch 9900/11616 Training Results ------------ 
Total Loss: 1.991805659265568
Span Start Loss: 0.869439540172579
Span End Loss: 0.8166316117282317
Type Loss: 0.3057344727258132
 ------------ Epoch 5/10 Batch 9950/11616 Training Results ------------ 
Total Loss: 1.9926057657451561
Span Start Loss: 0.8699034950614499
Span End Loss: 0.8170010579100356
Type Loss: 0.30570117809716146
 ------------ Epoch 5/10 Batch 10000/11616 Training Results ------------ 
Total Loss: 1.9935991662712769
Span Start Loss: 0.8703540420694277
Span End Loss: 0.8176931797597558
Type Loss: 0.3055519098350313
 --------------- Epoch 5/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 56.7, 'f1': 67.8, 'turns': 1425}), ('literature', {'em': 53.7, 'f1': 63.7, 'turns': 1630}), ('mid-high_school', {'em': 56.2, 'f1': 66.8, 'turns': 1653}), ('news', {'em': 56.2, 'f1': 67.5, 'turns': 1649}), ('wikipedia', {'em': 60.9, 'f1': 72.2, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 56.7, 'f1': 67.6, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 56.7, 'f1': 67.6, 'turns': 7983})])
 ------------ Epoch 5/10 Batch 10050/11616 Training Results ------------ 
Total Loss: 1.9947423736819654
Span Start Loss: 0.8711201409452516
Span End Loss: 0.8180627748181126
Type Loss: 0.305559423294669
 ------------ Epoch 5/10 Batch 10100/11616 Training Results ------------ 
Total Loss: 1.9957807313475646
Span Start Loss: 0.8721112510840417
Span End Loss: 0.8183264859909616
Type Loss: 0.3053429596582981
 ------------ Epoch 5/10 Batch 10150/11616 Training Results ------------ 
Total Loss: 1.9977913185632947
Span Start Loss: 0.8734306163577538
Span End Loss: 0.8189843396978278
Type Loss: 0.3053763278316376
 ------------ Epoch 5/10 Batch 10200/11616 Training Results ------------ 
Total Loss: 1.9975943854781708
Span Start Loss: 0.8734205216079878
Span End Loss: 0.818793142303167
Type Loss: 0.30538068702083776
 ------------ Epoch 5/10 Batch 10250/11616 Training Results ------------ 
Total Loss: 1.9988396674258317
Span Start Loss: 0.8738575927438896
Span End Loss: 0.8197692524132932
Type Loss: 0.30521278770626864
 ------------ Epoch 5/10 Batch 10300/11616 Training Results ------------ 
Total Loss: 1.9995088108037644
Span Start Loss: 0.8742628604012713
Span End Loss: 0.8203561852297303
Type Loss: 0.3048897305501227
 ------------ Epoch 5/10 Batch 10350/11616 Training Results ------------ 
Total Loss: 2.000380437836447
Span Start Loss: 0.8746902371127753
Span End Loss: 0.8205831879913663
Type Loss: 0.30510697807311327
 ------------ Epoch 5/10 Batch 10400/11616 Training Results ------------ 
Total Loss: 2.001338999307392
Span Start Loss: 0.8750314340924916
Span End Loss: 0.8213416539637658
Type Loss: 0.30496587662603886
 ------------ Epoch 5/10 Batch 10450/11616 Training Results ------------ 
Total Loss: 2.0018486600289007
Span Start Loss: 0.8753483865347704
Span End Loss: 0.8215867603014103
Type Loss: 0.3049134785422173
 ------------ Epoch 5/10 Batch 10500/11616 Training Results ------------ 
Total Loss: 2.0022133070239354
Span Start Loss: 0.8757244141376799
Span End Loss: 0.8216836122689503
Type Loss: 0.3048052460354797
 ------------ Epoch 5/10 Batch 10550/11616 Training Results ------------ 
Total Loss: 2.003592868485616
Span Start Loss: 0.8761024092814016
Span End Loss: 0.8224809698530989
Type Loss: 0.305009454799357
 ------------ Epoch 5/10 Batch 10600/11616 Training Results ------------ 
Total Loss: 2.0043269603012375
Span Start Loss: 0.8765237626319153
Span End Loss: 0.8227786234123106
Type Loss: 0.3050245398025134
 ------------ Epoch 5/10 Batch 10650/11616 Training Results ------------ 
Total Loss: 2.004071605115464
Span Start Loss: 0.8763811517214761
Span End Loss: 0.8227867456357664
Type Loss: 0.30490367327344564
 ------------ Epoch 5/10 Batch 10700/11616 Training Results ------------ 
Total Loss: 2.004971633594762
Span Start Loss: 0.8768338900015942
Span End Loss: 0.8230950439488081
Type Loss: 0.30504266517136736
 ------------ Epoch 5/10 Batch 10750/11616 Training Results ------------ 
Total Loss: 2.005720726633661
Span Start Loss: 0.8771855714568911
Span End Loss: 0.8233673446978941
Type Loss: 0.30516777607222456
 ------------ Epoch 5/10 Batch 10800/11616 Training Results ------------ 
Total Loss: 2.006558654792779
Span Start Loss: 0.8775281657911492
Span End Loss: 0.8235450433805171
Type Loss: 0.30548541119076417
 ------------ Epoch 5/10 Batch 10850/11616 Training Results ------------ 
Total Loss: 2.0063987061492927
Span Start Loss: 0.8774606544546749
Span End Loss: 0.8236741153191999
Type Loss: 0.3052639020093146
 ------------ Epoch 5/10 Batch 10900/11616 Training Results ------------ 
Total Loss: 2.0065786081793453
Span Start Loss: 0.8772118816078734
Span End Loss: 0.8240599778660257
Type Loss: 0.30530671428850586
 ------------ Epoch 5/10 Batch 10950/11616 Training Results ------------ 
Total Loss: 2.008230623423407
Span Start Loss: 0.877994984307892
Span End Loss: 0.8246202958660991
Type Loss: 0.30561530883406973
 ------------ Epoch 5/10 Batch 11000/11616 Training Results ------------ 
Total Loss: 2.0082203809349712
Span Start Loss: 0.8781905999940566
Span End Loss: 0.8245001755075698
Type Loss: 0.3055295709907077
 ------------ Epoch 5/10 Batch 11050/11616 Training Results ------------ 
Total Loss: 2.0083903633369915
Span Start Loss: 0.8784279189233529
Span End Loss: 0.8244323777616429
Type Loss: 0.30553003224999353
 ------------ Epoch 5/10 Batch 11100/11616 Training Results ------------ 
Total Loss: 2.00864675173301
Span Start Loss: 0.8787336399250135
Span End Loss: 0.8243834910088697
Type Loss: 0.3055295862392984
 ------------ Epoch 5/10 Batch 11150/11616 Training Results ------------ 
Total Loss: 2.0079692304744112
Span Start Loss: 0.8783476979534388
Span End Loss: 0.8236947818023608
Type Loss: 0.3059267161345118
 ------------ Epoch 5/10 Batch 11200/11616 Training Results ------------ 
Total Loss: 2.0078129505979763
Span Start Loss: 0.8782737353608744
Span End Loss: 0.8236822766394887
Type Loss: 0.30585690406005595
 ------------ Epoch 5/10 Batch 11250/11616 Training Results ------------ 
Total Loss: 2.008172686448528
Span Start Loss: 0.8784240768927667
Span End Loss: 0.823869686729047
Type Loss: 0.30587888825953835
 ------------ Epoch 5/10 Batch 11300/11616 Training Results ------------ 
Total Loss: 2.0075801778625926
Span Start Loss: 0.8782505836270222
Span End Loss: 0.8235165135690992
Type Loss: 0.3058130459594994
 ------------ Epoch 5/10 Batch 11350/11616 Training Results ------------ 
Total Loss: 2.0076326668237865
Span Start Loss: 0.8783808452342247
Span End Loss: 0.8235561240208438
Type Loss: 0.30569566282460114
 ------------ Epoch 5/10 Batch 11400/11616 Training Results ------------ 
Total Loss: 2.007628548231074
Span Start Loss: 0.8783978445215249
Span End Loss: 0.823674240583848
Type Loss: 0.3055564284243768
 ------------ Epoch 5/10 Batch 11450/11616 Training Results ------------ 
Total Loss: 2.0081752396965458
Span Start Loss: 0.878761386490187
Span End Loss: 0.8237159126668387
Type Loss: 0.3056979058822007
 ------------ Epoch 5/10 Batch 11500/11616 Training Results ------------ 
Total Loss: 2.0079006856849984
Span Start Loss: 0.8786944041260235
Span End Loss: 0.8235184933974043
Type Loss: 0.3056877535111878
 ------------ Epoch 5/10 Batch 11550/11616 Training Results ------------ 
Total Loss: 2.008819711692954
Span Start Loss: 0.8792190159058997
Span End Loss: 0.8235557920272861
Type Loss: 0.3060448691136006
 ------------ Epoch 5/10 Batch 11600/11616 Training Results ------------ 
Total Loss: 2.0088280126107216
Span Start Loss: 0.8791142621682957
Span End Loss: 0.8237392166838179
Type Loss: 0.30597449912930486
 --------------------- Epoch 5/10 Final Training Results ------------------------ 
Total Loss: 2.008808477447059
Span Start Loss: 0.8792681100083317
Span End Loss: 0.8235357142436465
Type Loss: 0.30600461856505035
 --------------- Epoch 5/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 53.4, 'f1': 65.7, 'turns': 1425}), ('literature', {'em': 51.4, 'f1': 62.3, 'turns': 1630}), ('mid-high_school', {'em': 53.3, 'f1': 65.5, 'turns': 1653}), ('news', {'em': 56.6, 'f1': 68.0, 'turns': 1649}), ('wikipedia', {'em': 57.4, 'f1': 70.3, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 54.4, 'f1': 66.4, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 54.4, 'f1': 66.4, 'turns': 7983})])
 --------------- Epoch 6/10 Training Start --------------- 
 ------------ Epoch 6/10 Batch 50/11616 Training Results ------------ 
Total Loss: 1.3075382858514786
Span Start Loss: 0.5723678100109101
Span End Loss: 0.493180378228426
Type Loss: 0.24199007142335177
 ------------ Epoch 6/10 Batch 100/11616 Training Results ------------ 
Total Loss: 1.4841210052371026
Span Start Loss: 0.6197205187380314
Span End Loss: 0.6107245495170355
Type Loss: 0.2536759069003165
 ------------ Epoch 6/10 Batch 150/11616 Training Results ------------ 
Total Loss: 1.4715059195955595
Span Start Loss: 0.6188751422365506
Span End Loss: 0.5919961682955424
Type Loss: 0.2606345791121324
 ------------ Epoch 6/10 Batch 200/11616 Training Results ------------ 
Total Loss: 1.4651040544733405
Span Start Loss: 0.6092397224903107
Span End Loss: 0.5910822260566055
Type Loss: 0.2647820734791458
 ------------ Epoch 6/10 Batch 250/11616 Training Results ------------ 
Total Loss: 1.4591513885557652
Span Start Loss: 0.6058104614317417
Span End Loss: 0.5932798064500093
Type Loss: 0.26006109131127597
 ------------ Epoch 6/10 Batch 300/11616 Training Results ------------ 
Total Loss: 1.4387648322929938
Span Start Loss: 0.5891091515993079
Span End Loss: 0.5788779464860757
Type Loss: 0.27077770853415134
 ------------ Epoch 6/10 Batch 350/11616 Training Results ------------ 
Total Loss: 1.4562742851674557
Span Start Loss: 0.6082513112361942
Span End Loss: 0.5826744771429471
Type Loss: 0.26534847331100275
 ------------ Epoch 6/10 Batch 400/11616 Training Results ------------ 
Total Loss: 1.4599455653689801
Span Start Loss: 0.6133166500087828
Span End Loss: 0.5818637931346893
Type Loss: 0.26476510040694845
 ------------ Epoch 6/10 Batch 450/11616 Training Results ------------ 
Total Loss: 1.460039340207974
Span Start Loss: 0.6046489992986123
Span End Loss: 0.5890303425739208
Type Loss: 0.26635997767663666
 ------------ Epoch 6/10 Batch 500/11616 Training Results ------------ 
Total Loss: 1.4627926301956178
Span Start Loss: 0.6007051529213786
Span End Loss: 0.5872430563941597
Type Loss: 0.2748443987555802
 ------------ Epoch 6/10 Batch 550/11616 Training Results ------------ 
Total Loss: 1.4504270972176032
Span Start Loss: 0.594316133795814
Span End Loss: 0.5799764554507353
Type Loss: 0.2761344856328585
 ------------ Epoch 6/10 Batch 600/11616 Training Results ------------ 
Total Loss: 1.451732111349702
Span Start Loss: 0.5967283313659331
Span End Loss: 0.5781041172860811
Type Loss: 0.2768996408302337
 ------------ Epoch 6/10 Batch 650/11616 Training Results ------------ 
Total Loss: 1.462063072449886
Span Start Loss: 0.6016193673473138
Span End Loss: 0.5797801029997376
Type Loss: 0.2806635796307371
 ------------ Epoch 6/10 Batch 700/11616 Training Results ------------ 
Total Loss: 1.465570650302938
Span Start Loss: 0.6008649330799068
Span End Loss: 0.5859287582949868
Type Loss: 0.27877693652840596
 ------------ Epoch 6/10 Batch 750/11616 Training Results ------------ 
Total Loss: 1.4663241063555081
Span Start Loss: 0.6002907843093077
Span End Loss: 0.5862848141044378
Type Loss: 0.27974848419676224
 ------------ Epoch 6/10 Batch 800/11616 Training Results ------------ 
Total Loss: 1.4747162368381397
Span Start Loss: 0.6048566384566948
Span End Loss: 0.5895806715590879
Type Loss: 0.28027890280354767
 ------------ Epoch 6/10 Batch 850/11616 Training Results ------------ 
Total Loss: 1.4710826347198556
Span Start Loss: 0.602812299233149
Span End Loss: 0.5871575171106002
Type Loss: 0.2811127943677061
 ------------ Epoch 6/10 Batch 900/11616 Training Results ------------ 
Total Loss: 1.4742083069144023
Span Start Loss: 0.6049865781474445
Span End Loss: 0.588735269871023
Type Loss: 0.2804864362109866
 ------------ Epoch 6/10 Batch 950/11616 Training Results ------------ 
Total Loss: 1.4775299674390179
Span Start Loss: 0.6057016082735438
Span End Loss: 0.5897113113771928
Type Loss: 0.28211702582671455
 ------------ Epoch 6/10 Batch 1000/11616 Training Results ------------ 
Total Loss: 1.4818839647434652
Span Start Loss: 0.608847892805934
Span End Loss: 0.5909589481912554
Type Loss: 0.28207710178382694
 ------------ Epoch 6/10 Batch 1050/11616 Training Results ------------ 
Total Loss: 1.4879949684121778
Span Start Loss: 0.61128309573446
Span End Loss: 0.5954403886589266
Type Loss: 0.2812714609982712
 ------------ Epoch 6/10 Batch 1100/11616 Training Results ------------ 
Total Loss: 1.4877722155472095
Span Start Loss: 0.6097607351433147
Span End Loss: 0.5965294838053259
Type Loss: 0.2814819742315872
 ------------ Epoch 6/10 Batch 1150/11616 Training Results ------------ 
Total Loss: 1.4881559862552778
Span Start Loss: 0.6113275145026653
Span End Loss: 0.5958582066291053
Type Loss: 0.28097024161854517
 ------------ Epoch 6/10 Batch 1200/11616 Training Results ------------ 
Total Loss: 1.4994734262892355
Span Start Loss: 0.6182993033497284
Span End Loss: 0.5985984656742463
Type Loss: 0.2825756332060943
 ------------ Epoch 6/10 Batch 1250/11616 Training Results ------------ 
Total Loss: 1.496264257594943
Span Start Loss: 0.6164646824121475
Span End Loss: 0.5996999331206083
Type Loss: 0.2800996176764369
 ------------ Epoch 6/10 Batch 1300/11616 Training Results ------------ 
Total Loss: 1.4923162086795156
Span Start Loss: 0.6138712077014722
Span End Loss: 0.6000929740035477
Type Loss: 0.2783520021540328
 ------------ Epoch 6/10 Batch 1350/11616 Training Results ------------ 
Total Loss: 1.4853294564893953
Span Start Loss: 0.6119786455427055
Span End Loss: 0.597057497707644
Type Loss: 0.2762932876807948
 ------------ Epoch 6/10 Batch 1400/11616 Training Results ------------ 
Total Loss: 1.4908747731841037
Span Start Loss: 0.6143042598212404
Span End Loss: 0.5997958626298766
Type Loss: 0.27677462592454893
 ------------ Epoch 6/10 Batch 1450/11616 Training Results ------------ 
Total Loss: 1.4867472580537713
Span Start Loss: 0.6124500146276992
Span End Loss: 0.5975319922452086
Type Loss: 0.2767652257692454
 ------------ Epoch 6/10 Batch 1500/11616 Training Results ------------ 
Total Loss: 1.490101172402501
Span Start Loss: 0.6165333122126758
Span End Loss: 0.5974804714738081
Type Loss: 0.27608736271224915
 ------------ Epoch 6/10 Batch 1550/11616 Training Results ------------ 
Total Loss: 1.495080248723107
Span Start Loss: 0.6190916836610244
Span End Loss: 0.5990948098002663
Type Loss: 0.276893729980314
 ------------ Epoch 6/10 Batch 1600/11616 Training Results ------------ 
Total Loss: 1.4953387785470114
Span Start Loss: 0.6198002905969042
Span End Loss: 0.5982780095882481
Type Loss: 0.2772604531090474
 ------------ Epoch 6/10 Batch 1650/11616 Training Results ------------ 
Total Loss: 1.5018613888007222
Span Start Loss: 0.622322981616087
Span End Loss: 0.6013617801841236
Type Loss: 0.2781766021742739
 ------------ Epoch 6/10 Batch 1700/11616 Training Results ------------ 
Total Loss: 1.4979338438304908
Span Start Loss: 0.6206044150067165
Span End Loss: 0.6000912727497737
Type Loss: 0.2772381314114832
 ------------ Epoch 6/10 Batch 1750/11616 Training Results ------------ 
Total Loss: 1.498785667845181
Span Start Loss: 0.620975987439709
Span End Loss: 0.6000720622406475
Type Loss: 0.2777375930936209
 ------------ Epoch 6/10 Batch 1800/11616 Training Results ------------ 
Total Loss: 1.4991613740101457
Span Start Loss: 0.6215578388453772
Span End Loss: 0.5984894951075936
Type Loss: 0.27911401546508485
 ------------ Epoch 6/10 Batch 1850/11616 Training Results ------------ 
Total Loss: 1.4982144407726623
Span Start Loss: 0.6208162898177634
Span End Loss: 0.5970775241299054
Type Loss: 0.28032060238205503
 ------------ Epoch 6/10 Batch 1900/11616 Training Results ------------ 
Total Loss: 1.4950762058951352
Span Start Loss: 0.6188863008851676
Span End Loss: 0.5947950398897458
Type Loss: 0.28139484065780906
 ------------ Epoch 6/10 Batch 1950/11616 Training Results ------------ 
Total Loss: 1.4914303551117578
Span Start Loss: 0.6173859354653037
Span End Loss: 0.5931341881386172
Type Loss: 0.28091020697250196
 ------------ Epoch 6/10 Batch 2000/11616 Training Results ------------ 
Total Loss: 1.4949600968509913
Span Start Loss: 0.6201943252766505
Span End Loss: 0.5942575303246267
Type Loss: 0.280508216786664
 --------------- Epoch 6/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 55.3, 'f1': 67.7, 'turns': 1425}), ('literature', {'em': 54.0, 'f1': 64.9, 'turns': 1630}), ('mid-high_school', {'em': 53.4, 'f1': 65.6, 'turns': 1653}), ('news', {'em': 58.1, 'f1': 69.3, 'turns': 1649}), ('wikipedia', {'em': 59.1, 'f1': 71.4, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 56.0, 'f1': 67.8, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 56.0, 'f1': 67.8, 'turns': 7983})])
 ------------ Epoch 6/10 Batch 2050/11616 Training Results ------------ 
Total Loss: 1.495052832837512
Span Start Loss: 0.6203199338703984
Span End Loss: 0.5947544008806893
Type Loss: 0.2799784738311499
 ------------ Epoch 6/10 Batch 2100/11616 Training Results ------------ 
Total Loss: 1.4942773200660235
Span Start Loss: 0.619861753764784
Span End Loss: 0.5941884945692228
Type Loss: 0.28022704731185166
 ------------ Epoch 6/10 Batch 2150/11616 Training Results ------------ 
Total Loss: 1.4904822602304955
Span Start Loss: 0.6183090831014479
Span End Loss: 0.5922538837385473
Type Loss: 0.2799192687968702
 ------------ Epoch 6/10 Batch 2200/11616 Training Results ------------ 
Total Loss: 1.4904052816797049
Span Start Loss: 0.6173361431828446
Span End Loss: 0.5932472201752138
Type Loss: 0.2798218937942081
 ------------ Epoch 6/10 Batch 2250/11616 Training Results ------------ 
Total Loss: 1.4901020349934697
Span Start Loss: 0.6154061494987044
Span End Loss: 0.5945834802399493
Type Loss: 0.28011238081960216
 ------------ Epoch 6/10 Batch 2300/11616 Training Results ------------ 
Total Loss: 1.4886242289460547
Span Start Loss: 0.6147471022966278
Span End Loss: 0.5939352738527257
Type Loss: 0.27994182807636325
 ------------ Epoch 6/10 Batch 2350/11616 Training Results ------------ 
Total Loss: 1.489468349579801
Span Start Loss: 0.6146161442650284
Span End Loss: 0.5949977619163296
Type Loss: 0.27985441879587286
 ------------ Epoch 6/10 Batch 2400/11616 Training Results ------------ 
Total Loss: 1.4866081612727915
Span Start Loss: 0.6125356497285732
Span End Loss: 0.5936070197193961
Type Loss: 0.28046546746084156
 ------------ Epoch 6/10 Batch 2450/11616 Training Results ------------ 
Total Loss: 1.486323010967094
Span Start Loss: 0.6120977549220682
Span End Loss: 0.5940969579797999
Type Loss: 0.2801282734861027
 ------------ Epoch 6/10 Batch 2500/11616 Training Results ------------ 
Total Loss: 1.4813493558689952
Span Start Loss: 0.6102744497474283
Span End Loss: 0.5909996178762987
Type Loss: 0.28007526348046957
 ------------ Epoch 6/10 Batch 2550/11616 Training Results ------------ 
Total Loss: 1.4803363745192102
Span Start Loss: 0.609021613176766
Span End Loss: 0.590533379209867
Type Loss: 0.28078135734789217
 ------------ Epoch 6/10 Batch 2600/11616 Training Results ------------ 
Total Loss: 1.4817547633312642
Span Start Loss: 0.6090559119269109
Span End Loss: 0.5910986209992104
Type Loss: 0.2816002057947648
 ------------ Epoch 6/10 Batch 2650/11616 Training Results ------------ 
Total Loss: 1.4856159615980566
Span Start Loss: 0.6122301787710837
Span End Loss: 0.5919646695848414
Type Loss: 0.28142108880791744
 ------------ Epoch 6/10 Batch 2700/11616 Training Results ------------ 
Total Loss: 1.4853538132031208
Span Start Loss: 0.6136402118726875
Span End Loss: 0.5908860940612094
Type Loss: 0.2808274830074084
 ------------ Epoch 6/10 Batch 2750/11616 Training Results ------------ 
Total Loss: 1.4846880489249121
Span Start Loss: 0.6123122935020788
Span End Loss: 0.5908513472556052
Type Loss: 0.28152438372068783
 ------------ Epoch 6/10 Batch 2800/11616 Training Results ------------ 
Total Loss: 1.4829437042120843
Span Start Loss: 0.6108340932465425
Span End Loss: 0.5902626083207516
Type Loss: 0.28184697845584844
 ------------ Epoch 6/10 Batch 2850/11616 Training Results ------------ 
Total Loss: 1.4848765120106309
Span Start Loss: 0.6104447840817534
Span End Loss: 0.5927339764202438
Type Loss: 0.28169772723820385
 ------------ Epoch 6/10 Batch 2900/11616 Training Results ------------ 
Total Loss: 1.481753749348994
Span Start Loss: 0.6085735851022062
Span End Loss: 0.5910586472191238
Type Loss: 0.28212149296501843
 ------------ Epoch 6/10 Batch 2950/11616 Training Results ------------ 
Total Loss: 1.4790080234155816
Span Start Loss: 0.6071446159175771
Span End Loss: 0.5894416070731072
Type Loss: 0.2824217767584122
 ------------ Epoch 6/10 Batch 3000/11616 Training Results ------------ 
Total Loss: 1.4750888944293061
Span Start Loss: 0.6045163741034145
Span End Loss: 0.5888519978475136
Type Loss: 0.28172049891576173
 ------------ Epoch 6/10 Batch 3050/11616 Training Results ------------ 
Total Loss: 1.472357724448208
Span Start Loss: 0.6035975526581656
Span End Loss: 0.5877457775653446
Type Loss: 0.28101437074605556
 ------------ Epoch 6/10 Batch 3100/11616 Training Results ------------ 
Total Loss: 1.4722606667467664
Span Start Loss: 0.6031152866252008
Span End Loss: 0.5876883417549693
Type Loss: 0.2814570150380173
 ------------ Epoch 6/10 Batch 3150/11616 Training Results ------------ 
Total Loss: 1.4736718164195144
Span Start Loss: 0.6035318100121286
Span End Loss: 0.5885457561107441
Type Loss: 0.281594226700919
 ------------ Epoch 6/10 Batch 3200/11616 Training Results ------------ 
Total Loss: 1.4775792710250244
Span Start Loss: 0.6054760798846837
Span End Loss: 0.589502810563572
Type Loss: 0.28260035676765255
 ------------ Epoch 6/10 Batch 3250/11616 Training Results ------------ 
Total Loss: 1.4776453637618285
Span Start Loss: 0.6051448900344281
Span End Loss: 0.5900612183879488
Type Loss: 0.2824392314507411
 ------------ Epoch 6/10 Batch 3300/11616 Training Results ------------ 
Total Loss: 1.4820485548878257
Span Start Loss: 0.6068658755211668
Span End Loss: 0.5923370788067184
Type Loss: 0.28284557652236386
 ------------ Epoch 6/10 Batch 3350/11616 Training Results ------------ 
Total Loss: 1.4823975079539997
Span Start Loss: 0.6075669372248561
Span End Loss: 0.5924837526836113
Type Loss: 0.2823467939230266
 ------------ Epoch 6/10 Batch 3400/11616 Training Results ------------ 
Total Loss: 1.4835192379706046
Span Start Loss: 0.6078240144691046
Span End Loss: 0.5935623560869135
Type Loss: 0.28213284354821283
 ------------ Epoch 6/10 Batch 3450/11616 Training Results ------------ 
Total Loss: 1.4839369644505391
Span Start Loss: 0.6075196579347054
Span End Loss: 0.5941146911293322
Type Loss: 0.28230259142708086
 ------------ Epoch 6/10 Batch 3500/11616 Training Results ------------ 
Total Loss: 1.4841033662962062
Span Start Loss: 0.6071431645133666
Span End Loss: 0.5939583632862195
Type Loss: 0.28300181421318227
 ------------ Epoch 6/10 Batch 3550/11616 Training Results ------------ 
Total Loss: 1.4824465697290192
Span Start Loss: 0.6066142666413331
Span End Loss: 0.5932454027278795
Type Loss: 0.2825868759923418
 ------------ Epoch 6/10 Batch 3600/11616 Training Results ------------ 
Total Loss: 1.4816899667080077
Span Start Loss: 0.6057578106255581
Span End Loss: 0.5924390230753407
Type Loss: 0.2834931084803409
 ------------ Epoch 6/10 Batch 3650/11616 Training Results ------------ 
Total Loss: 1.4830991230496804
Span Start Loss: 0.606967974477639
Span End Loss: 0.5935431772624844
Type Loss: 0.2825879469105642
 ------------ Epoch 6/10 Batch 3700/11616 Training Results ------------ 
Total Loss: 1.484361359477043
Span Start Loss: 0.6083204935299786
Span End Loss: 0.5936987671972177
Type Loss: 0.28234207416204987
 ------------ Epoch 6/10 Batch 3750/11616 Training Results ------------ 
Total Loss: 1.484969474290808
Span Start Loss: 0.6084886379363637
Span End Loss: 0.593695862961933
Type Loss: 0.28278494857350983
 ------------ Epoch 6/10 Batch 3800/11616 Training Results ------------ 
Total Loss: 1.4849631227209772
Span Start Loss: 0.6081530532655061
Span End Loss: 0.5930011474303166
Type Loss: 0.2838088973248868
 ------------ Epoch 6/10 Batch 3850/11616 Training Results ------------ 
Total Loss: 1.4844121294749248
Span Start Loss: 0.6087470188836095
Span End Loss: 0.5918850763298136
Type Loss: 0.28378000965462874
 ------------ Epoch 6/10 Batch 3900/11616 Training Results ------------ 
Total Loss: 1.4839295063502131
Span Start Loss: 0.6083722616046763
Span End Loss: 0.5917707396949379
Type Loss: 0.28378648035801374
 ------------ Epoch 6/10 Batch 3950/11616 Training Results ------------ 
Total Loss: 1.4836946998711062
Span Start Loss: 0.6085784513637706
Span End Loss: 0.5912320380661448
Type Loss: 0.2838841857756429
 ------------ Epoch 6/10 Batch 4000/11616 Training Results ------------ 
Total Loss: 1.4850935863447376
Span Start Loss: 0.6093907829059754
Span End Loss: 0.5918391398134408
Type Loss: 0.28386363895842803
 --------------- Epoch 6/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 56.8, 'f1': 68.0, 'turns': 1425}), ('literature', {'em': 53.8, 'f1': 64.7, 'turns': 1630}), ('mid-high_school', {'em': 55.7, 'f1': 66.3, 'turns': 1653}), ('news', {'em': 59.1, 'f1': 69.8, 'turns': 1649}), ('wikipedia', {'em': 59.6, 'f1': 71.1, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 57.0, 'f1': 68.0, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 57.0, 'f1': 68.0, 'turns': 7983})])
 ------------ Epoch 6/10 Batch 4050/11616 Training Results ------------ 
Total Loss: 1.4866802658862721
Span Start Loss: 0.6100392131684463
Span End Loss: 0.592819601278867
Type Loss: 0.2838214265504553
 ------------ Epoch 6/10 Batch 4100/11616 Training Results ------------ 
Total Loss: 1.4880226383780744
Span Start Loss: 0.6099918142535792
Span End Loss: 0.5929043601433996
Type Loss: 0.28512643911080754
 ------------ Epoch 6/10 Batch 4150/11616 Training Results ------------ 
Total Loss: 1.487687771275909
Span Start Loss: 0.6099243579026177
Span End Loss: 0.5928930049900145
Type Loss: 0.2848703834903707
 ------------ Epoch 6/10 Batch 4200/11616 Training Results ------------ 
Total Loss: 1.4891894388433902
Span Start Loss: 0.610512315310937
Span End Loss: 0.5940148120815866
Type Loss: 0.28466228644230535
 ------------ Epoch 6/10 Batch 4250/11616 Training Results ------------ 
Total Loss: 1.4886269527695635
Span Start Loss: 0.6100207208984477
Span End Loss: 0.5939646901092765
Type Loss: 0.2846415168739417
 ------------ Epoch 6/10 Batch 4300/11616 Training Results ------------ 
Total Loss: 1.491676111885368
Span Start Loss: 0.6112309697593108
Span End Loss: 0.5953923114892745
Type Loss: 0.2850528056902248
 ------------ Epoch 6/10 Batch 4350/11616 Training Results ------------ 
Total Loss: 1.4924637423185954
Span Start Loss: 0.6122362692288027
Span End Loss: 0.5957741204085064
Type Loss: 0.2844533277110293
 ------------ Epoch 6/10 Batch 4400/11616 Training Results ------------ 
Total Loss: 1.4928978537061166
Span Start Loss: 0.6123675063085234
Span End Loss: 0.5959916362158997
Type Loss: 0.28453868595756254
 ------------ Epoch 6/10 Batch 4450/11616 Training Results ------------ 
Total Loss: 1.4937033390165881
Span Start Loss: 0.6124775375277223
Span End Loss: 0.5968763940531259
Type Loss: 0.2843493821986773
 ------------ Epoch 6/10 Batch 4500/11616 Training Results ------------ 
Total Loss: 1.4945396858888367
Span Start Loss: 0.6130601873987664
Span End Loss: 0.5971517131221998
Type Loss: 0.28432776007842686
 ------------ Epoch 6/10 Batch 4550/11616 Training Results ------------ 
Total Loss: 1.4967229370826056
Span Start Loss: 0.6149792477859
Span End Loss: 0.5975756199012132
Type Loss: 0.2841680439033515
 ------------ Epoch 6/10 Batch 4600/11616 Training Results ------------ 
Total Loss: 1.4979941736927014
Span Start Loss: 0.6153609546239528
Span End Loss: 0.5979954630036272
Type Loss: 0.28463773073669035
 ------------ Epoch 6/10 Batch 4650/11616 Training Results ------------ 
Total Loss: 1.4980470341319838
Span Start Loss: 0.6149917056113081
Span End Loss: 0.5982719633909523
Type Loss: 0.28478333987576027
 ------------ Epoch 6/10 Batch 4700/11616 Training Results ------------ 
Total Loss: 1.4995983344383854
Span Start Loss: 0.615720954049339
Span End Loss: 0.5987250858367956
Type Loss: 0.2851522694480546
 ------------ Epoch 6/10 Batch 4750/11616 Training Results ------------ 
Total Loss: 1.5002962371883424
Span Start Loss: 0.6170193617824269
Span End Loss: 0.5983196419976455
Type Loss: 0.2849572082577567
 ------------ Epoch 6/10 Batch 4800/11616 Training Results ------------ 
Total Loss: 1.501794948260067
Span Start Loss: 0.6176721206441289
Span End Loss: 0.5992565303106676
Type Loss: 0.2848662720484814
 ------------ Epoch 6/10 Batch 4850/11616 Training Results ------------ 
Total Loss: 1.5041917635703979
Span Start Loss: 0.6187349967186138
Span End Loss: 0.6004913895320838
Type Loss: 0.2849653520915158
 ------------ Epoch 6/10 Batch 4900/11616 Training Results ------------ 
Total Loss: 1.5081708011597548
Span Start Loss: 0.6208340538403362
Span End Loss: 0.602080873922749
Type Loss: 0.2852558483567317
 ------------ Epoch 6/10 Batch 4950/11616 Training Results ------------ 
Total Loss: 1.508913211059766
Span Start Loss: 0.6212981685104244
Span End Loss: 0.6021591480329369
Type Loss: 0.285455869449058
 ------------ Epoch 6/10 Batch 5000/11616 Training Results ------------ 
Total Loss: 1.5084120522681623
Span Start Loss: 0.6209859544243663
Span End Loss: 0.6017404082407244
Type Loss: 0.2856856645807624
 ------------ Epoch 6/10 Batch 5050/11616 Training Results ------------ 
Total Loss: 1.5081846882082006
Span Start Loss: 0.6203534390338429
Span End Loss: 0.6018977533610039
Type Loss: 0.28593347074297987
 ------------ Epoch 6/10 Batch 5100/11616 Training Results ------------ 
Total Loss: 1.5080368456321165
Span Start Loss: 0.6200166669601173
Span End Loss: 0.6023768657728044
Type Loss: 0.2856432876480268
 ------------ Epoch 6/10 Batch 5150/11616 Training Results ------------ 
Total Loss: 1.510276306979358
Span Start Loss: 0.6208156907663328
Span End Loss: 0.604164704401679
Type Loss: 0.28529588645125187
 ------------ Epoch 6/10 Batch 5200/11616 Training Results ------------ 
Total Loss: 1.5106652018166362
Span Start Loss: 0.6218177279027609
Span End Loss: 0.6040693549673037
Type Loss: 0.28477809339672183
 ------------ Epoch 6/10 Batch 5250/11616 Training Results ------------ 
Total Loss: 1.5109713800943323
Span Start Loss: 0.621904524885118
Span End Loss: 0.6041513280589134
Type Loss: 0.28491550169051405
 ------------ Epoch 6/10 Batch 5300/11616 Training Results ------------ 
Total Loss: 1.5100261227965777
Span Start Loss: 0.6218434860792784
Span End Loss: 0.6032653757582752
Type Loss: 0.28491723549833414
 ------------ Epoch 6/10 Batch 5350/11616 Training Results ------------ 
Total Loss: 1.5104729164652875
Span Start Loss: 0.6219140722230077
Span End Loss: 0.6037499056266464
Type Loss: 0.28480891313978307
 ------------ Epoch 6/10 Batch 5400/11616 Training Results ------------ 
Total Loss: 1.5102979913923062
Span Start Loss: 0.6215770426775432
Span End Loss: 0.6038369706049734
Type Loss: 0.284883952600778
 ------------ Epoch 6/10 Batch 5450/11616 Training Results ------------ 
Total Loss: 1.5110113323913938
Span Start Loss: 0.6224197379325371
Span End Loss: 0.6038881664918822
Type Loss: 0.28470340253494747
 ------------ Epoch 6/10 Batch 5500/11616 Training Results ------------ 
Total Loss: 1.5117918318018995
Span Start Loss: 0.6237802888036451
Span End Loss: 0.6040978055079044
Type Loss: 0.28391371196542275
 ------------ Epoch 6/10 Batch 5550/11616 Training Results ------------ 
Total Loss: 1.5136627564340963
Span Start Loss: 0.6248099153472094
Span End Loss: 0.6048852143461002
Type Loss: 0.28396760114510583
 ------------ Epoch 6/10 Batch 5600/11616 Training Results ------------ 
Total Loss: 1.5145444376311
Span Start Loss: 0.6253200454866913
Span End Loss: 0.6050517895668911
Type Loss: 0.2841725770394052
 ------------ Epoch 6/10 Batch 5650/11616 Training Results ------------ 
Total Loss: 1.5132986475606407
Span Start Loss: 0.6247450595128193
Span End Loss: 0.6046993093585711
Type Loss: 0.28385425316565466
 ------------ Epoch 6/10 Batch 5700/11616 Training Results ------------ 
Total Loss: 1.5128739413070051
Span Start Loss: 0.6240880442427047
Span End Loss: 0.6050662503702762
Type Loss: 0.2837196211678613
 ------------ Epoch 6/10 Batch 5750/11616 Training Results ------------ 
Total Loss: 1.513746094826771
Span Start Loss: 0.6248419657793381
Span End Loss: 0.6047714083253851
Type Loss: 0.2841326952328824
 ------------ Epoch 6/10 Batch 5800/11616 Training Results ------------ 
Total Loss: 1.5145558553394574
Span Start Loss: 0.6246953050933521
Span End Loss: 0.6055625927815181
Type Loss: 0.28429793180946006
 ------------ Epoch 6/10 Batch 5850/11616 Training Results ------------ 
Total Loss: 1.5147906158915443
Span Start Loss: 0.625184873117723
Span End Loss: 0.6054302570307388
Type Loss: 0.28417545997672994
 ------------ Epoch 6/10 Batch 5900/11616 Training Results ------------ 
Total Loss: 1.5161490445293613
Span Start Loss: 0.6258655069623206
Span End Loss: 0.6060184792493138
Type Loss: 0.28426503269080766
 ------------ Epoch 6/10 Batch 5950/11616 Training Results ------------ 
Total Loss: 1.5155051384653364
Span Start Loss: 0.6254200859486806
Span End Loss: 0.6057814456911429
Type Loss: 0.28430358120608107
 ------------ Epoch 6/10 Batch 6000/11616 Training Results ------------ 
Total Loss: 1.516668830383569
Span Start Loss: 0.6260812465058019
Span End Loss: 0.6062452003397824
Type Loss: 0.28434235785730805
 --------------- Epoch 6/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 56.1, 'f1': 67.7, 'turns': 1425}), ('literature', {'em': 53.4, 'f1': 64.1, 'turns': 1630}), ('mid-high_school', {'em': 54.4, 'f1': 66.1, 'turns': 1653}), ('news', {'em': 56.1, 'f1': 68.0, 'turns': 1649}), ('wikipedia', {'em': 58.9, 'f1': 71.2, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 55.8, 'f1': 67.4, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 55.8, 'f1': 67.4, 'turns': 7983})])
 ------------ Epoch 6/10 Batch 6050/11616 Training Results ------------ 
Total Loss: 1.5170856269738398
Span Start Loss: 0.6264201389068415
Span End Loss: 0.6061950702658154
Type Loss: 0.2844703921920443
 ------------ Epoch 6/10 Batch 6100/11616 Training Results ------------ 
Total Loss: 1.5163609686017525
Span Start Loss: 0.6265435142473119
Span End Loss: 0.6053888131593369
Type Loss: 0.28442861562167276
 ------------ Epoch 6/10 Batch 6150/11616 Training Results ------------ 
Total Loss: 1.516021228993569
Span Start Loss: 0.6261271191951705
Span End Loss: 0.60528334807915
Type Loss: 0.2846107362777116
 ------------ Epoch 6/10 Batch 6200/11616 Training Results ------------ 
Total Loss: 1.516722725722338
Span Start Loss: 0.627105716768532
Span End Loss: 0.604842513164941
Type Loss: 0.2847744703782542
 ------------ Epoch 6/10 Batch 6250/11616 Training Results ------------ 
Total Loss: 1.5162331567645073
Span Start Loss: 0.6270710557758808
Span End Loss: 0.6045378397879749
Type Loss: 0.2846242357069254
 ------------ Epoch 6/10 Batch 6300/11616 Training Results ------------ 
Total Loss: 1.5177383135826814
Span Start Loss: 0.6274738389637972
Span End Loss: 0.6054170259683289
Type Loss: 0.28484742303718885
 ------------ Epoch 6/10 Batch 6350/11616 Training Results ------------ 
Total Loss: 1.5158369656270883
Span Start Loss: 0.6268233251284192
Span End Loss: 0.6041807461850434
Type Loss: 0.2848328687758075
 ------------ Epoch 6/10 Batch 6400/11616 Training Results ------------ 
Total Loss: 1.5148332366457908
Span Start Loss: 0.6260710552081582
Span End Loss: 0.6038556993084058
Type Loss: 0.2849064565976732
 ------------ Epoch 6/10 Batch 6450/11616 Training Results ------------ 
Total Loss: 1.5143318402241597
Span Start Loss: 0.6254101902968431
Span End Loss: 0.603962549097735
Type Loss: 0.2849590753167579
 ------------ Epoch 6/10 Batch 6500/11616 Training Results ------------ 
Total Loss: 1.5146614623556915
Span Start Loss: 0.6256135021614341
Span End Loss: 0.6036423070110524
Type Loss: 0.2854056274442432
 ------------ Epoch 6/10 Batch 6550/11616 Training Results ------------ 
Total Loss: 1.515783275916945
Span Start Loss: 0.6263652451103198
Span End Loss: 0.603599634052304
Type Loss: 0.28581837094708823
 ------------ Epoch 6/10 Batch 6600/11616 Training Results ------------ 
Total Loss: 1.5154770656319505
Span Start Loss: 0.6261406863023612
Span End Loss: 0.6034297873673142
Type Loss: 0.2859065662912855
 ------------ Epoch 6/10 Batch 6650/11616 Training Results ------------ 
Total Loss: 1.5166404832042473
Span Start Loss: 0.6269018742705422
Span End Loss: 0.6037590846980229
Type Loss: 0.2859794985173844
 ------------ Epoch 6/10 Batch 6700/11616 Training Results ------------ 
Total Loss: 1.516385487978583
Span Start Loss: 0.6271075739761565
Span End Loss: 0.6035183165473308
Type Loss: 0.2857595716554449
 ------------ Epoch 6/10 Batch 6750/11616 Training Results ------------ 
Total Loss: 1.515897298168253
Span Start Loss: 0.6276558397053569
Span End Loss: 0.6027362603396867
Type Loss: 0.28550517227735234
 ------------ Epoch 6/10 Batch 6800/11616 Training Results ------------ 
Total Loss: 1.515510512362508
Span Start Loss: 0.6276971621071811
Span End Loss: 0.6025222922111725
Type Loss: 0.285291032192489
 ------------ Epoch 6/10 Batch 6850/11616 Training Results ------------ 
Total Loss: 1.517092541703125
Span Start Loss: 0.6287037736704967
Span End Loss: 0.6027443601915028
Type Loss: 0.2856443819505618
 ------------ Epoch 6/10 Batch 6900/11616 Training Results ------------ 
Total Loss: 1.517045186465417
Span Start Loss: 0.6287317970206124
Span End Loss: 0.6024905716903859
Type Loss: 0.28582279190299625
 ------------ Epoch 6/10 Batch 6950/11616 Training Results ------------ 
Total Loss: 1.5181411697415355
Span Start Loss: 0.6288619665045747
Span End Loss: 0.6031845405376884
Type Loss: 0.2860946366307493
 ------------ Epoch 6/10 Batch 7000/11616 Training Results ------------ 
Total Loss: 1.51867855838633
Span Start Loss: 0.6291981129545187
Span End Loss: 0.6032503990169747
Type Loss: 0.2862300203276266
 ------------ Epoch 6/10 Batch 7050/11616 Training Results ------------ 
Total Loss: 1.5179671633169582
Span Start Loss: 0.6287937552563794
Span End Loss: 0.6030035895186456
Type Loss: 0.2861697924960114
 ------------ Epoch 6/10 Batch 7100/11616 Training Results ------------ 
Total Loss: 1.5197269454650895
Span Start Loss: 0.6300911310013436
Span End Loss: 0.6036743759262231
Type Loss: 0.2859614123470924
 ------------ Epoch 6/10 Batch 7150/11616 Training Results ------------ 
Total Loss: 1.5198721415799605
Span Start Loss: 0.6305936709103676
Span End Loss: 0.6033901657956698
Type Loss: 0.2858882787546234
 ------------ Epoch 6/10 Batch 7200/11616 Training Results ------------ 
Total Loss: 1.5202027614306037
Span Start Loss: 0.6307147188852024
Span End Loss: 0.6030658722762665
Type Loss: 0.286422144224117
 ------------ Epoch 6/10 Batch 7250/11616 Training Results ------------ 
Total Loss: 1.521238680656614
Span Start Loss: 0.6317932856622441
Span End Loss: 0.6030740879877503
Type Loss: 0.2863712808581005
 ------------ Epoch 6/10 Batch 7300/11616 Training Results ------------ 
Total Loss: 1.5216029796188009
Span Start Loss: 0.6316537560037758
Span End Loss: 0.6033744972955104
Type Loss: 0.28657470004441704
 ------------ Epoch 6/10 Batch 7350/11616 Training Results ------------ 
Total Loss: 1.5198965878156172
Span Start Loss: 0.6312410935921734
Span End Loss: 0.6022446895155701
Type Loss: 0.28641077851141694
 ------------ Epoch 6/10 Batch 7400/11616 Training Results ------------ 
Total Loss: 1.5208333453465555
Span Start Loss: 0.6317435620090849
Span End Loss: 0.6028488599552421
Type Loss: 0.28624089716946255
 ------------ Epoch 6/10 Batch 7450/11616 Training Results ------------ 
Total Loss: 1.5223688982177102
Span Start Loss: 0.6324837987199926
Span End Loss: 0.6033112030164433
Type Loss: 0.2865738703811779
 ------------ Epoch 6/10 Batch 7500/11616 Training Results ------------ 
Total Loss: 1.52317199378709
Span Start Loss: 0.6326919188385208
Span End Loss: 0.6037319044801717
Type Loss: 0.2867481443416327
 ------------ Epoch 6/10 Batch 7550/11616 Training Results ------------ 
Total Loss: 1.5234487045926368
Span Start Loss: 0.6328041174716704
Span End Loss: 0.6040733578147995
Type Loss: 0.28657120308351536
 ------------ Epoch 6/10 Batch 7600/11616 Training Results ------------ 
Total Loss: 1.524163389882367
Span Start Loss: 0.6327634533685877
Span End Loss: 0.6049791698018702
Type Loss: 0.28642074054290884
 ------------ Epoch 6/10 Batch 7650/11616 Training Results ------------ 
Total Loss: 1.524854778186753
Span Start Loss: 0.6329644588429748
Span End Loss: 0.60527778837982
Type Loss: 0.2866125048612993
 ------------ Epoch 6/10 Batch 7700/11616 Training Results ------------ 
Total Loss: 1.5244831571863455
Span Start Loss: 0.6326809878139333
Span End Loss: 0.6050381098240735
Type Loss: 0.28676403347293955
 ------------ Epoch 6/10 Batch 7750/11616 Training Results ------------ 
Total Loss: 1.5236906428130403
Span Start Loss: 0.6321934654711475
Span End Loss: 0.6049177565121002
Type Loss: 0.2865793947302286
 ------------ Epoch 6/10 Batch 7800/11616 Training Results ------------ 
Total Loss: 1.524018898909577
Span Start Loss: 0.6326426589702709
Span End Loss: 0.6047819214353027
Type Loss: 0.2865942924628512
 ------------ Epoch 6/10 Batch 7850/11616 Training Results ------------ 
Total Loss: 1.526299905935955
Span Start Loss: 0.6337509738026293
Span End Loss: 0.6054111055812093
Type Loss: 0.2871378003427415
 ------------ Epoch 6/10 Batch 7900/11616 Training Results ------------ 
Total Loss: 1.5269801104252663
Span Start Loss: 0.6343627000773396
Span End Loss: 0.6055501177096504
Type Loss: 0.28706726644706876
 ------------ Epoch 6/10 Batch 7950/11616 Training Results ------------ 
Total Loss: 1.52689720918688
Span Start Loss: 0.6341107326337142
Span End Loss: 0.605653201828084
Type Loss: 0.2871332486151899
 ------------ Epoch 6/10 Batch 8000/11616 Training Results ------------ 
Total Loss: 1.5265891035122332
Span Start Loss: 0.6335373279798078
Span End Loss: 0.6059324422232457
Type Loss: 0.28711930713476613
 --------------- Epoch 6/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 54.9, 'f1': 67.5, 'turns': 1425}), ('literature', {'em': 51.7, 'f1': 63.4, 'turns': 1630}), ('mid-high_school', {'em': 54.1, 'f1': 66.4, 'turns': 1653}), ('news', {'em': 56.2, 'f1': 67.8, 'turns': 1649}), ('wikipedia', {'em': 60.1, 'f1': 72.5, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 55.4, 'f1': 67.5, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 55.4, 'f1': 67.5, 'turns': 7983})])
 ------------ Epoch 6/10 Batch 8050/11616 Training Results ------------ 
Total Loss: 1.5268262576624196
Span Start Loss: 0.6335129756194695
Span End Loss: 0.6062730489644043
Type Loss: 0.28704020696836785
 ------------ Epoch 6/10 Batch 8100/11616 Training Results ------------ 
Total Loss: 1.5268638549453037
Span Start Loss: 0.6331737514075904
Span End Loss: 0.6068713866763483
Type Loss: 0.28681869072333716
 ------------ Epoch 6/10 Batch 8150/11616 Training Results ------------ 
Total Loss: 1.5278689826387295
Span Start Loss: 0.6334170627832824
Span End Loss: 0.607316799154048
Type Loss: 0.2871350946256811
 ------------ Epoch 6/10 Batch 8200/11616 Training Results ------------ 
Total Loss: 1.5278466727414237
Span Start Loss: 0.6331351487570238
Span End Loss: 0.6073490267363377
Type Loss: 0.287362471135424
 ------------ Epoch 6/10 Batch 8250/11616 Training Results ------------ 
Total Loss: 1.528710075622481
Span Start Loss: 0.6331792423165206
Span End Loss: 0.6078058125000221
Type Loss: 0.2877249947501854
 ------------ Epoch 6/10 Batch 8300/11616 Training Results ------------ 
Total Loss: 1.528075847147504
Span Start Loss: 0.6327349958685508
Span End Loss: 0.6074808363637225
Type Loss: 0.28785998879000546
 ------------ Epoch 6/10 Batch 8350/11616 Training Results ------------ 
Total Loss: 1.5277530361535752
Span Start Loss: 0.6325227092081558
Span End Loss: 0.6073750420684876
Type Loss: 0.28785525867570483
 ------------ Epoch 6/10 Batch 8400/11616 Training Results ------------ 
Total Loss: 1.5276993781073196
Span Start Loss: 0.6322520603993464
Span End Loss: 0.6075562617189938
Type Loss: 0.28789102974081676
 ------------ Epoch 6/10 Batch 8450/11616 Training Results ------------ 
Total Loss: 1.5288235452896832
Span Start Loss: 0.6325302174590396
Span End Loss: 0.6083983137073359
Type Loss: 0.2878949879458141
 ------------ Epoch 6/10 Batch 8500/11616 Training Results ------------ 
Total Loss: 1.5289213691706605
Span Start Loss: 0.6327356074592646
Span End Loss: 0.6086755493167678
Type Loss: 0.2875101862229845
 ------------ Epoch 6/10 Batch 8550/11616 Training Results ------------ 
Total Loss: 1.5285329750417704
Span Start Loss: 0.6324750903322857
Span End Loss: 0.608467962215355
Type Loss: 0.28758989631781107
 ------------ Epoch 6/10 Batch 8600/11616 Training Results ------------ 
Total Loss: 1.528328032963801
Span Start Loss: 0.6323410904870996
Span End Loss: 0.6083347330569983
Type Loss: 0.2876521831828841
 ------------ Epoch 6/10 Batch 8650/11616 Training Results ------------ 
Total Loss: 1.528838397410693
Span Start Loss: 0.6329077443398664
Span End Loss: 0.6084304919286033
Type Loss: 0.28750013493755133
 ------------ Epoch 6/10 Batch 8700/11616 Training Results ------------ 
Total Loss: 1.5299357944532115
Span Start Loss: 0.6330282000484395
Span End Loss: 0.6092333461744871
Type Loss: 0.2876742219464618
 ------------ Epoch 6/10 Batch 8750/11616 Training Results ------------ 
Total Loss: 1.5290183018435326
Span Start Loss: 0.6327430618828961
Span End Loss: 0.6088724397648925
Type Loss: 0.28740277391842434
 ------------ Epoch 6/10 Batch 8800/11616 Training Results ------------ 
Total Loss: 1.5275465756051496
Span Start Loss: 0.6317307254813865
Span End Loss: 0.6085371222172663
Type Loss: 0.2872787016208961
 ------------ Epoch 6/10 Batch 8850/11616 Training Results ------------ 
Total Loss: 1.528191808052361
Span Start Loss: 0.6318261634602638
Span End Loss: 0.6086786851513405
Type Loss: 0.28768693322315814
 ------------ Epoch 6/10 Batch 8900/11616 Training Results ------------ 
Total Loss: 1.5281019420551367
Span Start Loss: 0.6318361718783134
Span End Loss: 0.6088157127742404
Type Loss: 0.2874500311592991
 ------------ Epoch 6/10 Batch 8950/11616 Training Results ------------ 
Total Loss: 1.5276164102290013
Span Start Loss: 0.6316153396909546
Span End Loss: 0.608457282437714
Type Loss: 0.28754376197608406
 ------------ Epoch 6/10 Batch 9000/11616 Training Results ------------ 
Total Loss: 1.5275718808639795
Span Start Loss: 0.631567373416697
Span End Loss: 0.6082470259264163
Type Loss: 0.2877574554072279
 ------------ Epoch 6/10 Batch 9050/11616 Training Results ------------ 
Total Loss: 1.5275777826044523
Span Start Loss: 0.6316855111967217
Span End Loss: 0.6081065953857631
Type Loss: 0.2877856499214981
 ------------ Epoch 6/10 Batch 9100/11616 Training Results ------------ 
Total Loss: 1.5283726684738892
Span Start Loss: 0.6321704046127314
Span End Loss: 0.6084346056980552
Type Loss: 0.2877676319632843
 ------------ Epoch 6/10 Batch 9150/11616 Training Results ------------ 
Total Loss: 1.5283537554111881
Span Start Loss: 0.6322835590297489
Span End Loss: 0.6082733448567581
Type Loss: 0.2877968253782579
 ------------ Epoch 6/10 Batch 9200/11616 Training Results ------------ 
Total Loss: 1.5278399683322514
Span Start Loss: 0.6320807294695354
Span End Loss: 0.6080294409946224
Type Loss: 0.28772977183927256
 ------------ Epoch 6/10 Batch 9250/11616 Training Results ------------ 
Total Loss: 1.5277371790471512
Span Start Loss: 0.6318941721974595
Span End Loss: 0.6081302323721198
Type Loss: 0.28771274845372585
 ------------ Epoch 6/10 Batch 9300/11616 Training Results ------------ 
Total Loss: 1.5284759166129736
Span Start Loss: 0.6322121167689642
Span End Loss: 0.6084034336071151
Type Loss: 0.2878603402324902
 ------------ Epoch 6/10 Batch 9350/11616 Training Results ------------ 
Total Loss: 1.5281920025125146
Span Start Loss: 0.6320358702530438
Span End Loss: 0.6080673389357171
Type Loss: 0.28808876740223266
 ------------ Epoch 6/10 Batch 9400/11616 Training Results ------------ 
Total Loss: 1.529245402583694
Span Start Loss: 0.6324838642389613
Span End Loss: 0.6087417995102674
Type Loss: 0.2880197127310044
 ------------ Epoch 6/10 Batch 9450/11616 Training Results ------------ 
Total Loss: 1.5301213464911574
Span Start Loss: 0.6329155610046453
Span End Loss: 0.6090681705396455
Type Loss: 0.28813758874243056
 ------------ Epoch 6/10 Batch 9500/11616 Training Results ------------ 
Total Loss: 1.5299749297168301
Span Start Loss: 0.6329202369551518
Span End Loss: 0.6091109024714924
Type Loss: 0.28794376400113103
 ------------ Epoch 6/10 Batch 9550/11616 Training Results ------------ 
Total Loss: 1.5299004938931287
Span Start Loss: 0.6329102663557333
Span End Loss: 0.6091003169296114
Type Loss: 0.2878898842652275
 ------------ Epoch 6/10 Batch 9600/11616 Training Results ------------ 
Total Loss: 1.530316052553826
Span Start Loss: 0.6330921822966775
Span End Loss: 0.6093385768174631
Type Loss: 0.28788526717107743
 ------------ Epoch 6/10 Batch 9650/11616 Training Results ------------ 
Total Loss: 1.5306103434423322
Span Start Loss: 0.6335008925199509
Span End Loss: 0.6092789600099741
Type Loss: 0.28783046478395946
 ------------ Epoch 6/10 Batch 9700/11616 Training Results ------------ 
Total Loss: 1.5322218567929855
Span Start Loss: 0.6345131255179336
Span End Loss: 0.6097045420906411
Type Loss: 0.2880041631426393
 ------------ Epoch 6/10 Batch 9750/11616 Training Results ------------ 
Total Loss: 1.532070846556662
Span Start Loss: 0.6345001190243623
Span End Loss: 0.6096849719042627
Type Loss: 0.2878857295436737
 ------------ Epoch 6/10 Batch 9800/11616 Training Results ------------ 
Total Loss: 1.5327853391583706
Span Start Loss: 0.6346903727119979
Span End Loss: 0.610301999236975
Type Loss: 0.28779294113875653
 ------------ Epoch 6/10 Batch 9850/11616 Training Results ------------ 
Total Loss: 1.5337416345764159
Span Start Loss: 0.6353290390824606
Span End Loss: 0.6104449828166921
Type Loss: 0.28796758647554266
 ------------ Epoch 6/10 Batch 9900/11616 Training Results ------------ 
Total Loss: 1.533514533614307
Span Start Loss: 0.6349782545394217
Span End Loss: 0.6104637507181098
Type Loss: 0.2880725022972646
 ------------ Epoch 6/10 Batch 9950/11616 Training Results ------------ 
Total Loss: 1.5345797019247809
Span Start Loss: 0.6353011297621769
Span End Loss: 0.6110308840636155
Type Loss: 0.2882476621082792
 ------------ Epoch 6/10 Batch 10000/11616 Training Results ------------ 
Total Loss: 1.5351162903094664
Span Start Loss: 0.6357839262273163
Span End Loss: 0.6113504338962259
Type Loss: 0.28798190418211744
 --------------- Epoch 6/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 57.1, 'f1': 68.9, 'turns': 1425}), ('literature', {'em': 53.0, 'f1': 64.3, 'turns': 1630}), ('mid-high_school', {'em': 54.5, 'f1': 65.8, 'turns': 1653}), ('news', {'em': 57.6, 'f1': 69.3, 'turns': 1649}), ('wikipedia', {'em': 59.7, 'f1': 71.9, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 56.4, 'f1': 68.0, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 56.4, 'f1': 68.0, 'turns': 7983})])
 ------------ Epoch 6/10 Batch 10050/11616 Training Results ------------ 
Total Loss: 1.5355654999882502
Span Start Loss: 0.6361276091275672
Span End Loss: 0.6115471717579272
Type Loss: 0.28789069306967197
 ------------ Epoch 6/10 Batch 10100/11616 Training Results ------------ 
Total Loss: 1.5357825258368683
Span Start Loss: 0.6362893703406548
Span End Loss: 0.6118271096763754
Type Loss: 0.28766601980940465
 ------------ Epoch 6/10 Batch 10150/11616 Training Results ------------ 
Total Loss: 1.5359629541791497
Span Start Loss: 0.6362238785180405
Span End Loss: 0.6119270699637542
Type Loss: 0.28781197970439426
 ------------ Epoch 6/10 Batch 10200/11616 Training Results ------------ 
Total Loss: 1.5368524831308819
Span Start Loss: 0.636711324469789
Span End Loss: 0.6123164288591201
Type Loss: 0.28782470368332796
 ------------ Epoch 6/10 Batch 10250/11616 Training Results ------------ 
Total Loss: 1.5368313381848175
Span Start Loss: 0.636783442967912
Span End Loss: 0.6125632547388399
Type Loss: 0.28748461441795636
 ------------ Epoch 6/10 Batch 10300/11616 Training Results ------------ 
Total Loss: 1.5370442125268136
Span Start Loss: 0.6368241481775943
Span End Loss: 0.6124653290654533
Type Loss: 0.2877547091812583
 ------------ Epoch 6/10 Batch 10350/11616 Training Results ------------ 
Total Loss: 1.5376845640269814
Span Start Loss: 0.6371563869282819
Span End Loss: 0.6128239300243493
Type Loss: 0.28770422092229486
 ------------ Epoch 6/10 Batch 10400/11616 Training Results ------------ 
Total Loss: 1.5391995203654425
Span Start Loss: 0.637929681035905
Span End Loss: 0.6135149156747502
Type Loss: 0.28775489747121286
 ------------ Epoch 6/10 Batch 10450/11616 Training Results ------------ 
Total Loss: 1.5403479913962586
Span Start Loss: 0.6384010265161546
Span End Loss: 0.6139125159202126
Type Loss: 0.2880344227240666
 ------------ Epoch 6/10 Batch 10500/11616 Training Results ------------ 
Total Loss: 1.5408965079708115
Span Start Loss: 0.6386638436338731
Span End Loss: 0.6140787398245052
Type Loss: 0.2881538982716877
 ------------ Epoch 6/10 Batch 10550/11616 Training Results ------------ 
Total Loss: 1.5415303749787088
Span Start Loss: 0.6387387909383571
Span End Loss: 0.6144937588487447
Type Loss: 0.2882977989449285
 ------------ Epoch 6/10 Batch 10600/11616 Training Results ------------ 
Total Loss: 1.5419243324591936
Span Start Loss: 0.6391011496858214
Span End Loss: 0.6146015477216723
Type Loss: 0.28822160881179615
 ------------ Epoch 6/10 Batch 10650/11616 Training Results ------------ 
Total Loss: 1.5425747058979573
Span Start Loss: 0.6395120977550884
Span End Loss: 0.6147228071521399
Type Loss: 0.28833977463335986
 ------------ Epoch 6/10 Batch 10700/11616 Training Results ------------ 
Total Loss: 1.5434573260849673
Span Start Loss: 0.6399060209770905
Span End Loss: 0.6152286651024865
Type Loss: 0.28832261364724243
 ------------ Epoch 6/10 Batch 10750/11616 Training Results ------------ 
Total Loss: 1.5446074857242231
Span Start Loss: 0.640851342699902
Span End Loss: 0.6156459477479072
Type Loss: 0.288110168859276
 ------------ Epoch 6/10 Batch 10800/11616 Training Results ------------ 
Total Loss: 1.5450815124756276
Span Start Loss: 0.6409376332891622
Span End Loss: 0.6159455385318897
Type Loss: 0.2881983142526372
 ------------ Epoch 6/10 Batch 10850/11616 Training Results ------------ 
Total Loss: 1.5453456491583846
Span Start Loss: 0.6408465665087859
Span End Loss: 0.616446625119706
Type Loss: 0.28805243114112505
 ------------ Epoch 6/10 Batch 10900/11616 Training Results ------------ 
Total Loss: 1.5454641455331595
Span Start Loss: 0.6406825895860381
Span End Loss: 0.6164706544208324
Type Loss: 0.2883108750986226
 ------------ Epoch 6/10 Batch 10950/11616 Training Results ------------ 
Total Loss: 1.5457696314739515
Span Start Loss: 0.6408974601733222
Span End Loss: 0.6166714671699194
Type Loss: 0.2882006776903556
 ------------ Epoch 6/10 Batch 11000/11616 Training Results ------------ 
Total Loss: 1.5453017004164444
Span Start Loss: 0.6405915020540018
Span End Loss: 0.6164530905460812
Type Loss: 0.2882570814166049
 ------------ Epoch 6/10 Batch 11050/11616 Training Results ------------ 
Total Loss: 1.5448386195010393
Span Start Loss: 0.6402728953389493
Span End Loss: 0.6164030161455518
Type Loss: 0.2881626815845288
 ------------ Epoch 6/10 Batch 11100/11616 Training Results ------------ 
Total Loss: 1.5453782633441995
Span Start Loss: 0.6404164302481657
Span End Loss: 0.6165297349555174
Type Loss: 0.2884320717824304
 ------------ Epoch 6/10 Batch 11150/11616 Training Results ------------ 
Total Loss: 1.5466462288127019
Span Start Loss: 0.6411549588595203
Span End Loss: 0.6170920445004294
Type Loss: 0.2883991990498196
 ------------ Epoch 6/10 Batch 11200/11616 Training Results ------------ 
Total Loss: 1.5470883225681193
Span Start Loss: 0.6413889435021806
Span End Loss: 0.617300893696894
Type Loss: 0.2883984588599664
 ------------ Epoch 6/10 Batch 11250/11616 Training Results ------------ 
Total Loss: 1.5478885371213158
Span Start Loss: 0.641975493733916
Span End Loss: 0.6174773254764784
Type Loss: 0.28843569130889246
 ------------ Epoch 6/10 Batch 11300/11616 Training Results ------------ 
Total Loss: 1.5481074896855362
Span Start Loss: 0.6420466730016721
Span End Loss: 0.6177159190114662
Type Loss: 0.2883448710351095
 ------------ Epoch 6/10 Batch 11350/11616 Training Results ------------ 
Total Loss: 1.5486269631763583
Span Start Loss: 0.6424381511472986
Span End Loss: 0.6179730753667267
Type Loss: 0.28821571008117647
 ------------ Epoch 6/10 Batch 11400/11616 Training Results ------------ 
Total Loss: 1.5483620158829645
Span Start Loss: 0.6424656682025249
Span End Loss: 0.6178893578609068
Type Loss: 0.2880069632368293
 ------------ Epoch 6/10 Batch 11450/11616 Training Results ------------ 
Total Loss: 1.5487236050758755
Span Start Loss: 0.6424755333124875
Span End Loss: 0.6182066745302278
Type Loss: 0.28804137059605345
 ------------ Epoch 6/10 Batch 11500/11616 Training Results ------------ 
Total Loss: 1.5497386524267496
Span Start Loss: 0.6427614959347183
Span End Loss: 0.6187932241676659
Type Loss: 0.2881839057146369
 ------------ Epoch 6/10 Batch 11550/11616 Training Results ------------ 
Total Loss: 1.5495255304774616
Span Start Loss: 0.6425246017543979
Span End Loss: 0.618926239473709
Type Loss: 0.2880746627016017
 ------------ Epoch 6/10 Batch 11600/11616 Training Results ------------ 
Total Loss: 1.5501968414723424
Span Start Loss: 0.6428513616777892
Span End Loss: 0.6192534597964098
Type Loss: 0.2880919934803588
 --------------------- Epoch 6/10 Final Training Results ------------------------ 
Total Loss: 1.5508651917522258
Span Start Loss: 0.643088839016053
Span End Loss: 0.6194977794393883
Type Loss: 0.2882785467090523
 --------------- Epoch 6/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 54.5, 'f1': 67.2, 'turns': 1425}), ('literature', {'em': 49.4, 'f1': 61.5, 'turns': 1630}), ('mid-high_school', {'em': 50.7, 'f1': 63.2, 'turns': 1653}), ('news', {'em': 52.8, 'f1': 65.3, 'turns': 1649}), ('wikipedia', {'em': 56.3, 'f1': 69.6, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 52.7, 'f1': 65.3, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 52.7, 'f1': 65.3, 'turns': 7983})])
 --------------- Epoch 7/10 Training Start --------------- 
 ------------ Epoch 7/10 Batch 50/11616 Training Results ------------ 
Total Loss: 1.263243391364813
Span Start Loss: 0.46901793718338014
Span End Loss: 0.4928877188265324
Type Loss: 0.3013377198949456
 ------------ Epoch 7/10 Batch 100/11616 Training Results ------------ 
Total Loss: 1.302716726064682
Span Start Loss: 0.47210128121078015
Span End Loss: 0.5241069699078799
Type Loss: 0.3065084567945451
 ------------ Epoch 7/10 Batch 150/11616 Training Results ------------ 
Total Loss: 1.2459763988852501
Span Start Loss: 0.44137462340295314
Span End Loss: 0.501568443675836
Type Loss: 0.30303331394369404
 ------------ Epoch 7/10 Batch 200/11616 Training Results ------------ 
Total Loss: 1.2267899956554174
Span Start Loss: 0.4495855417661369
Span End Loss: 0.4893759333714843
Type Loss: 0.28782850395422427
 ------------ Epoch 7/10 Batch 250/11616 Training Results ------------ 
Total Loss: 1.2532186533510685
Span Start Loss: 0.4750423591285944
Span End Loss: 0.4870807620808482
Type Loss: 0.29109551637992265
 ------------ Epoch 7/10 Batch 300/11616 Training Results ------------ 
Total Loss: 1.2204410535593828
Span Start Loss: 0.4605601309550305
Span End Loss: 0.4759016539963583
Type Loss: 0.2839792531573524
 ------------ Epoch 7/10 Batch 350/11616 Training Results ------------ 
Total Loss: 1.2067468135484627
Span Start Loss: 0.4508943207721625
Span End Loss: 0.47533485697316274
Type Loss: 0.28051762023940685
 ------------ Epoch 7/10 Batch 400/11616 Training Results ------------ 
Total Loss: 1.185246585076675
Span Start Loss: 0.44146615726174787
Span End Loss: 0.4607166653731838
Type Loss: 0.2830637455102988
 ------------ Epoch 7/10 Batch 450/11616 Training Results ------------ 
Total Loss: 1.1757699614597692
Span Start Loss: 0.4380478872131142
Span End Loss: 0.45875019155028796
Type Loss: 0.2789718665368855
 ------------ Epoch 7/10 Batch 500/11616 Training Results ------------ 
Total Loss: 1.1509046494960784
Span Start Loss: 0.4289210571944714
Span End Loss: 0.4474468849403784
Type Loss: 0.27453669082187115
 ------------ Epoch 7/10 Batch 550/11616 Training Results ------------ 
Total Loss: 1.1459083923562006
Span Start Loss: 0.42803001978180627
Span End Loss: 0.44501380065202034
Type Loss: 0.27286455529487946
 ------------ Epoch 7/10 Batch 600/11616 Training Results ------------ 
Total Loss: 1.142822668297837
Span Start Loss: 0.42811139883163074
Span End Loss: 0.4400197813559013
Type Loss: 0.27469147128829113
 ------------ Epoch 7/10 Batch 650/11616 Training Results ------------ 
Total Loss: 1.144414838947929
Span Start Loss: 0.4258552686755474
Span End Loss: 0.4418091581468112
Type Loss: 0.2767503948079852
 ------------ Epoch 7/10 Batch 700/11616 Training Results ------------ 
Total Loss: 1.1470341526504073
Span Start Loss: 0.4318018873514874
Span End Loss: 0.43910427538877617
Type Loss: 0.2761279723527176
 ------------ Epoch 7/10 Batch 750/11616 Training Results ------------ 
Total Loss: 1.1501631042857965
Span Start Loss: 0.43459259282052515
Span End Loss: 0.4411313290912658
Type Loss: 0.27443916460623347
 ------------ Epoch 7/10 Batch 800/11616 Training Results ------------ 
Total Loss: 1.1509185184258968
Span Start Loss: 0.4377727218950167
Span End Loss: 0.43991230861924124
Type Loss: 0.2732334691635333
 ------------ Epoch 7/10 Batch 850/11616 Training Results ------------ 
Total Loss: 1.1573358519112362
Span Start Loss: 0.4389485281749683
Span End Loss: 0.4452959749557297
Type Loss: 0.273091329750331
 ------------ Epoch 7/10 Batch 900/11616 Training Results ------------ 
Total Loss: 1.1492978108632896
Span Start Loss: 0.43558835704293514
Span End Loss: 0.4409263168477143
Type Loss: 0.27278311831048796
 ------------ Epoch 7/10 Batch 950/11616 Training Results ------------ 
Total Loss: 1.1422644521845013
Span Start Loss: 0.4324320785701275
Span End Loss: 0.4368564230002659
Type Loss: 0.27297593136367043
 ------------ Epoch 7/10 Batch 1000/11616 Training Results ------------ 
Total Loss: 1.1323038315400482
Span Start Loss: 0.4252695035543293
Span End Loss: 0.4334363979441114
Type Loss: 0.27359791059140115
 ------------ Epoch 7/10 Batch 1050/11616 Training Results ------------ 
Total Loss: 1.131556672265842
Span Start Loss: 0.42402091206184456
Span End Loss: 0.43443698534608954
Type Loss: 0.2730987556535928
 ------------ Epoch 7/10 Batch 1100/11616 Training Results ------------ 
Total Loss: 1.1224270031872121
Span Start Loss: 0.41810155182145536
Span End Loss: 0.4305785113560375
Type Loss: 0.27374692085707053
 ------------ Epoch 7/10 Batch 1150/11616 Training Results ------------ 
Total Loss: 1.1178558576042237
Span Start Loss: 0.4157305591090825
Span End Loss: 0.4269724985556272
Type Loss: 0.275152780626617
 ------------ Epoch 7/10 Batch 1200/11616 Training Results ------------ 
Total Loss: 1.1224949735216796
Span Start Loss: 0.41673153340573965
Span End Loss: 0.4301519323873799
Type Loss: 0.27561148833405846
 ------------ Epoch 7/10 Batch 1250/11616 Training Results ------------ 
Total Loss: 1.1191893563121558
Span Start Loss: 0.4145814529790543
Span End Loss: 0.42808343988396225
Type Loss: 0.27652444363459944
 ------------ Epoch 7/10 Batch 1300/11616 Training Results ------------ 
Total Loss: 1.117960218196878
Span Start Loss: 0.4131074106076267
Span End Loss: 0.4296858960458149
Type Loss: 0.2751668917710105
 ------------ Epoch 7/10 Batch 1350/11616 Training Results ------------ 
Total Loss: 1.119386210485741
Span Start Loss: 0.4137924564539248
Span End Loss: 0.4307474939507881
Type Loss: 0.2748462408098082
 ------------ Epoch 7/10 Batch 1400/11616 Training Results ------------ 
Total Loss: 1.1154134939025555
Span Start Loss: 0.41173436622991827
Span End Loss: 0.4292665024311282
Type Loss: 0.2744126060185954
 ------------ Epoch 7/10 Batch 1450/11616 Training Results ------------ 
Total Loss: 1.1141216831313896
Span Start Loss: 0.4106915574921456
Span End Loss: 0.4291254335741416
Type Loss: 0.27430467280655585
 ------------ Epoch 7/10 Batch 1500/11616 Training Results ------------ 
Total Loss: 1.112490099032099
Span Start Loss: 0.4105961871757948
Span End Loss: 0.4277869815339024
Type Loss: 0.2741069119268407
 ------------ Epoch 7/10 Batch 1550/11616 Training Results ------------ 
Total Loss: 1.1109653141680023
Span Start Loss: 0.4093803569400353
Span End Loss: 0.42708299358856056
Type Loss: 0.2745019454070397
 ------------ Epoch 7/10 Batch 1600/11616 Training Results ------------ 
Total Loss: 1.1150837515335297
Span Start Loss: 0.4118563554473076
Span End Loss: 0.42869628072570776
Type Loss: 0.2745310969982529
 ------------ Epoch 7/10 Batch 1650/11616 Training Results ------------ 
Total Loss: 1.1155102413909679
Span Start Loss: 0.41231757911112665
Span End Loss: 0.4281606708836714
Type Loss: 0.27503197294688136
 ------------ Epoch 7/10 Batch 1700/11616 Training Results ------------ 
Total Loss: 1.1167883696554997
Span Start Loss: 0.4142799553490819
Span End Loss: 0.42771249581374887
Type Loss: 0.274795899627292
 ------------ Epoch 7/10 Batch 1750/11616 Training Results ------------ 
Total Loss: 1.1162907438219658
Span Start Loss: 0.41490084698656576
Span End Loss: 0.4275743562214609
Type Loss: 0.27381552151856675
 ------------ Epoch 7/10 Batch 1800/11616 Training Results ------------ 
Total Loss: 1.119833701220341
Span Start Loss: 0.41680142347979
Span End Loss: 0.42847923470842136
Type Loss: 0.27455302380594526
 ------------ Epoch 7/10 Batch 1850/11616 Training Results ------------ 
Total Loss: 1.1195237029570382
Span Start Loss: 0.4160408129207664
Span End Loss: 0.42973099576435175
Type Loss: 0.2737518746582036
 ------------ Epoch 7/10 Batch 1900/11616 Training Results ------------ 
Total Loss: 1.1170420839230677
Span Start Loss: 0.41501668586414064
Span End Loss: 0.42932771209210746
Type Loss: 0.2726976665153511
 ------------ Epoch 7/10 Batch 1950/11616 Training Results ------------ 
Total Loss: 1.1178670360229146
Span Start Loss: 0.41535256566211154
Span End Loss: 0.4299791613218781
Type Loss: 0.27253528971845903
 ------------ Epoch 7/10 Batch 2000/11616 Training Results ------------ 
Total Loss: 1.1173109863628634
Span Start Loss: 0.415601066189527
Span End Loss: 0.4303172427683603
Type Loss: 0.27139265835424886
 --------------- Epoch 7/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 54.2, 'f1': 67.7, 'turns': 1425}), ('literature', {'em': 49.9, 'f1': 62.6, 'turns': 1630}), ('mid-high_school', {'em': 51.7, 'f1': 65.0, 'turns': 1653}), ('news', {'em': 52.7, 'f1': 65.2, 'turns': 1649}), ('wikipedia', {'em': 57.8, 'f1': 71.4, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 53.2, 'f1': 66.4, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 53.2, 'f1': 66.4, 'turns': 7983})])
 ------------ Epoch 7/10 Batch 2050/11616 Training Results ------------ 
Total Loss: 1.1178017101659463
Span Start Loss: 0.41587332326527005
Span End Loss: 0.43140337369041293
Type Loss: 0.2705249943325251
 ------------ Epoch 7/10 Batch 2100/11616 Training Results ------------ 
Total Loss: 1.117465970321514
Span Start Loss: 0.415182483156511
Span End Loss: 0.4318032379907423
Type Loss: 0.2704802303144797
 ------------ Epoch 7/10 Batch 2150/11616 Training Results ------------ 
Total Loss: 1.1173193234971963
Span Start Loss: 0.4160019398733519
Span End Loss: 0.43103758465043857
Type Loss: 0.2702797800930607
 ------------ Epoch 7/10 Batch 2200/11616 Training Results ------------ 
Total Loss: 1.118410570532168
Span Start Loss: 0.4171517209380082
Span End Loss: 0.4310557640801099
Type Loss: 0.27020306694956325
 ------------ Epoch 7/10 Batch 2250/11616 Training Results ------------ 
Total Loss: 1.1219535151558617
Span Start Loss: 0.41816058165181635
Span End Loss: 0.43360506069825755
Type Loss: 0.2701878542610341
 ------------ Epoch 7/10 Batch 2300/11616 Training Results ------------ 
Total Loss: 1.12438615010039
Span Start Loss: 0.41990778747718255
Span End Loss: 0.4337258431339717
Type Loss: 0.27075250096214204
 ------------ Epoch 7/10 Batch 2350/11616 Training Results ------------ 
Total Loss: 1.1282700503462966
Span Start Loss: 0.42145866885716887
Span End Loss: 0.43569298991339006
Type Loss: 0.27111837292605256
 ------------ Epoch 7/10 Batch 2400/11616 Training Results ------------ 
Total Loss: 1.1267555239164115
Span Start Loss: 0.42157632780479615
Span End Loss: 0.433803840756882
Type Loss: 0.2713753367805233
 ------------ Epoch 7/10 Batch 2450/11616 Training Results ------------ 
Total Loss: 1.1276066334290924
Span Start Loss: 0.42177868901984766
Span End Loss: 0.4347679861589354
Type Loss: 0.2710599395252612
 ------------ Epoch 7/10 Batch 2500/11616 Training Results ------------ 
Total Loss: 1.1285080199483781
Span Start Loss: 0.42279354870677927
Span End Loss: 0.434978042152524
Type Loss: 0.2707364104315639
 ------------ Epoch 7/10 Batch 2550/11616 Training Results ------------ 
Total Loss: 1.1270425324949125
Span Start Loss: 0.4216435445482185
Span End Loss: 0.43493114537029876
Type Loss: 0.27046782390378854
 ------------ Epoch 7/10 Batch 2600/11616 Training Results ------------ 
Total Loss: 1.1295432315297569
Span Start Loss: 0.4220013801419051
Span End Loss: 0.4372252948173823
Type Loss: 0.27031653774400743
 ------------ Epoch 7/10 Batch 2650/11616 Training Results ------------ 
Total Loss: 1.1314846771944946
Span Start Loss: 0.42241834932151867
Span End Loss: 0.43837441477502853
Type Loss: 0.27069189413679096
 ------------ Epoch 7/10 Batch 2700/11616 Training Results ------------ 
Total Loss: 1.1321336385500789
Span Start Loss: 0.4213535598508962
Span End Loss: 0.4393842761632469
Type Loss: 0.2713957836872174
 ------------ Epoch 7/10 Batch 2750/11616 Training Results ------------ 
Total Loss: 1.130980774548582
Span Start Loss: 0.42095634936833415
Span End Loss: 0.438851436377249
Type Loss: 0.2711729700558565
 ------------ Epoch 7/10 Batch 2800/11616 Training Results ------------ 
Total Loss: 1.1303027363344362
Span Start Loss: 0.4204729422127379
Span End Loss: 0.4381215657047661
Type Loss: 0.2717082096473314
 ------------ Epoch 7/10 Batch 2850/11616 Training Results ------------ 
Total Loss: 1.1303603501941422
Span Start Loss: 0.42129682749562913
Span End Loss: 0.4377942292503359
Type Loss: 0.27126927447194854
 ------------ Epoch 7/10 Batch 2900/11616 Training Results ------------ 
Total Loss: 1.1314927652141016
Span Start Loss: 0.4223331589716214
Span End Loss: 0.43827034451003216
Type Loss: 0.2708892427372008
 ------------ Epoch 7/10 Batch 2950/11616 Training Results ------------ 
Total Loss: 1.1324752784997099
Span Start Loss: 0.422424931009892
Span End Loss: 0.4388529026451505
Type Loss: 0.2711974258253635
 ------------ Epoch 7/10 Batch 3000/11616 Training Results ------------ 
Total Loss: 1.1324844154451663
Span Start Loss: 0.42194778919425635
Span End Loss: 0.4388828168101609
Type Loss: 0.2716537904546907
 ------------ Epoch 7/10 Batch 3050/11616 Training Results ------------ 
Total Loss: 1.1349875336589261
Span Start Loss: 0.4222118414018196
Span End Loss: 0.4402775615252188
Type Loss: 0.2724981116039343
 ------------ Epoch 7/10 Batch 3100/11616 Training Results ------------ 
Total Loss: 1.1337426165824815
Span Start Loss: 0.42116371047131024
Span End Loss: 0.44056564668253545
Type Loss: 0.27201324018619716
 ------------ Epoch 7/10 Batch 3150/11616 Training Results ------------ 
Total Loss: 1.136026404926642
Span Start Loss: 0.42165662588244895
Span End Loss: 0.4413712814485743
Type Loss: 0.27299847840671504
 ------------ Epoch 7/10 Batch 3200/11616 Training Results ------------ 
Total Loss: 1.1393788519126247
Span Start Loss: 0.42346340077598144
Span End Loss: 0.4424312321026809
Type Loss: 0.27348419979098254
 ------------ Epoch 7/10 Batch 3250/11616 Training Results ------------ 
Total Loss: 1.1404654886673848
Span Start Loss: 0.42356218639893745
Span End Loss: 0.4426194427254108
Type Loss: 0.27428384034880077
 ------------ Epoch 7/10 Batch 3300/11616 Training Results ------------ 
Total Loss: 1.1405574134452212
Span Start Loss: 0.424256205311165
Span End Loss: 0.44247928432781586
Type Loss: 0.2738219047086596
 ------------ Epoch 7/10 Batch 3350/11616 Training Results ------------ 
Total Loss: 1.1405673706417543
Span Start Loss: 0.42433308286859944
Span End Loss: 0.44287790087732803
Type Loss: 0.27335636776302064
 ------------ Epoch 7/10 Batch 3400/11616 Training Results ------------ 
Total Loss: 1.1407652299563564
Span Start Loss: 0.42456092873660106
Span End Loss: 0.4430425106265637
Type Loss: 0.2731617713004679
 ------------ Epoch 7/10 Batch 3450/11616 Training Results ------------ 
Total Loss: 1.1427039856752517
Span Start Loss: 0.4250141626069039
Span End Loss: 0.4437307971103144
Type Loss: 0.2739590066695667
 ------------ Epoch 7/10 Batch 3500/11616 Training Results ------------ 
Total Loss: 1.143174227421571
Span Start Loss: 0.4250528441488277
Span End Loss: 0.4438701308010412
Type Loss: 0.274251233195886
 ------------ Epoch 7/10 Batch 3550/11616 Training Results ------------ 
Total Loss: 1.1430208466304335
Span Start Loss: 0.42487595880394274
Span End Loss: 0.4440333830924626
Type Loss: 0.27411148542223474
 ------------ Epoch 7/10 Batch 3600/11616 Training Results ------------ 
Total Loss: 1.1435177227779705
Span Start Loss: 0.42592414051063116
Span End Loss: 0.44368374990402826
Type Loss: 0.27390981279210086
 ------------ Epoch 7/10 Batch 3650/11616 Training Results ------------ 
Total Loss: 1.1428416304991975
Span Start Loss: 0.4251600983363978
Span End Loss: 0.4439286975514093
Type Loss: 0.27375281513808936
 ------------ Epoch 7/10 Batch 3700/11616 Training Results ------------ 
Total Loss: 1.1461032307870385
Span Start Loss: 0.4271899762389886
Span End Loss: 0.4447713445174835
Type Loss: 0.2741418905123263
 ------------ Epoch 7/10 Batch 3750/11616 Training Results ------------ 
Total Loss: 1.1438892357684671
Span Start Loss: 0.42692257208577356
Span End Loss: 0.44296315501953165
Type Loss: 0.2740034891585509
 ------------ Epoch 7/10 Batch 3800/11616 Training Results ------------ 
Total Loss: 1.1415280624010944
Span Start Loss: 0.4260154694779684
Span End Loss: 0.44148388203846195
Type Loss: 0.27402869141611613
 ------------ Epoch 7/10 Batch 3850/11616 Training Results ------------ 
Total Loss: 1.1418333300099066
Span Start Loss: 0.4267076841316204
Span End Loss: 0.4416896229612266
Type Loss: 0.2734360033205964
 ------------ Epoch 7/10 Batch 3900/11616 Training Results ------------ 
Total Loss: 1.142450895964001
Span Start Loss: 0.42757005337483955
Span End Loss: 0.44160580386741993
Type Loss: 0.273275019000165
 ------------ Epoch 7/10 Batch 3950/11616 Training Results ------------ 
Total Loss: 1.143239517822343
Span Start Loss: 0.4276040898209373
Span End Loss: 0.4418061955332096
Type Loss: 0.27382921274421335
 ------------ Epoch 7/10 Batch 4000/11616 Training Results ------------ 
Total Loss: 1.1428158158634323
Span Start Loss: 0.42721990200436266
Span End Loss: 0.4419385691958014
Type Loss: 0.27365732490527445
 --------------- Epoch 7/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 55.2, 'f1': 67.5, 'turns': 1425}), ('literature', {'em': 55.0, 'f1': 66.1, 'turns': 1630}), ('mid-high_school', {'em': 53.4, 'f1': 65.5, 'turns': 1653}), ('news', {'em': 55.5, 'f1': 67.7, 'turns': 1649}), ('wikipedia', {'em': 59.4, 'f1': 72.2, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 55.7, 'f1': 67.8, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 55.7, 'f1': 67.8, 'turns': 7983})])
 ------------ Epoch 7/10 Batch 4050/11616 Training Results ------------ 
Total Loss: 1.1436416981776278
Span Start Loss: 0.42819148927477163
Span End Loss: 0.4425595777042578
Type Loss: 0.27289061145351073
 ------------ Epoch 7/10 Batch 4100/11616 Training Results ------------ 
Total Loss: 1.1445600148074602
Span Start Loss: 0.4282750351082891
Span End Loss: 0.44350166780442546
Type Loss: 0.27278329216647984
 ------------ Epoch 7/10 Batch 4150/11616 Training Results ------------ 
Total Loss: 1.146069977500936
Span Start Loss: 0.429480064956188
Span End Loss: 0.4436449710694871
Type Loss: 0.2729449216687087
 ------------ Epoch 7/10 Batch 4200/11616 Training Results ------------ 
Total Loss: 1.148148879409945
Span Start Loss: 0.43021049974185493
Span End Loss: 0.4442778227853012
Type Loss: 0.2736605370610154
 ------------ Epoch 7/10 Batch 4250/11616 Training Results ------------ 
Total Loss: 1.150730842725538
Span Start Loss: 0.43139553789015145
Span End Loss: 0.44502915249217084
Type Loss: 0.27430613252060376
 ------------ Epoch 7/10 Batch 4300/11616 Training Results ------------ 
Total Loss: 1.1496551717100882
Span Start Loss: 0.43153654955683524
Span End Loss: 0.44409408415017954
Type Loss: 0.27402451817661005
 ------------ Epoch 7/10 Batch 4350/11616 Training Results ------------ 
Total Loss: 1.153646423129215
Span Start Loss: 0.4333931145518242
Span End Loss: 0.4459828829184046
Type Loss: 0.2742704055460448
 ------------ Epoch 7/10 Batch 4400/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27447032708695834
 ------------ Epoch 7/10 Batch 4450/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27517541028412706
 ------------ Epoch 7/10 Batch 4500/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27477456490219465
 ------------ Epoch 7/10 Batch 4550/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2743780889529939
 ------------ Epoch 7/10 Batch 4600/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27477923124907133
 ------------ Epoch 7/10 Batch 4650/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27448582290201096
 ------------ Epoch 7/10 Batch 4700/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27439084915285733
 ------------ Epoch 7/10 Batch 4750/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2744986048299623
 ------------ Epoch 7/10 Batch 4800/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2742442561975137
 ------------ Epoch 7/10 Batch 4850/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2741124390578378
 ------------ Epoch 7/10 Batch 4900/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27441984902193045
 ------------ Epoch 7/10 Batch 4950/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27412434603849595
 ------------ Epoch 7/10 Batch 5000/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2741789391549304
 ------------ Epoch 7/10 Batch 5050/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2741486694429549
 ------------ Epoch 7/10 Batch 5100/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2741665460163837
 ------------ Epoch 7/10 Batch 5150/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.274029984082665
 ------------ Epoch 7/10 Batch 5200/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27405223907580456
 ------------ Epoch 7/10 Batch 5250/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2738864765898103
 ------------ Epoch 7/10 Batch 5300/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2743316149671193
 ------------ Epoch 7/10 Batch 5350/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27445007899253865
 ------------ Epoch 7/10 Batch 5400/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27432035450254466
 ------------ Epoch 7/10 Batch 5450/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27450873968951883
 ------------ Epoch 7/10 Batch 5500/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27448667482523753
 ------------ Epoch 7/10 Batch 5550/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.274120543018856
 ------------ Epoch 7/10 Batch 5600/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2740785109002276
 ------------ Epoch 7/10 Batch 5650/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27393535604519126
 ------------ Epoch 7/10 Batch 5700/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27363201325399833
 ------------ Epoch 7/10 Batch 5750/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27396909714684536
 ------------ Epoch 7/10 Batch 5800/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27377847048518217
 ------------ Epoch 7/10 Batch 5850/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27373158689269905
 ------------ Epoch 7/10 Batch 5900/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27432007204362396
 ------------ Epoch 7/10 Batch 5950/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2744109834386271
 ------------ Epoch 7/10 Batch 6000/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27467981026166427
 --------------- Epoch 7/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 56.9, 'f1': 68.9, 'turns': 1425}), ('literature', {'em': 51.7, 'f1': 62.8, 'turns': 1630}), ('mid-high_school', {'em': 53.7, 'f1': 65.9, 'turns': 1653}), ('news', {'em': 57.0, 'f1': 68.5, 'turns': 1649}), ('wikipedia', {'em': 60.5, 'f1': 72.7, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 55.9, 'f1': 67.7, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 55.9, 'f1': 67.7, 'turns': 7983})])
 ------------ Epoch 7/10 Batch 6050/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2748137456720526
 ------------ Epoch 7/10 Batch 6100/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2746145388606142
 ------------ Epoch 7/10 Batch 6150/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27443913239031664
 ------------ Epoch 7/10 Batch 6200/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2744882346826395
 ------------ Epoch 7/10 Batch 6250/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2744281479997933
 ------------ Epoch 7/10 Batch 6300/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.274602427052127
 ------------ Epoch 7/10 Batch 6350/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27445757272909943
 ------------ Epoch 7/10 Batch 6400/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27439626785169824
 ------------ Epoch 7/10 Batch 6450/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27421887643144344
 ------------ Epoch 7/10 Batch 6500/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27399206389429476
 ------------ Epoch 7/10 Batch 6550/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27371119835506413
 ------------ Epoch 7/10 Batch 6600/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2734065347289046
 ------------ Epoch 7/10 Batch 6650/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27383251408885295
 ------------ Epoch 7/10 Batch 6700/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2736057595074622
 ------------ Epoch 7/10 Batch 6750/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27350374049169046
 ------------ Epoch 7/10 Batch 6800/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27362686509841727
 ------------ Epoch 7/10 Batch 6850/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2736003023235087
 ------------ Epoch 7/10 Batch 6900/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2731398291679342
 ------------ Epoch 7/10 Batch 6950/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2728309802463104
 ------------ Epoch 7/10 Batch 7000/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2725687322977132
 ------------ Epoch 7/10 Batch 7050/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27239866471816354
 ------------ Epoch 7/10 Batch 7100/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2726298083505318
 ------------ Epoch 7/10 Batch 7150/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2724638320889871
 ------------ Epoch 7/10 Batch 7200/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2723201163924144
 ------------ Epoch 7/10 Batch 7250/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2721514390595257
 ------------ Epoch 7/10 Batch 7300/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27198337962404406
 ------------ Epoch 7/10 Batch 7350/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27185594125834556
 ------------ Epoch 7/10 Batch 7400/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2719531934908465
 ------------ Epoch 7/10 Batch 7450/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2721836186531656
 ------------ Epoch 7/10 Batch 7500/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27222619094016653
 ------------ Epoch 7/10 Batch 7550/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2721167339733648
 ------------ Epoch 7/10 Batch 7600/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2722754825586698
 ------------ Epoch 7/10 Batch 7650/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.272306933351762
 ------------ Epoch 7/10 Batch 7700/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27239771054150513
 ------------ Epoch 7/10 Batch 7750/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2724159238217098
 ------------ Epoch 7/10 Batch 7800/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2721734194827672
 ------------ Epoch 7/10 Batch 7850/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27254555327117824
 ------------ Epoch 7/10 Batch 7900/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27285251557968465
 ------------ Epoch 7/10 Batch 7950/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27316879858245263
 ------------ Epoch 7/10 Batch 8000/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27351516341185195
 --------------- Epoch 7/10 Validation Start --------------- 
Results: --------------- 
OrderedDict([('children_stories', {'em': 56.8, 'f1': 68.8, 'turns': 1425}), ('literature', {'em': 51.6, 'f1': 63.4, 'turns': 1630}), ('mid-high_school', {'em': 53.8, 'f1': 65.9, 'turns': 1653}), ('news', {'em': 55.8, 'f1': 67.9, 'turns': 1649}), ('wikipedia', {'em': 60.0, 'f1': 72.6, 'turns': 1626}), ('reddit', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('science', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('in_domain', {'em': 55.6, 'f1': 67.7, 'turns': 7983}), ('out_domain', {'em': 0.0, 'f1': 0.0, 'turns': 0}), ('overall', {'em': 55.6, 'f1': 67.7, 'turns': 7983})])
 ------------ Epoch 7/10 Batch 8050/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27350790650624296
 ------------ Epoch 7/10 Batch 8100/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27338614833851654
 ------------ Epoch 7/10 Batch 8150/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27340773865626267
 ------------ Epoch 7/10 Batch 8200/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2735214345543305
 ------------ Epoch 7/10 Batch 8250/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27342039658309836
 ------------ Epoch 7/10 Batch 8300/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2734092886140009
 ------------ Epoch 7/10 Batch 8350/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2735313852851887
 ------------ Epoch 7/10 Batch 8400/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27326584001493065
 ------------ Epoch 7/10 Batch 8450/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27313621533318205
 ------------ Epoch 7/10 Batch 8500/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2728908074268085
 ------------ Epoch 7/10 Batch 8550/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2726558299774402
 ------------ Epoch 7/10 Batch 8600/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2726299923173217
 ------------ Epoch 7/10 Batch 8650/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.2729062683432433
 ------------ Epoch 7/10 Batch 8700/11616 Training Results ------------ 
Total Loss: nan
Span Start Loss: nan
Span End Loss: nan
Type Loss: 0.27338198486587095
